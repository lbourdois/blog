---
title: "UN GUIDE VISUEL SUR LE MELANGE D'EXPERTS (MoE)"
tags:
  - NLP
  - MoE
  - Mixture of experts
  - Mélanges d'experts
excerpt : "Divers - Démystifier le rôle des mélanges d'experts (mixture of experts ou MoE)"
header :
    overlay_color: "#1C2A4D"
    teaser : "https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_0.png"
author_profile: false
sidebar:
    nav: sidebar-misc
classes: wide
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> 

# <span style="color: #FF0000"> **Avant-propos** </span>
Cet article est une traduction de celui de Maarten Grootendorst : [A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts).<br>
Maarten est co-auteur du livre [*Hands-On Large Language Models*](https://www.llm-book.com/).
<br><br><br>

# <span style="color: #FF0000"> **Introduction** </span>
Dans cet article, nous allons prendre le temps d'explorer la composante importante, qu’est le **mélange d'experts** ou MoE (pour *mixture of experts*), à travers plus de 50 visualisations !  
Nous passerons en revue les deux principaux composants d’un MoE, à savoir les **experts** et le **routeur**, tels qu'ils sont appliqués dans les architectures basées sur les grands modèles de langue ou LLM (pour *Large Language Model*).
<br><br><br>

# <span style="color: #FF0000"> **Qu'est-ce qu'un mélange d'experts ?** </span>
Le mélange d'experts est une technique qui utilise de nombreux sous-modèles différents (ou « experts ») pour améliorer la qualité des LLM.
Deux éléments principaux définissent un MoE :  
•	Experts - Chaque [couche FFNN](https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_%C3%A0_action_directe) dispose alors d'un ensemble d'experts dont un sous-ensemble peut être choisi. Ces experts sont généralement des eux-mêmes FFNN.  
•	Routeur - Détermine quels *tokens* sont envoyés à quels experts.  
Dans chaque couche d'un LLM avec un MoE, nous trouvons des experts (quelque peu spécialisés) :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_1.png">
</figure>
</center>
Il faut savoir qu'un expert n'est pas spécialisé dans un domaine spécifique comme la psychologie ou la biologie par exemple. Il apprend tout au plus des informations syntaxiques au niveau du mot :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_2.png">
</figure>
</center>
Plus précisément, leur expertise porte sur le traitement de *tokens* spécifiques dans des contextes spécifiques.
Le routeur sélectionne le(s) expert(s) le(s) mieux adapté(s) à une entrée donnée :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_3.png">
</figure>
</center>
Chaque expert n'est pas un LLM complet mais un sous-modèle faisant partie de l'architecture du LLM. 
<br><br><br>

# <span style="color: #FF0000"> **Les experts** </span>
Pour explorer ce que les experts représentent et comment ils fonctionnent, examinons d'abord ce que le MoE est censé remplacer : les *couches denses*.
<br><br>

## <span style="color: #FFBF00"> **Couches denses** </span>
Le mélange d’experts part d'une fonctionnalité relativement basique des LLM, à savoir le *Feedforward Neural Network* (FFNN).  
Rappelons qu'un [*transformer*](https://lbourdois.github.io/blog/nlp/Transformer/)-décodeur (i.e. un [GPT](https://lbourdois.github.io/blog/nlp/GPT2/)) standard applique le FFNN après la couche de normalisation :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_4.png">
</figure>
</center>
Un FFNN permet au modèle d'utiliser les informations contextuelles créées par le mécanisme d'attention, en les transformant davantage pour saisir des relations plus complexes dans les données.  
Cependant, le FFNN croît rapidement en taille. Pour apprendre ces relations complexes, il étend généralement les données qu'il reçoit :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_5.png">
</figure>
</center>
<br><br>

## <span style="color: #FFBF00"> **Couches éparses** </span>
Le FFNN dans un *transformer* standard est appelé un modèle dense puisque tous les paramètres (ses poids et ses biais) sont activés. Rien n'est oublié et tout est utilisé pour calculer la sortie.  
Si nous examinons de plus près le modèle dense, nous remarquons que l'entrée active tous les paramètres dans une certaine mesure :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_6.png">
</figure>
</center>
Au contraire, les modèles épars n'activent qu'une partie de l’ensemble de leurs paramètres et sont étroitement liés au mélange d'experts.  
Pour illustrer ceci, nous pouvons découper notre modèle dense en morceaux (appelés experts), le réentraîner et n'activer qu'un sous-ensemble d'experts à un moment donné :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_7.png">
</figure>
</center>
L'idée sous-jacente est que chaque expert apprend des informations différentes au cours de l’entraînement. Ensuite, lors de l'inférence, seuls des experts spécifiques sont utilisés car ils sont les plus pertinents pour une tâche donnée.  
Lors d’une question, nous pouvons sélectionner l'expert le mieux adapté à une tâche donnée :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_8.png">
</figure>
</center>
<br><br>

## <span style="color: #FFBF00"> **Qu'est-ce qu'apprend un expert?** </span>
Comme l’avons indiqué, les experts apprennent des informations plus fines que des domaines entiers (voir notamment le papier [*ST-MoE: Designing stable and transferable sparse expert models*](https://arxiv.org/abs/2202.08906) de ZOPH, BELLO, FEDUS et al. (2022)). C'est pourquoi les qualifier d' « experts » a parfois été considéré comme trompeur. 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_9.png">
<figcaption>Tableau 13 du papier <i>ST-MoE</i> montrant des exemples de spécialisation d’experts dans un encodeur</figcaption>
</figure>
</center>
 

Les experts de modèle décodeur ne semblent toutefois pas avoir le même type de spécialisation. Cela ne signifie pas pour autant que tous les experts sont égaux.  
Un bon exemple se trouve dans le papier [*Mixtral of Experts*](https://arxiv.org/abs/2401.04088) de Mistral (2024) où chaque *token* est surligné avec le premier choix de l'expert.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_10.png">
  <figcaption>Image tirée de la figure 8 à la page 8 du papier où sont visibles d’autres exemples</figcaption>
</figure>
</center>
Ainsi, bien que les experts d’un décodeur ne semblent pas avoir de spécialisation, ils apparaissent être utilisés de manière cohérente pour certains types de *tokens*.
<br><br>

## <span style="color: #FFBF00"> **L'architecture des experts** </span>
Bien qu'il soit agréable de voir les experts comme une couche cachée d'un modèle dense découpé en morceaux, ils sont souvent eux-mêmes des FFNN entiers :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_11.png">
</figure>
</center>
Comme la plupart des LLM sont un empilement de plusieurs blocs décodeur, une entrée passera par plusieurs experts avant de générer la sortie :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_12.png">
</figure>
</center>
Les experts choisis diffèrent probablement d'un *token* à l'autre, ce qui se traduit par des « chemins » différents :
<center>
<figure class="image">
 <img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_13.png">
</figure>
</center>
Si nous modifions notre visualisation du bloc décodeur,  ce dernier contiendra désormais davantage de FFNN (un pour chaque expert) :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_14.png">
</figure>
</center>
Le bloc décodeur dispose désormais de plusieurs FFNN (chacun étant un expert) qu'il peut utiliser lors de l'inférence.
<br><br><br>

# <span style="color: #FF0000"> **Le mécanisme de routage** </span>
Maintenant que nous disposons d'un ensemble d'experts, comment le modèle sait-il quels experts utiliser ?  
Juste avant les experts, un **routeur** (également appelé **réseau de portes**) est ajouté ; il est entraîné à sélectionner l'expert à choisir pour un *token* donné.
<br><br>

## <span style="color: #FFBF00"> **Le routeur** </span>
Le **routeur** est également un FFNN. Il est utilisé pour sélectionner l’expert le plus adapté en fonction d'une entrée particulière via les probabilités qu'il produit :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_15.png">
</figure>
</center>
La couche expert renvoie la sortie de l'expert sélectionné multipliée par la valeur de la porte (probabilités de sélection).  
Le routeur et les experts (dont seuls quelques-uns sont sélectionnés) constituent la **couche MoE** :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_16.png">
</figure>
</center>
Une couche de MoE donnée peut être de deux tailles, soit éparse, soit dense.  
Toutes deux utilisent un routeur pour sélectionner les experts, mais un MoE épars n'en sélectionne que quelques-uns, tandis qu'un MoE dense les sélectionne tous, mais potentiellement selon des distributions différentes. 
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_17.png">
</figure>
</center>
Par exemple, étant donné un ensemble de *tokens*, un MoE dense répartira les *tokens* entre tous les experts, tandis MoE épars n’en sélectionnera que quelques-uns.  
Dans l'état actuel des LLM, lorsque vous voyez un mélange d’experts, il s'agit généralement d'un MoE épars car il vous permet d'utiliser un sous-ensemble d'experts. Cette méthode est moins coûteuse en termes de calcul, ce qui est une caractéristique importante pour les LLM. 
<br><br>

## <span style="color: #FFBF00"> **Sélection des experts** </span>
Le réseau à portes est sans doute le composant le plus important de tout MoE car il décide non seulement des experts à sélectionner lors de l'inférence, mais aussi lors de l'entraînement.  
Dans sa forme la plus élémentaire, nous multiplions l'entrée (<span style="color:#18651D;">**x**</span>) par la matrice de poids du routeur (<span style="color:#A04601;">**W**</span>):
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_18.png">
</figure>
</center>
Ensuite, nous appliquons une fonction **SoftMax** sur la sortie pour créer une distribution de probabilité <span style="color:#F4D7C1;">**G**</span>(<span style="color:#18651D;">**x**</span>)  par expert :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_19.png">
</figure>
</center>
Le routeur utilise cette distribution de probabilité pour choisir le meilleur expert correspondant à une entrée donnée.  
Enfin, nous multiplions la sortie de chaque routeur par chaque expert sélectionné et nous additionnons les résultats.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_20.png">
</figure>
</center>
Assemblons le tout et explorons la manière dont l'entrée passe par le routeur et les experts :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_21.png">
</figure>
</center>
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_22.png">
</figure>
</center>
<br><br>

## <span style="color: #FFBF00"> **La complexité du routage** </span>
Toutefois, cette fonction simple conduit souvent le routeur à choisir le même expert car certains experts peuvent apprendre plus vite que d'autres :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_23.png">
</figure>
</center>
Non seulement la distribution des experts choisis sera inégale, mais certains experts ne seront pratiquement pas entraînés. Il en résulte des problèmes tant au niveau de l’entraînement que de l'inférence.  
Au contraire, nous voulons que les experts aient la même importance pendant l’entraînement et l'inférence, ce que nous appelons l'**équilibrage de la charge**. D'une certaine manière, il s'agit d'éviter le surentraînement des mêmes experts.
<br><br><br>

# <span style="color: #FF0000"> **Équilibrage de la charge** </span>
Pour équilibrer l'importance des experts, nous devons examiner le routeur qui est le principal élément permettant de décider quels experts choisir à un moment donné. 
<br><br>

## <span style="color: #FFBF00"> **KeepTopK** </span>
Une méthode pour équilibrer la charge du routeur consiste à utiliser une méthode simple appelée [KeepTopK](https://arxiv.org/abs/1701.06538) par SHAZEER, MIRHOSEINI, MAZIARZ et al. (2017). En introduisant un bruit (gaussien) entraînable, nous pouvons éviter que les mêmes experts soient toujours sélectionnés :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_24.png">
</figure>
</center>
Ensuite, les poids de tous les experts, sauf les $$k$$ premiers que vous souhaitez activer (par exemple $$2$$), seront fixés à $$-∞$$ :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_25.png">
</figure>
</center>
En fixant ces poids à $$-∞$$, la sortie de la fonction SoftMax sur ces poids se traduira par une probabilité de $$0$$ :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_27.png">
</figure>
</center>
L’approche KeepTopK est une stratégie que de nombreux LLM utilisent encore malgré de nombreuses alternatives prometteuses. Notez qu’elle peut également être utilisée sans le bruit supplémentaire.
<br>

### <span style="color: #51C353"> **Choix des *tokens*** </span>
L’approche KeepTopK achemine chaque *token* vers quelques experts sélectionnés. Cette méthode, appelée « *Token Choice* », permet d'envoyer un *token* donné à un seul expert (routage top-$$1$$) :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_28.png">
</figure>
</center>
ou à plusieurs experts (routage top-$$k$$) :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_29.png">
</figure>
</center>
L'un de ses principaux avantages est de permettre d'évaluer et d'intégrer les contributions respectives des experts. 
<br>
### <span style="color: #51C353"> **Perte auxiliaire** </span>
Pour obtenir une répartition plus homogène des experts pendant l’entraînement, la <span style="color:#329C08;">**perte auxiliaire**</span> (également appelée perte d'équilibrage de la charge) a été ajoutée à la perte standard du réseau.  
Cela ajoute une contrainte qui oblige les experts à avoir la même importance.  
La première composante de cette perte auxiliaire consiste à additionner les valeurs des routeurs pour chaque expert sur l'ensemble du batch : 
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_30.png">
</figure>
</center>
Cela nous donne les <span style="color:#9B4505;"><i>**scores d'importance**</i></span> par expert qui représentent la probabilité qu'un expert donné soit choisi indépendamment de l'entrée.  
Nous pouvons les utiliser pour calculer le *coefficient de variation* (<span style="color:#A50BA8;">**CV**</span>), qui nous indique à quel point les scores d'importance sont différents d'un expert à l'autre :  
<span style="color:#A50BA8;">**Coefficient de variation**</span>(<span style="color:#A50BA8;">**CV**</span>) = écart-type (σ) / moyenne (μ)  

Par exemple, s'il y a beaucoup de différences dans les scores d'importance, le <span style="color:#A50BA8;">**CV**</span> sera élevé :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_31.png">
</figure>
</center>
En revanche, si tous les experts ont des scores d'importance similaires, le <span style="color:#A50BA8;">**CV**</span> sera faible (ce qui est notre objectif) :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_32.png">
</figure>
</center>
À l'aide de ce score <span style="color:#A50BA8;">**CV**</span>, nous pouvons mettre à jour la <span style="color:#329C08;">**perte auxiliaire**</span> pendant l’entraînement de manière à ce qu'elle vise à réduire autant que possible le score <span style="color:#A50BA8;">**CV**</span>  (en accordant ainsi la même importance à chaque expert) :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_33.png">
</figure>
</center>
Enfin, la <span style="color:#329C08;">**perte auxiliaire**</span> est ajoutée en tant que perte distincte à optimiser pendant la formation. 
<br><br>

## <span style="color: #FFBF00"> **Capacité des experts** </span>
Le déséquilibre ne concerne pas seulement les experts sélectionnés, mais aussi les distributions des *tokens* envoyés à l'expert.  
Par exemple, si les *tokens* d'entrée sont envoyés de manière disproportionnée à un expert plutôt qu'à un autre, cela peut également entraîner un sousentraînement :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_34.png">
</figure>
</center>
Ici, il ne s'agit pas seulement de savoir quels experts sont utilisés, mais aussi **dans quelle mesure** ils le sont.  
Une solution à ce problème consiste à limiter la quantité de *tokens* qu'un expert donné peut traiter, autrement dit la [capacité de l'expert](https://arxiv.org/abs/2006.16668) (LEPIKHIN et al. (2020)). Lorsqu'un expert a atteint sa capacité, les *tokens* restants sont envoyés à l'expert suivant :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_35.png">
</figure>
</center>
Si les deux experts ont atteint leur capacité, le *token* n'est traité par aucun expert et est envoyé à la couche suivante. C'est ce que l'on appelle le débordement de *token* (*token overflow*).
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_36.png">
</figure>
</center>
<br><br>

## <span style="color: #FFBF00"> **Simplifier le MoE avec *Switch Transformer*** </span>
L'un des premiers modèles de MoE basés sur un *transformer* ayant traité les problèmes d'instabilité durant l'entraînement (tels que l'équilibrage de la charge) est le [Switch Transformer](https://arxiv.org/abs/2101.03961) de FEDUS, ZOPH et SHAZZER (2021).  Il simplifie une grande partie de l'architecture et de l’entraînement tout en augmentant la stabilité de ce dernier.
<br>

### <span style="color: #51C353"> **La couche de commutation (*Switching Layer*)** </span>
Le *Switch Transformer* est un modèle T5 (encodeur-décodeur) qui remplace la couche FFNN traditionnelle par une couche de commutation. Cette couche est une couche MoE éparse qui sélectionne un seul expert pour chaque *token* (routage top-$$1$$). 
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_37.png">
</figure>
</center>
Le routeur ne fait aucun effort particulier pour calculer quel expert choisir et prend la softmax de l'entrée multipliée par les poids de l'expert (comme nous l'avons fait précédemment).
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_38.png">
</figure>
</center>
Cette architecture (routage top-$$1$$) suppose qu'un seul expert est nécessaire pour que le routeur apprenne comment acheminer l'entrée. Cela contraste avec ce que nous avons vu précédemment où nous avons supposé que les *tokens* devaient être acheminés vers plusieurs experts (routage top-$$k$$) pour apprendre le comportement de routage.
<br>

### <span style="color: #51C353"> **Facteur de capacité** </span>
Le facteur de capacité est une valeur importante car il détermine le nombre de *tokens* qu'un expert peut traiter. Le *Switch Transformer* va plus loin en introduisant un facteur de capacité qui influe directement sur la capacité de l'expert :  
capacité de l’expert = (*tokens* par batch/nombre d’experts) * facteur de capacité  

Les composantes de la capacité de l'expert sont simples :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_39.png">
</figure>
</center>
Si nous augmentons le facteur de capacité, chaque expert pourra traiter davantage de *tokens*.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_40.png">
</figure>
</center>
Cependant, si le facteur de capacité est trop élevé, nous gaspillons des ressources de calcul. À l'inverse, si le facteur de capacité est trop faible, les performances du modèle chuteront en raison du *débordement des tokens*.
<br>

### <span style="color: #51C353"> **Perte auxiliaire** </span>
Pour éviter de laisser tomber des *tokens*, une version simplifiée de la perte auxiliaire a été introduite.  
Au lieu de calculer la variation du coefficient, cette perte pondère la fraction de *tokens* distribués par rapport à la fraction de probabilité de routage par expert :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_41.png">
</figure>
</center>
Puisque l'objectif est d'obtenir un routage uniforme des *tokens* parmi les $$N$$ experts, nous voulons que les vecteurs $$P$$ et $$f$$ aient des valeurs de $$1/N$$.  
$$α$$ est un hyperparamètre que nous pouvons utiliser pour finetuner l'importance de cette perte pendant l’entraînement.  
Des valeurs trop élevées prendront le pas sur la fonction de perte primaire et des valeurs trop faibles n'auront que peu d'effet sur l'équilibrage de la charge. 
<br><br>

## <span style="color: #FFBF00">**Mélange d'experts dans les modèles de vision**</span>
Le mélange d’expert n'est pas une technique réservée aux modèles de langage. En effet, les modèles de vision, tels que les *Vision-Transformer* ([ViT](https://arxiv.org/abs/2010.11929) par DOSOVITSKIY, BEYER, KOLESNIKOV, WEISSENBORN, ZHAI, HOULSBY et al. (2020)), s'appuyant sur des architectures basées sur des *transformers* peuvent également utiliser le MoE.  
Pour rappel, le ViT est une architecture qui divise les images en patchs qui sont traitées de la même manière que les *tokens*.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_42.jpg">
</figure>
</center>
Ces patchs (ou *tokens*) sont ensuite projetés dans des enchâssements (avec des enchâssements positionnels) avant d'être introduits dans un encodeur classique :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_43.png">
</figure>
</center>
Au moment où ces patchs entrent dans l’encodeur, ils sont traités comme des *tokens*, ce qui permet à cette architecture de s'adapter parfaitement à un MoE. 
<br>

### <span style="color: #51C353"> **Vision-MoE** </span>
[Vision-MoE](https://arxiv.org/abs/2106.05974) (V-MoE) par RIQUELME, PUIGCERVER, MUSTAFA et al. (2021) est l'une des premières implémentations de MoE dans un modèle d'image. Elle prend le ViT comme nous l'avons vu précédemment et remplace le FFNN dense dans l'encodeur par un MoE épars.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_44.png">
</figure>
</center>
Cela permet aux ViT, généralement plus petits que les modèles de langage, de passer massivement à l’échelle via l'ajout d'experts.  
Une petite capacité d'expertise prédéfinie a été utilisée pour chaque expert afin de réduire les contraintes matérielles, car les images comportent généralement de nombreux patchs. Toutefois, une faible capacité a tendance à entraîner l'abandon de correctifs (ce qui s'apparente à un débordement de *tokens*).
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_45.jpg">
</figure>
</center>
Pour maintenir la capacité basse, le réseau attribue des scores d'importance aux patchs et les traite en premier, ainsi les patchs débordants sont généralement moins importants. C'est ce qu'on appelle le *routage prioritaire* par batchs.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_46.jpg">
</figure>
</center>
Par conséquent, les patchs importants devraient toujours être acheminés si le pourcentage de *tokens* diminue.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_47.png">
</figure>
</center>
Le routage prioritaire permet de traiter moins de patchs en se concentrant sur les plus importants.
<br>

### <span style="color: #51C353"> **De V-MoE à Soft-MoE** </span>
Dans V-MoE, le système d'évaluation des priorités permet de différencier les patchs plus ou moins importants. Cependant, ils sont attribués à chaque expert et les informations contenues dans ceux non traités sont perdues.  
L'objectif de [Soft-MoE](https://arxiv.org/abs/2308.00951) de PUIGCERVER, RIQUELME et al. (2023) est de passer d'une affectation discrète à une affectation souple en mélangeant les patchs.  
Dans un premier temps, nous multiplions l'entrée <span style="color:#488946;">**x**</span>  (les patchs d’enchâssements) par une matrice apprenable <span style="color:#6A73AB;">**Φ**</span>. Ceci nous donne des informations pour le routeur pour indiquer dans quelle mesure un certain token est lié à un expert donné.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_48.png">
</figure>
</center>
En prenant ensuite la softmax (sur les colonnes) de la matrice d'information du routeur <span style="color:#456E96;">**R**</span>, nous mettons à jour les enchâssements de chaque patch.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_49.jpg">
</figure>
</center>
La mise à jour des enchâssements des patchs est essentiellement la moyenne pondérée de tous les enchâssements des patchs.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_50.png">
</figure>
</center>
Visuellement, c'est comme si tous les patchs étaient mélangés. Ces patchs combinés sont ensuite envoyés à chaque expert. Après avoir généré la sortie, ils sont à nouveau multipliés par la matrice de routage <span style="color:#456E96;">**R**</span>.
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_51.png">
</figure>
</center>
La matrice de routage affecte l'entrée au niveau du *token* et la sortie au niveau de l'expert.  
En conséquence, nous obtenons des patchs/tokens « souples » qui sont traités au lieu d'une entrée discrète.
<br><br><br>

# <span style="color: #FF0000">**Mixtral 8x7B : paramètres actifs vs. Paramètres épars**</span>
Une grande partie de l'intérêt d'un MoE réside dans ses impératifs en matière de calcul. Étant donné que seul un sous-ensemble d'experts est utilisé à un moment donné, nous avons accès à plus de paramètres que nous n'en utilisons.  
Bien qu'un MoE donné ait plus de paramètres à charger (*paramètres épars*), moins sont activés puisque nous n'utilisons que certains experts pendant l'inférence (*paramètres actifs*). 
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_52.png">
</figure>
</center>
En d'autres termes, nous devons toujours charger l'ensemble du modèle (incluant tous les experts) sur l’appareil (paramètres épars), mais lorsque nous procédons à l'inférence, nous ne devons utiliser qu'un sous-ensemble (paramètres actifs). Les modèles avec MoE ont besoin de plus de VRAM pour charger tous les experts, mais s'exécutent plus rapidement pendant l'inférence.  
Comme exemple, examinons le nombre de paramètres épars par rapport aux paramètres actifs du [Mixtral 8x7B](https://arxiv.org/abs/2401.04088) de Mistral (2024).
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_52.png">
</figure>
</center>
Nous pouvons constater que chaque expert a une taille de $$5,6$$Mds et non de $$7$$ (bien qu'il y ait $$8$$ experts). 
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/MoE/image_53.png">
</figure>
</center>
Nous devrons charger $$8$$x$$5,6$$Mds ($$46,7$$Mds) de paramètres (ainsi que tous les paramètres partagés) mais nous n’utiliserons que $$2$$x$$5,6$$Mds  ($$12,8$$Mds) paramètres pour l'inférence. 
<br><br><br>

# <span style="color: #FF0000"> **Conclusion** </span>
Ceci conclut notre voyage avec les mélanges d'experts ! En espérant que cet article vous a permis de mieux comprendre le potentiel de cette technique. Maintenant que presque tous les LLM contiennent au moins une variante de MoE, il semble que cette technique soit là pour rester.  
J'espère que cette introduction a été accessible. Si vous souhaitez aller plus loin sur ce sujet, je vous suggère les ressources suivantes :  
•	Ce [papier](https://arxiv.org/abs/2209.01667) par FEDUS, DEAN et ZOPH (2022) ainsi que [celui-ci](https://arxiv.org/abs/2407.06204) par CAI, JIANG, WANG et al. (2024) offrent une excellente vue d'ensemble des dernières innovations concernant les MoE.  
•	Le [papier](https://arxiv.org/abs/2202.09368) de ZHOU et al. (2022) sur le choix du routage.  
•	Un [excellent article de blog](https://cameronrwolfe.substack.com/p/conditional-computation-the-birth) par Cameron R. WOLFE qui passe en revue quelques-uns des principaux documents (et leurs conclusions).  
•	Un [article de blog](https://brunomaga.github.io/Mixture-of-Experts) par Bruno MAGALHAES retraçant la chronologie de MoE.
<br><br><br>

# <span style="color: #FF0000"> **Références** </span>
- [*A Visual Guide to Mixture of Experts (MoE)*](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts) de Maarten GROOTENDORST (2024) 
- [*ST-MoE: Designing Stable and Transferable Sparse Expert Models*](https://arxiv.org/abs/2202.08906) de Barret ZOPH, Irwan BELLO, Sameer KUMAR, Nan DU, Yanping HUANG, Jeff DEAN, Noam SHAZEER et William FEDUS (2022)  
- [*Mixtral of Experts*](https://arxiv.org/abs/2401.04088) de Mistral (2024)
- [*Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*](https://arxiv.org/abs/1701.06538) par Noam SHAZEER, Azalia MIRHOSEINI, Krzysztof MAZIARZ Andy DAVIS, Quoc LE, Geoffrey HINTON et Jeff DEAN (2017)
- [*GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding*]([https://arxiv.org/abs/1701.06538](https://arxiv.org/abs/2006.16668)) de Dmitry LEPIKHIN, HyoukJoong LEE, Yuanzhong XU, Dehao CHEN, Orhan FIRAT, Yanping HUANG, Maxim KRIKUN, Noam SHAZEER et Zhifeng CHEN (2020)
- [*Switch Transformer: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*](https://arxiv.org/abs/2101.03961) de William FEDUS, Barret ZOPH et Noam SHAZZER (2021)
- [*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/abs/2010.11929) par Alexey DOSOVITSKIY, Lucas BEYER, Alexander KOLESNIKOV, Dirk WEISSENBORN, Xiaohua ZHAI, Thomas UNTERTHINER, Mostafa DEHGHANI, Matthias MINDERER, Georg HEIGOLD, Sylvain GELLY, Jakob USKOREIT et Neil HOULSBY (2020)
- [*Scaling Vision with Sparse Mixture of Experts*](https://arxiv.org/abs/2106.05974) (V-MoE) par Carlos RIQUELME, Joan PUIGCERVER, Basil MUSTAFA, Maxim NEUMANN, Rodolphe JENATTON, André SUSANO PINTO, Daniel KEYSERS et Neil HOULSBY (2021)
- [*From Sparse to Soft Mixtures of Experts*](https://arxiv.org/abs/2308.00951) de Joan PUIGCERVER, Carlos RIQUELME, Basil MUSTAFA et Neil HOULSBY (2023)
- [*A Review of Sparse Expert Models in Deep Learning*](https://arxiv.org/abs/2209.01667) par William FEDUS, Jeff DEAN et Barret ZOPH (2022)
- [*A Survey on Mixture of Experts*](https://arxiv.org/abs/2407.06204) par Weilin CAI, Juyong JIANG, Fan WANG, Jing TANG, Sunghun KIM et Jiayi HUANG (2024)
- [*Mixture-of-Experts with Expert Choice Routing*](https://arxiv.org/abs/2202.09368) de Yanqi ZHOU, Tao LEI, Hanxiao LIU, Nan DU, Yanping HUANG, Vincent ZHAO, Andrew DAI, Zhifeng CHEN, Quoc LE et James LAUDON (2022)

<br><br><br>
# <span style="color: #FF0000"> **Citation** </span>
```
@inproceedings{MoE_blog_post,
author = {Loïck BOURDOIS},
title = {Un guide visuel sur le mélange d'experts (MoE)},
year = {2025},
url = {https://lbourdois.github.io/blog/MoE/}
}
```
