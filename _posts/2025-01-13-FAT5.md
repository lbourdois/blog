---
title: "LE FLASH ATTENTION T5 (FAT5)"
tags:
  - NLP
  - FAT5
  - Flash Attention T5
excerpt : "NLP - Extension de la Flash Attention au T5 via des noyaux CUDA et Triton"
header:
   overlay_color: "#1C2A4D"
   teaser : "https://raw.githubusercontent.com/catie-aq/flashT5/main/assets/FAT5_dark.gif"
author_profile: false
sidebar:
    nav: sidebar-sample
classes: wide
---


# <span style="color: #FF0000"> **Avant-propos** </span>
Ce travail a été fait en collaboration avec [Boris ALBAR](https://b-albar.github.io/portfolio/) au [CATIE](https://www.catie.fr/).  
Le présent article est une introduction renvoyant ensuite sur un *Space* Hugging Face permettant une meilleure mise en page.

<br><br>

# <span style="color: #FF0000"> **Introduction** </span>
Alors que beaucoup d’efforts ont été consacrés à l’optimisation de transformer de type décodeur, abandonnant ainsi l’encodeur, nous pensons qu’il est essentiel de maintenir une architecture encodeur-décodeur.<br>
En effet, cette architecture qui présente des performances intéressantes pour l’instruction tuning, est propice à la distillation et semble supérieure aux modèles décodeur lorsqu’elle est finetunée.<br>
Il a aussi été montré que les modèles encodeur-décodeur entraînés avec une modélisation du langage masqué obtiennent une meilleure performance zéro-<i>shot</i> après un finetuning multitâche par rapport à un modèle décodeur.<br>
Au-delà du NLP sur lequel nous nous sommes concentrés, l’architecture encodeur-décodeur est très utilisée dans d’autres domaines comme l’audio ou les séries temporelles par exemple.<br>
Dans cette logique, nous avons décidé de nous intéresser au T5.<br><br>

Les optimisations mises en place afin de pré-entraîner de manière efficiente un T5 de 147M de paramètres en français en un temps raisonnable et un coût limité et plus généralement la méthodologie appliquée, sont détaillées dans un article de blog disponible sur <a href="https://hf.co/spaces/CATIE-AQ/FAT5-rapport">Hugging Face</a>.<br>
Nous y détaillons notamment la conception de noyaux CUDA/Triton pour rendre la Flash Attention compatible avec le T5 et disposer d'une inférence linéaire étendant ainsi la taille de contexte prenable en compte par le modèle.<br>
</p>
<br>

<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/catie-aq/flashT5/main/assets/FAT5_dark.gif">
</figure>
</center>

<br>

<b>Le code de pré-entrainement est disponible sur notre <a href="https://github.com/catie-aq/flashT5">répertoire GitHub</a> sous licence Apache-2.0 
et les poids du modèle entraîné sur le compte <a href="https://huggingface.co/CATIE-AQ">Hugging Face du CATIE</a>.</b>

<br>

<center>
    Lire l'article complet sur <a href="https://hf.co/spaces/CATIE-AQ/FAT5-rapport">Hugging Face</a>.
</center>
