---
title: "LE FLASH ATTENTION T5 (FAT5)"
tags:
  - NLP
  - FAT5
  - Flash Attention T5
excerpt : "NLP - Extension de la Flash Attention au T5 via des noyaux CUDA et Triton"
header:
   overlay_color: "#1C2A4D"
   teaser : "https://raw.githubusercontent.com/catie-aq/flashT5/main/assets/FAT5_dark.gif"
author_profile: false
sidebar:
    nav: sidebar-sample
classes: wide
---


# <span style="color: #FF0000"> **Avant-propos** </span>
Ce travail a été fait en collaboration avec [Boris ALBAR](https://b-albar.github.io/portfolio/) au [CATIE](https://www.catie.fr/).  
Le présent article est une introduction renvoyant ensuite sur un *Space* Hugging Face permettant une meilleure mise en page.

<br><br>

# <span style="color: #FF0000"> **Introduction** </span>
Alors que beaucoup d’efforts ont été consacrés à l’optimisation de <i>transformer</i> de type décodeur, abandonnant ainsi l’encodeur, nous pensons qu’il est essentiel de maintenir une architecture encodeur-décodeur.  
En effet, cette architecture qui présente des performances intéressantes pour l’[instruction tuning](http://arxiv.org/abs/2306.04757), est propice à la [distillation](https://arxiv.org/abs/2305.02301) et semble supérieure aux modèles décodeur lorsqu’elle est [finetunée](https://arxiv.org/abs/2402.00841).  
Il a aussi été [montré](https://arxiv.org/abs/2204.05832) que les modèles encodeur-décodeur entraînés avec une modélisation du langage masqué obtiennent une meilleure performance zéro-<i>shot</i> après un <i>finetuning</i> multitâche par rapport à un modèle décodeur.  
Au-delà du NLP sur lequel nous nous sommes concentrés, l’architecture encodeur-décodeur est très utilisée dans d’autres domaines comme l’audio ou les séries temporelles par exemple.  
Dans cette logique, nous avons décidé de nous concentrer sur le [T5](https://jmlr.org/papers/v21/20-074.html).<br><br>

Dans cet article, sont détaillons les optimisations que nous avons mises en place afin de pré-entraîner de manière efficiente un T5 de 147M de paramètres en français en un temps raisonnable (1 461 H pour 419Mds de <i>tokens</i>) et avec des moyens limités (1 A100 ; soit un budget de calcul d'environ 2 200 euros). Pour ce faire, nous avons conçu des noyaux CUDA/Triton afin de rendre la Flash Attention compatible avec T5 et de fournir une inférence linéaire, étendant ainsi la taille du contexte qui peut être prise en compte par le modèle. L'ensemble des optimisations appliquées sont détaillées sur [Hugging Face](https://hf.co/spaces/CATIE-AQ/FAT5-rapport).
<br>

<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/catie-aq/flashT5/main/assets/FAT5_dark.gif">
</figure>
</center>

<br>

<b>Le code de pré-entrainement est disponible sur notre <a href="https://github.com/catie-aq/flashT5">répertoire GitHub</a> sous licence Apache-2.0 
et les poids du modèle entraîné sur le compte <a href="https://huggingface.co/CATIE-AQ">Hugging Face du CATIE</a>.</b>

<br>

<center>
    Lire l'article complet sur <a href="https://hf.co/spaces/CATIE-AQ/FAT5-rapport">Hugging Face</a>.
</center>
