---
title: "ÉVOLUTION DES STATE SPACE MODELS (SSM) EN 2022"
categories:
  - SSM
tags:
  - Introduction aux SSM 
excerpt : SSM - Revue de littérature des SSM parus lors de l'année 2022
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/NLP_radom_blog.png"
    teaser: "https://github.com/lbourdois/blog/assets/58078086/cb2dca34-9a3e-481a-8773-2360a1ceaa1c"
author_profile: false
sidebar:
    nav: sidebar-ssm
classes: wide
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

⚠ **WIP** ⚠ 


# <span style="color: #FF0000"> **Avant-propos** </span>
Dans l’[article précédent](https://lbourdois.github.io/blog/ssm/introduction_ssm/), nous avons défini ce qu’est un State Space Model (SSM) à l’aide d’un système en temps continu. Nous l’avons discrétisé pour faire apparaître sa vue récurrente ainsi que sa vue convolutive. L’intérêt est alors de pouvoir entraîner notre modèle de manière convolutive puis de réaliser l’inférence de manière récurrente sur de très longues séquences.  
<br>

| ![image](https://github.com/lbourdois/blog/assets/58078086/12bbe1cf-3911-4bad-9a3b-3f427bc6bc82)|
|:--:|
| Figure 1 : *Image provenant de l'article de blog « [Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3) » d'Albert GU et al. (2022)*|

<br>
Cette vision a été introduite par Albert GU dans ses papiers LSSL et S4 parus en 2021. Le S4 étant l’équivalent du « *Attention is all you need* » pour les transformers.
Dans cet article, nous allons faire une revue de littérature des SSM parus durant l’année 2022. Ceux apparus en 2023 seront listés dans le prochain article.
L’objectif est de montrer les différentes évolutions de ces types de modèles au cours des mois, tout en restant à une vue globale des choses (i.e. je ne vais pas rentrer dans tous les détails des papiers listés). Lors de cette année 2022, les différentes avancées se sont focalisées à appliquer des algorithmes de discrétisations différents, ainsi qu’à remplacer la matrice HiPPO par une plus simple.
<br><br><br>

## <span style="color: #FFBF00"> **Modèles théoriques** </span>

Dans cette section, nous allons passer en revue des travaux théorique, au sens où ils sont des propositions d’amélioration de l’architecture du S4. Nous aborderons ensuite dans une section différente, des applications concrètes sur différentes tâches (audio, vision, etc.).
<br><br>

### <span style="color: #51C353"> **S4 V2** </span>

Commençons par une partie que le lecteur peut considérer comme facultative car dès la section suivante nous verrons que ce qui est décrit dans celle-ci se retrouve caduc du fait du remplacement de la matrice HiPPO par une plus simple. Nous rédigeons cette section afin de rendre compte de l’ensemble des travaux effectués sur la matrice HiPPO avant son abandon.   
  
Le 4 mars 2022, les auteurs du S4 ont actualisé leur papier afin d’y intégrer une section sur l’importance de la matrice HiPPO. Il s’agit de la section 4.4 dans la version la plus récente du papier.  
Pour résumer, elle consiste à rapporter les résultats observés à la suite de la réalisation d’ablations sur le jeu de données séquentielles CIFAR-10. Au lieu d'utiliser un SSM avec la matrice HiPPO, les auteurs ont essayé d'utiliser diverses paramétrisations comme une matrice dense aléatoire et une matrice diagonale aléatoire. 

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Hippo.png) |
|:--:|
| Figure 2 : *Accuracy sur l'échantillon de validation de CIFAR-10, tirée de la figure 3 du papier du S4* |

<br>
HiPPO est donc importante, mais est-ce que ces performances sont du spécifiquement à cette matrice ou bien est-ce que n’importe quelle matrice normale à faible rang (NPLR pour normal plus low-rank) suffirait ?

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/NPLR.png) |
|:--:|
| Figure 3 : *Accuracy sur l'échantillon de validation de CIFAR-10 avec différentes initialisations et parameterisations, tirée de la figure 4 du papier du S4* |

<br>
Initialiser une matrice NPLR avec HiPPO augmente considérablement les performances.
Ainsi, d’après ces expériences, la matrice HiPPO est primordiale afin d’obtenir un modèle performant.

Les auteurs du S4 ont approfondi leurs travaux et le 24 juin 2022, ont publié l’article [*How to Train Your HiPPO*](https://arxiv.org/abs/2206.12037). Il s’agit d’un papier extrêmement détaillé de plus de 39 pages.  
Pour résumer, dans ce travail, les auteurs se concentrent sur une interprétation plus intuitive des SSM en tant que modèle convolutif où le noyau de convolution est une combinaison linéaire de fonctions de base particulières, ce qui conduit à plusieurs généralisations et à de nouvelles méthodes.  
Ainsi, ils prouvent que la matrice A du S4 produit des polynômes de Legendre à échelle exponentielle (LegS). Cela confère au système une meilleure capacité à modéliser les dépendances à long terme via des noyaux très lisses.  
Les auteurs dérivent également un nouveau SSM qui produit des approximations de fonctions de Fourier tronquées (FouT). Cette méthode généralise les [transformées de Fourier à court terme]( https://fr.wikipedia.org/wiki/Transform%C3%A9e_de_Fourier_%C3%A0_court_terme) et les convolutions locales (c'est-à-dire un ConvNet standard), et peut également coder des fonctions de pointe pour résoudre des tâches de mémorisation classiques.  
A noter que c’est surtout HiPPO-FouT qui est introduit dans ce papier. HiPPO-LegS ayant été introduit dans le papier original d’HiPPO deux ans plus tôt. De même qu’HiPPO-LegT (polynômes de Legendre tronqués).  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master//assets/images/SSM_2022/Hippo_LegS_LegT_FouT.png) |
|:--:|
| Figure 4 : *Les différentes variantes d'Hippo* |

<br>
Les couleurs représentent les 4 premières fonctions de base $$K_n(t)$$ (le noyau de convolution) pour chacune des méthodes (nous invitons le lecteur à regarder le tableau 1 du papier pour savoir à quoi équivaut $$K_n(t)$$ dans chacune des méthodes).

De plus, les auteurs travaillent également sur le pas de temps $$∆$$, qui indépendamment d'une notion de discrétisation, peut être interprété simplement comme contrôlant la longueur des dépendances ou la largeur des noyaux du SSM. Les auteurs détaillent aussi comment choisir une bonne valeur de $$∆$$ pour une tâche donnée.

Les travaux menés permettent d’améliorer les résultats du S4 sur le benchmark LRA de plus de 6 points :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4v2_results.png) |
|:--:|
| Figure 5 : *Résultats du S4 v2 sur le benchmark* |

<br>
Le modèle résultant de ce papier est généralement appelé « S4 V2 » ou « S4 updated » dans la littérature à opposer au « S4 original » ou « S4 V1 ».

<br><br>

### <span style="color: #51C353"> Le DSS : *Diagonal State Spaces* </span>

Le 27 mars 2022, Ankit GUPTA introduit dans son papier [*Diagonal State Spaces are as Effective as Structured State Spaces*](https://arxiv.org/abs/2203.14343) les *Diagonal State Spaces* (DSS).  
Il semble que suite à ce papier, Albert GU et lui se sont mis à travailler ensemble d’une part sur une version actualisée de ce papier (GU apparaissant par la suite comme co-auteur dans les v2 et v3 de l’article) et d’autre part dans le cadre du S4D (voir section suivante).  
La principale chose à retenir est que cette approche est sensiblement plus simple que le S4. En effet, le DSS repose sur des matrices d'état diagonales (donc sans la correction de rang faible du S4, i.e. sans la matrice HiPPO) qui, si sont initialisées de manière appropriée, fonctionne mieux que le S4 original. L’usage d’une matrice diagonale à la place de la matrice HiPPO pour A est depuis devenu une norme.  

Arrêtons-nous néanmoins sur les quelques complexités/limites que contiennent ce papier. En effet, c’est en les listant que nous pourrons comprendre les apports des méthodes suivantes qui visent à simplifier davantage les choses.

**1. La discrétisation**  
Le DSS utilise le même système d’équations différentielles que le S4 : 

$$
  \begin{aligned}
    x' &= \mathbf{A}x + \mathbf{B}u \\
    y &= \mathbf{C}x
  \end{aligned}
$$

Cependant il utilise une discrétisation différente afin d’aboutir aux vues convolutives et récurrentes, à savoir la discrétisation [*zero-order hold*](https://en.wikipedia.org/wiki/Zero-order_hold) (ZOH) ou bloqueur d'ordre zéro en français, au lieu de la discrétisation bilinéaire, qui suppose que le signal que nous échantillonnons est constant entre chaque point d'échantillonnage.  
Ci-dessous un tableau comparatif des valeurs de $$A$$, $$B$$ et $$C$$ pour chacunes des deux discrétisations dans la vue récurrente, ainsi que l’expression du noyau de convolution dans la vue convolutive :

| Discrétisation | Bilinéaire | ZOH                    |
| -------------- | ---------- | ----------------------- |
| Récurrence     | $$\mathbf{\bar{A}} = (\mathbf {I} - \frac{\Delta}{2} \mathbf{A})^{-1}(\mathbf {I} + \frac{\Delta}{2} \mathbf{A})$$ <br> $$\mathbf {\bar{B}} = (\mathbf{I} - \frac{\Delta}{2}  \mathbf{A})^{-1} \Delta \mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$   | $$\mathbf{\bar{A}} = e^{\mathbf{A}\Delta}$$ <br> $$\mathbf{\bar{B}} = (\mathbf{\bar{A}} - I)\mathbf{A}^{-1}\mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$  |
| Convolution    | $$\mathbf{\bar{K}}_k = (\mathbf{\bar{C}}  \mathbf{\bar{B}}, \mathbf{\bar{C}} \mathbf{\bar{A}}  \mathbf{\bar{B}}, …, \mathbf{\bar{C}}  \mathbf{\bar{A}}^{k} \mathbf{\bar{B}})$$       | $$\mathbf{\bar{K}} = (\ \mathbf{C} e^{\mathbf{A}\cdot k\Delta} (e^{\mathbf{A}\Delta} - I)\mathbf{A}^{-1}\mathbf{B}\ )_{0 \leq k < L}$$ |

<br>

Pour la ZOH, après avoir déroulé les calculs, on obtient en fin de compte $$y_k = \sum_{j=0}^k \bar{C}\bar{A}^j\bar{B}\cdot u_{k-j} = \sum_{j=0}^k \bar{K}_j\cdot u_{k-j}$$.

Calculer $$y$$ à partir de $$u$$ et $$\bar{K}$$ s’effectue alors par Transformation de Fourier rapide en $$O(L~log(L))$$ avec $$L$$ la longueur de la séquence en calculant simultanément la multiplication de deux polynômes de degrés $$L-1$$.
<br>

**2. DSSsoftmax et DSSexp**  

<u>Version courte</u>  

GUPTA formule une proposition pour obtenir des DSS qui soient aussi expressifs que le S4, ce qui en pratique abouti à la formulation de deux DSS différents : le DSSexp et le DSSsoftmax. Les informations à retenir les concernant peuvent se résumer au tableau suivant :

| Approche        | DSSexp          | DSSsoftmax      |
| --------------- | --------------- | --------------- |
| Vue convolutive | $$K = \bar{K}_{\Delta, L}(\Lambda,\mathbb{I}_{1 \leq i \leq N},\ \widetilde{w})\\ = \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$ | $$K = \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\\ = w \cdot \Lambda^{-1} \cdot \text{row-softmax}(P)$$  |
| Vue récurrente  | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$ | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$ |
| Interprétation  | Agit comme la porte d'oubli d'une LSTM  | Si $$\Re(\lambda)<<0$$ : conserve l'information locale,<br> si $$\Re(\lambda)>>0$$ : peut capturer des informations à très longues distances                                                                                        |

Nous travaillons donc ici sur $$ℂ$$ et non pas $$ℝ$$.  

<br>

<u>Version longue</u>  

GUPTA formule la proposition suivante pour obtenir des DSS qui sont aussi expressifs que le S4 :

Let $$K \in \mathbb{R}^{1\times L}$$ be the kernel of length $$L$$ of a given state space $$(A, B, C)$$ and sample time $$\Delta > 0$$, where $$A \in \mathbb{C}^{N \times N}$$ is diagonalizable over $$\mathbb{C}$$ with eigenvalues $$\lambda_1,\ldots,\lambda_N$$ and $$\forall i$$, $$\lambda_i \neq 0$$ and $$e^{L\lambda_i\Delta} \neq 1$$. Let $$P \in \mathbb{C}^{N \times L}$$ be $$P_{i,k} = \lambda_i k\Delta$$ and $$\Lambda$$ be the diagonal matrix with $$\lambda_1,\ldots,\lambda_N$$. Then there exist $$\widetilde{w}, w \in \mathbb{C}^{1\times N}$$ such that

- (a) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ (1)_{1 \leq i \leq N},\ \widetilde{w})\ \ =\ \ \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$
- (b) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\ \ =\ \ w \cdot \Lambda^{-1} \cdot  \text{row-softmax}(P)$$

(a) suggère que nous pouvons paramétrer les espaces d'état via $$\Lambda, \widetilde{w} \in \mathbb{C}^N$$ et calculer le noyau comme indiqué. Malheureusement, dans la pratique, la partie réelle des éléments de $$Λ$$ peut devenir positive pendant l'apprentissage, ce qui rend l'apprentissage instable pour les entrées longues. Pour résoudre ça, les auteurs proposent deux méthodes : DSSexp et DSSsoftmax.

2.1 Convolution  
Dans DSSexp, les parties réelles de $$Λ$$ doivent être négatives. On a alors $$\Lambda = - \text{elementwise-exp}(\Lambda_\mathrm{re}) + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$. $$K$$ se calcule alors comme dans la formule indiquée dans la partie (a) de la proposition.  
Dans DSSsoftmax, on normalise chaque ligne de $$Λ$$ par la somme de ces éléments. On a alors $$\Lambda = \Lambda_\mathrm{re} + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$.  
$$K$$ se calcule alors comme dans la formule indiquée dans la partie (b) de la proposition.  
A noter que softmax sur $$\mathbb{C}$$ pouvant par exemple ne pas être défini lors de sofmax $$(0, i \pi)$$, les auteurs utilisent une version corrigée du softmax (détaillée dans l’annexe A.2. du papier) pour prévenir ce problème.

2.2 Récurrence  
Dans DSSexp, en utilisant la formule de la récurrence dans le tableau ci-dessus, on obtient $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$, où dans les deux égalités, $$\lambda_i$$ est la ième valeur propre de Lambda.  
Etant donné que $$\bar{A}$$ est diagonale, il est possible de calculer les $$x_k$$ indépendamment de la manière suivante :  $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + \lambda_i^{-1} (e^{\lambda_i\Delta} - 1)u_k$$.  
Et il est alors possible de déduire que, si $$|\lambda_i|\Delta \approx 0$$, nous avons $$x_{i,k} \approx x_{i,k-1}$$ et nous copions donc l'histoire sur de nombreux pas de temps. En revanche, si $$\mathrm{Re}(\lambda_i)\Delta \ll 0$$, alors $$_{i,k} \approx -\lambda_i^{-1}u_k$$ et donc l'information des pas de temps précédents est oubliée, similaire à une porte « forget » dans les LSTMs.

Dans DSSsoftmax, en utilisant la formule de la récurrence dans le tableau ci-dessus, on obtient : $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$.  
Ce qui fait que $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + {u_k(e^{\lambda_i\Delta} - 1) \over \lambda_i (e^{\lambda_i\Delta L} - 1)}$$.  
A noter que $$e^{\lambda_i\Delta}$$ peut être instable, et on calcule alors deux cas différents en fonction du signe de $$\mathrm{Re}(\lambda)$$ en introduisant un état intermédiaire $$\widetilde{x}_{k}$$.  
-	Si $$\mathrm{Re}(\lambda) \leq 0$$ : $$\widetilde{x}_{k} = e^{\lambda\Delta}\cdot \widetilde{x}_{k-1} + u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {(e^{\lambda\Delta} - 1) \over \lambda (e^{\lambda\Delta L} - 1) }$$

Et notamment si $$\mathrm{Re}(\lambda) \ll 0$$ alors $$\widetilde{x}_k \approx u_k$$ et $$x_k \approx u_k / \lambda$$, ce qui fait qu’on se focalise sur une information locale (les pas précédents sont ignorés)

-	Si $$\mathrm{Re}(\lambda) > 0$$ : 
$$\widetilde{x}_{k} = \widetilde{x}_{k-1} + e^{-k\lambda\Delta} \cdot u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {e^{\lambda\Delta (k-(L-1))} \over \lambda}\cdot {e^{-\lambda\Delta}-1 \over e^{-\lambda\Delta L} - 1 }$$

Et notamment si $$\mathrm{Re}(\lambda) \gg 0$$ alors $$\widetilde{x}_0 \approx u_0$$ et $$\widetilde{x}_k \approx \widetilde{x}_{k-1} \approx u_0$$, $$x_{k < L-1} \approx 0$$ et $$x_{L-1} \approx u_0 / \lambda$$, ce qui fait que le modèle peut capturer des informations à très longues distances.
En pratique, les auteurs du S4D indiquant que $$\mathrm{Re}(\lambda) \gg 0$$ ne fonctionne pas si $$L$$ est très grande (= ça explose quand $$t \rightarrow \infty$$ dans $$K(t) = C \exp(A^\intercal).B)$$.  

**3. Initialisation**  
Les parties réelles et imaginaires de w sont initialisées à partir de $$\mathcal{N}(0,1)$$, les éléments de $$\Delta_{\log}$$ à partir de $$\exp(\mathcal{U}~(log(0.001), log(0.1)))$$, et $$\Delta$$ via les valeurs propres de la matrice HiPPO. Les auteurs se demandent s’il ne serait pas possible de trouver une initialisation plus simple pour $$\Delta$$. Ils notent néanmoins qu’une aléatoire conduit à de moins bons résultats.  

Concernant les résultats, DSS a été testé sur LRA et Speech Commands :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results.png) |
|:--:|
| Figure 6 : *Résultats du DSS sur LRA* |

<br>
Le DSS (en version softmax ou en version exp) obtient de meilleurs résultats moyens que ce du S4 original sur ce benchmark. Le DSSsoftmax semble performer légèrement mieux que le DSSexp. Un intérêt également de ce papier est qu’il est le premier à reproduire les résultats du S4 et donc confirmer que les SSM passent ce benchmark.


| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results2.png) |
|:--:|
| Figure 7 : *Résultats du DSS sur Speech Commands* |

<br>
Sur Speech Commands, le S4 garde l’avantage sur les DSS.

<br><br>

### <span style="color: #51C353"> **S4D** </span>
Le 23 juin 2022, GU, GUPTA et al. introduisent le S4D dans leur article [*On the Parameterization and Initialization of Diagonal State Space Models*](https://arxiv.org/abs/2206.11893).  
L'initialisation de la matrice d'état $$A$$ du DSS repose sur une approximation particulière de la matrice HiPPO du S4. Si la matrice du S4 a une interprétation mathématique pour traiter les dépendances à longue portée, l'efficacité de l'approximation diagonale reste théoriquement inexpliquée.  
Avec le S4D, les auteurs introduisent un SSM diagonal combinant meilleur du calcul et de la paramétrisation de S4 et de l'initialisation de DSS, ce qui donne une méthode extrêmement simple, théoriquement fondée et empiriquement efficace.  

Une comparaison des trois méthodes est donnée par le tableau 1 du papier :  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4D_table1.png) |
|:--:|
| Figure 8 : *Comparaison du S4, du DSS et du S4D* |

<br>
Le S4D peut utiliser la discrétisation bilinéaire du S4 ou bien la discrétisation ZOH du DSS.  

Dans le S4D, le noyau de convolution $$K$$ de l’équation 
$$y = u \ast \mathbf{\overline{K}}$$ où $$\mathbf{\overline{K}} = (\mathbf{C}\mathbf{\overline{B}}, \mathbf{C}\mathbf{\overline{A}}\mathbf{\overline{B}}, \dots, \mathbf{C}\mathbf{\overline{A}}^{L-1}\mathbf{\overline{B}})$$, se calcule de la façon suivante : 
$$\mathbf{\overline{K}}_\ell = \sum_{n = 0}^{N-1} \mathbf{C}_n \mathbf{\overline{A}}_n^\ell \mathbf{\overline{B}}_n \implies \mathbf{\overline{K}} = (\mathbf{\overline{B}}^\top \circ \mathbf{C}) \cdot \mathcal{V}_L(\mathbf{\overline{A}})$$  où :  
• $$\circ$$ représente le [produit matriciel d’Hadamard]( https://fr.wikipedia.org/wiki/Produit_matriciel_de_Hadamard),  
• $$\cdot$$ un produit matriciel classique,  
• $$\mathcal{V}_L$$ est la [matrice de Vandermonde]( https://fr.wikipedia.org/wiki/Matrice_de_Vandermonde) c’est-à-dire : $$\mathcal{V} = 
\begin{bmatrix}
1&\alpha _{1}&{\alpha _{1}}^{2}&\dots &{\alpha _{1}}^{n-1}\\
1&\alpha _{2}&{\alpha _{2}}^{2}&\dots &{\alpha _{2}}^{n-1}\\
1&\alpha _{3}&{\alpha _{3}}^{2}&\dots &{\alpha _{3}}^{n-1}\\
vdots &\vdots &\vdots & \vdots \\
1&\alpha _{m}&{\alpha _{m}}^{2}&\dots &{\alpha _{m}}^{n-1}
\end{bmatrix}$$.

Autrement dit, pour tous $$i$$ et $$j$$, le coefficient en ligne $$i$$ et colonne $$j$$ est $$\displaystyle V_{i,j}={\alpha _{i}}^{j-1}$$.  

Au final, dans le S4D, 

$$
  \mathbf{\overline{K}} =
  \begin{bmatrix}
    \mathbf{\overline{B}}_0 \mathbf{C}_0 & \dots & \mathbf{\overline{B}}_{N-1} \mathbf{C}_{N-1}
  \end{bmatrix}
  \begin{bmatrix}
    1      & \mathbf{\overline{A}}_0     & \mathbf{\overline{A}}_0^2     & \dots  & \mathbf{\overline{A}}_0^{L-1}     \\
    1      & \mathbf{\overline{A}}_1     & \mathbf{\overline{A}}_1^2     & \dots  & \mathbf{\overline{A}}_1^{L-1}     \\
    \vdots & \vdots                 & \vdots                    & \ddots & \vdots                        \\
    1      & \mathbf{\overline{A}}_{N-1} & \mathbf{\overline{A}}_{N-1}^2 & \dots  & \mathbf{\overline{A}}_{N-1}^{L-1} \\
  \end{bmatrix}
\qquad \text{où } \mathcal{V}_L(\mathbf{\overline{A}})_{n, \ell} = \mathbf{\overline{A}}_n^\ell
$$.  
Le tout est calculable en $$\widetilde{O}(N+L)$$ opérations et espace.  

La paramétrisation des différentes matrices est la suivante :
-	$$\mathbf{A} = -\exp(\Re(\mathbf{A})) + i \cdot \Im(\mathbf{A})$$.  
Les auteurs indiquent qu’il est possible de remplacer l’exponentielle par n’importe quelles fonctions positives.
-	$$\mathbf{B} = 1$$ puis est entraîné
-	$$\mathbf{C}$$ aléatoire avec un écart-type de 1 puis entraîné.
  
Notons que le S4 prend en compte des réels alors que S4D des complexes en paramétrant avec une taille d’état de $$N/2$$ et en ajoutant implicitement les paires conjuguées aux paramètres. On a alors l’équivalent de $$N$$ paramètres réels assurant que la sortie est réelle.  

Concernant l’initialisation, les auteurs en introduisent deux :    
• **S4D-Inv** qui est une approximation de S4-LegS : $$\quad \mathbf{A}_n = -\frac{1}{2} + i \frac{N}{\pi} \left( \frac{N}{2n+1}-1 \right)$$  
• **S4D-Lin** qui est une approximation de S4-FouT : $$\quad \mathbf{A}_n = -\frac{1}{2}\mathbf{1} + i \pi n$$

Nous invitons le lecteur à consulter la partie 4 du papier pour plus de détails sur d’où sortent ces équations.  
D’un point de vue interprétabilité, la partie réelle de $$\mathbf{A}_n$$ contrôle le taux de décroissance des poids. La partie imaginaire de $$\mathbf{A}_n$$ contrôle quant à elle les fréquences d’oscillations de la fonction de base $$K_n(t) = e^{t\mathbf{A}}\mathbf{B}$$.  

Enfin les auteurs avancent quelques résultats :  
1)	Calculer le modèle avec une softmax au lieu de Vandermonde ne fait pas de grande différence  
2)	Entraîner B donne toujours de meilleurs résultats  
3)	Pas de différences notables entres les deux discrétisations possibles  
4)	Restreindre la partie réelle de A donne de meilleurs résultats (très légers cependant)  
5)	Toutes les modifications testées pour l’initialisation ont dégradé les résultats. A savoir appliquer un coefficient sur la partie imaginaire ou utiliser une partie imaginaire aléatoire, ou utiliser une partie réelle aléatoire, ou utiliser une partie imaginaire et une partie réelle aléatoire.  

Cette méthode étant très facile à implémenter par rapport aux autres (Vandermade se limitant à deux lignes de code), le S4D a remplacé le S4 dans les usages (on peut d’ailleurs observer des abus de langage où dans la table 6 du papier du Mamba par exemple, les auteurs utilisent le terme S4 pour désigner le S4D).
<br><br>

### <span style="color: #51C353"> **GSS** </span>

<br><br>

### <span style="color: #51C353"> **Liquid-S4** </span>

<br><br>

### <span style="color: #51C353"> **S5** </span>

<br><br>

### <span style="color: #51C353"> **Mega** </span>

Le 21 septembre 2022, MA, ZHOU et al., ont publié le [*Mega: Moving Average Equipped Gated Attention*](https://arxiv.org/abs/2209.10655).  
Le Mega est un transformer avec un mécanisme d'attention à une seule tête, utilisant le système de portes du GAU, et est équipé d'une moyenne mobile exponentielle (EMA) amortie pour incorporer le biais inductif positionnel.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega.png) |
|:--:|
| Figure X : *Vue d’ensemble du Mega, figure conçue à partir de ma compréhension du papier* |

<br>
Les auteurs proposent aussi une variante, Mega-chunk, qui divise efficacement l'ensemble de la séquence en plusieurs morceaux de longueur fixe. Cela offre une complexité linéaire en termes de temps et d'espace tout en n'entraînant qu'une perte de qualité minimale.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega-chunk.png) |
|:--:|
| Figure X : *Le Mega chunk* |

<br>
Cela offre une complexité linéaire appliquant simplement l'attention localement à chaque morceau de longueur fixe.  
Plus précisément, on divise les séquences de requêtes, de clés et de valeurs dans en morceaux de longueur $$c$$. Par exemple, $$\mathbf{Q} = {\mathbf{Q}_1, ... , \mathbf{Q}_k}$$, où $$k = -\frac{n}{c}$$ est le nombre de morceaux. L'opération d'attention est appliquée individuellement à chaque bloc, ce qui donne une complexité linéaire $$\mathcal{O}(kc^2) = \mathcal{O}(nc)$$ par rapport à $$n$$.  
Cette méthode souffre d'une limitation critique, à savoir la perte d'informations contextuelles provenant d'autres blocs. Mais la sous-couche EMA atténue ce problème en capturant les informations contextuelles locales à proximité de chaque token, dont les résultats sont utilisés comme entrées dans la sous-couche d'attention. Ainsi, le contexte effectif exploité par l'attention au niveau du bloc peut aller au-delà de la limite du bloc.  

Le Mega est extrêmement compétitif puisqu’il devient alors le meilleur modèle sur le LRA :  
 
| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_Mega.png) |
|:--:|
| Figure X : *Les résultats du Mega sur le benchmark LRA* |

<br>
Que fait donc un transformer dans un article de blog sur les SSM ? Intéressons-nous à l’EMA amortie pour comprendre le lien entre le Mega et le S4D.  

• <u>Rappel sur l’EMA « classique »</u> :  
L’équation d’une [EMA « classique »](https://fr.wikipedia.org/wiki/Moyenne_mobile#Moyenne_mobile_exponentielle)  est $$𝐲_t = 𝜶 ⊙ 𝐱_t + (1−𝜶) ⊙ 𝐲_{t−1}$$ avec $$𝜶$$ in $$[0,1]^d$$ le coefficient de l’EMA représentant le degré de diminution de la pondération et ⊙ le produit matriciel de Hadamard.  
Un 𝜶 plus élevé décote plus rapidement les observations le plus anciennes.  
On impose donc ici un biais inductif : le poids de la dépendance entre deux tokens diminue de manière exponentielle au fil du temps avec un facteur 𝜶 agnostique à l’entrée. Cette propriété favorise les dépendances locales et limite les dépendances à long terme.  
Le calcul de l’EMA peut être représenté comme n convolutions individuelles pouvant être calculées efficacement par FFT.  

• <u>EMA utilisée dans le Mega</u> :  
Le Mega utilise une EMA « amortie » multidimensionnelle. C’est-à-dire que dans l’équation de l’EMA « amortie », $$𝐲_t = 𝜶 ⊙ 𝐱_t + (1−𝜶 ⊙ 𝜹) ⊙ 𝐲_{t−1}$$ où un paramètre $$𝜹$$ in $$[0,1]^d$$ est introduit qui représente le facteur d’amortissement, $$x$$ est étendue à $$h$$ dimensions via une matrice d’expansion $$𝜷$$ in $$R^{d \times h}$$.  
L’équation devient alors $$𝐲_{t,j} = 𝜼_j^{\intercal} 𝐡_t^{(j)}$$ avec $$𝐡_t^{(j)} = 𝜶_j ⊙ 𝐮_t^{(j)}+ (1−𝜶_j ⊙ 𝜹_j) ⊙ 𝐡_{t−1}^{(j)}$$ et $$𝜼 \in R^{d \times h}$$ est la matrice de projection qui renvoie l’état caché en $$h$$ dimension à la sortie unidimensionnelle $$𝐲_{t,j} \in \mathbb{R}$$.  

Preuve que l’ EMA « amortie » multidimensionnelle peut être calculée comme une convolution et donc par FTT (en fixant $$d = 1$$ pour $$𝜶$$ et $$𝜹$$) :  
On a $$𝐲_t = 𝜼^{\intercal} 𝐡_t$$ avec $$𝐡_t = 𝜶_j ⊙ 𝐮_t + (1−𝜶 ⊙ 𝜹) ⊙ 𝐡_{t−1}$$. Notons $$ϕ = 1−𝜶 ⊙ 𝜹$$.  
Alors :  $$𝐡_t = 𝜶 ⊙ 𝐮_t + (1−𝜶 ⊙ 𝜹) ⊙ 𝐡_{t−1} = 𝜶 ⊙ 𝜷 𝐱_t + ϕ ⊙ 𝐡_{t−1}$$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; et $$𝐲_t = 𝜼^{\intercal} 𝐡_t = 𝜼^{\intercal} (𝜶 ⊙ 𝜷 𝐱_t + ϕ ⊙ 𝐡_{t−1})$$    
Ensuite, en déroulant les deux équations ci-dessus, on obtient explicitement :  
Etape 0 : $$𝐡_1 = 𝜶 ⊙ 𝜷𝐱_1 + ϕ ⊙ 𝐡_0$$  
Etape 1 : $$𝐡_2 = 𝜶 ⊙ 𝜷𝐱_2 + ϕ ⊙ 𝐡_1$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$= 𝜶 ⊙ 𝜷 𝐱_2 + ϕ ⊙ (ϕ ⊙ 𝐡_0 + 𝜶 ⊙ 𝜷 𝐱_1) = 𝜶 ⊙ 𝜷 𝐱_2 + ϕ^2 ⊙ 𝐡_0 + ϕ ⊙ 𝜶 ⊙ 𝜷 𝐱_1$$  
…  

Et de même :  
Etape 0 : $$𝐲_1 = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_1 + ϕ ⊙𝐡0) = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_1 + 𝜼^{\intercal} ϕ ⊙ 𝐡_0$$    
Etape 1 : $$𝐲_2 = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_2 + 𝜼^{\intercal} ϕ ⊙ 𝐡_1$$ 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $$= 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_2 + 𝜼^{\intercal} ϕ ⊙ (𝜶 ⊙ 𝜷 𝐱_1 + ϕ ⊙ 𝐡_0) = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_2 + 𝜼^{\intercal} ϕ ⊙ 𝜶 ⊙ 𝜷 𝐱_1 + 𝜼^{\intercal} ϕ^2 ⊙ 𝐡_0$$  
…  
Etape $$t$$ :  $$𝐲_t = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_t + … + 𝜼^{\intercal} ϕ_{t−1} ⊙ 𝜶 ⊙ 𝜷 𝐱_{t−1} + 𝜼^{\intercal} ϕ^t ⊙ 𝐡_0$$.  

Et donc $$𝐲 = 𝒦 * 𝐱 + 𝜼^{\intercal} ϕ^t ⊙ 𝐡_0$$ avec $$𝒦 = (𝜼^{\intercal} (𝜶 ⊙ 𝜷), 𝜼^{\intercal} (ϕ ⊙ 𝜶 ⊙ 𝜷), …, 𝜼^{\intercal}(ϕ^t ⊙ 𝜶 ⊙ 𝜷) \in \mathbb{R}^n$$.  
$$𝒦$$ est calculé dans le Mega via le produit de Vandermonde, ce qui nous rappelle la méthode utilisée dans le S4D.    
Pour plus de détails sur les liens entre le Mega et le S4, le lecteur est invité à consulter les messages échangés entre Albert Gu et les auteurs du Mega trouvables en commentaires de la soumission du Mega sur [Open Review](https://openreview.net/forum?id=qNLe3iq2El). Mais en résumé, en établissant un lien entre l’étape de discrétisation des SSM et l’EMA amortie, il est possible de voir le Mega vue comme un hybride SSM/Attention simplifiant le S4 pour qu'il soit à valeur réelle plutôt que complexe. 

<br><br>

### <span style="color: #51C353"> **SGConv** </span>

<br><br>

## <span style="color: #FFBF00"> **Applications des SSM** </span>


<br><br><br>

# <span style="color: #FF0000"> **Conclusion** </span>

<br><br><br>

# <span style="color: #FF0000"> **Références** </span>
