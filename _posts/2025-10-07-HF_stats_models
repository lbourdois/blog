---
title: "TEST"
tags:
  - Hugging Face
excerpt: "Divers –  Statistiques des modèles des 50 entités les plus téléchargées sur Hugging Face"
header:
    overlay_color: "#1C2A4D"
    teaser: "https://raw.githubusercontent.com/lbourdois/blog/refs/heads/master/assets/images/Agents/image_0.png"
author_profile: false
sidebar: false
classes: wide
layout: single
---

<style>
/* Forcer la largeur maximale pour le contenu */
.page__content {
    max-width: 100% !important;
    width: 100% !important;
}

/* S'assurer que le conteneur principal utilise toute la largeur */
.page {
    max-width: 100% !important;
    width: 100% !important;
}

/* Optimiser l'iframe pour la responsivité */
.full-width-iframe {
    width: 100%;
    height: 1200px;
    border: none;
    margin: 0;
    padding: 0;
}

.full-width-iframe-model_size {
    width: 100%;
    height: 500px;
    border: none;
    margin: 0;
    padding: 0;
}

.full-width-iframe-treemap {
    width: 100%;
    height: 800px;
    border: none;
    margin: 0;
    padding: 0;
}
  
/* Media query pour les écrans plus petits */
@media (max-width: 768px) {
    .full-width-iframe {
        height: 400px;
    }
}

h1 {
    color: red; /* Rouge */
    font-weight: bold;
}

h2 {
    color: #FFBF00; /* Jaune-orangé */
    font-weight: bold;
}

h3 {
    color: #51C353; /* Vert clair */
    font-weight: bold;
}

/* Styling for the Note Block (similar to the markdown !NOTE) */
.note-block {
            border-left: 5px solid #007bff; /* Blue line on the left */
            background-color: #e9f5ff; /* Light blue background */
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
            color: #333;
        }

/* The actual content of the note */
.note-content {
    line-height: 1.6;
    font-size: 20px; 
}

</style>

<div style="width: 100%; margin: 0; padding: 0;">

<h1>Avant-propos</h1>
<p>Dans ce qui suit, les textes dans un encadré bleu expriment uniquement mon opinion personnelle, là où dans le reste je me limite à décrire les graphiques.<br>

A noter que pour les lecteurs sur téléphone, les figures 3, 4, 13, 14 et 19 s'afficheront mal (encore des choses à apprendre dans plotly visiblement).</p>

<br><br><br>

<h1>Introduction</h1>
<p>Dans cet article de blog, nous nous intéressons à analyser les <strong>modèles open-source les plus impactants en pratique</strong>.
Pour cela nous nous focalisons sur une métrique très pragmatique : « quels sont les modèles les plus téléchargés sur le Hub d'Hugging Face ? ».
L'hypothèse étant que des modèles massivement téléchargés sont ceux qui sont utilisés dans le monde réel.
C'est une approche qui se veut également plus juste vis-à-vis des individus/organisations qui n'ont pas un service de communication ou n'étant pas suivis/likés massivement sur le Hub.</p>

<br><br><br>

<h1>Données collectées</h1>
<p>Les données visibles dans cet article ont été collectées le 1er octobre 2025.<br>
Après avoir établi les 50 entités open-source comptant le plus de téléchargements, nous avons collecté tous les modèles associés à celles-ci. 
Cela représente 72 423 modèles sur les 2 126 833 hébergés sur le Hub soit 3,41% du total.<br>
Ces comptes représentent exactement 36 450 707 797 téléchargements sur les 45 438 836 957 totaux soit 80,22%.<br><br>

Pour chacun des modèles, les tags <code>pipeline</code> et de <code>language</code> ont également été recueillis lorsqu'ils étaient disponibles. 
De même, la taille estimée d'après le fichier <code>.safetensors</code>.<br><br>

Quand des informations étaient manquantes (pas de tags ou de taille de modèle notamment), 
nous avons manuellement corrigé les données en consultant la carte de modèle ou les publications des 1000 modèles open-source les plus téléchargés. 
Ces derniers représentant à eux seuls 77,89% de tous les téléchargements du Hub et 97,10% des 50 entités analysées.<br>

Le tout a finalement été stocké dans un dataframe ressemblant à ceci :</p>

<iframe
  src="https://huggingface.co/datasets/lbourdois/huggingface_downloads_October_2025"
  width="100%"
  height="600"
  frameborder="0"
></iframe>
<center><i>Figure 1: Jeu de données utilisé, disponible  <a href="https://huggingface.co/datasets/lbourdois/huggingface_downloads_October_2025" target="_blank">ici</a></i></center>

<br><br>

<p>C'est à partir de celui-ci que nous générons l'ensemble des graphiques visibles ci-après.<br><br>

Les montants sont arrondis au million le plus proche pour une question de visibilité mais aussi parce que le Hub rencontre quelques soucis d'affichage. 
En effet, par exemple pour le modèle <a href="https://huggingface.co/sentence-transformers/static-retrieval-mrl-en-v1">sentence-transformers/static-retrieval-mrl-en-v1</a>, aucun téléchargement n'est <a href="https://huggingface.co/docs/hub/models-download-stats">disponible</a>. 
L'objectif ici est surtout de connaître les ordres de grandeurs plutôt que de s'intéresser aux nombres à l'unité près.</p>

<figure>
    <img src="https://cdn-uploads.huggingface.co/production/uploads/613b0a62a14099d5afed7830/bJTNpy1u_HxRw3AfpTepK.png" alt="Download stats issue example" style="max-width:70%;">
</figure>
<center><i>Figure 2: Exemple de ce que l'on peut observer pour les modèles où le Hub n'affiche pas correctement le nombre de téléchargement</i></center>

<br><br><br>

<h1>Graphiques</h1>

<h2>Aperçu global</h2>
<p>Dans cette première section, nous affichons les téléchargements globaux de chacune des 50 principales entités contribuant à l'open-source, 
ainsi que leur type de catégorie et leur pays d'origine. Nous revenons sur ces deux derniers points dans des sections dédiées.<br><br>

Notons que nous utilisons le terme « entité » et pas « compte » (Hugging Face) car une entité peut être composée de plusieurs comptes. 
Par exemple Google est composée de <a href="https://huggingface.co/google">google</a>, <a href="https://huggingface.co/google-bert">google-bert</a>, <a href="https://huggingface.co/google-t5">google-t5</a> et <a href="https://huggingface.co/albert">albert</a>.<br>
Ainsi, nous proposons un graphique <em>global</em> permettant de comparer entre-elles les entités sont les plus téléchargées et un autre <em>par sous-comptes</em> pour visualiser comment se répartissent les différents comptes en leur sein.</p>

<br>

<b>Vision globale</b>

<br><br>

<iframe
    class="full-width-iframe"
    src="{{ '/assets/images/hf_models_stats/huggingface_downloads.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 3: Top 50 des entités par nombre total de téléchargements</i></center>

<br><br><br>

<b>Vision par sous-comptes</b>

<br>

<iframe
    class="full-width-iframe"
    src="{{ '/assets/images/hf_models_stats/huggingface_downloads_breakdown.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 4: Top 50 des entités par nombre total de téléchargements (détaillé au niveau des sous-comptes)</i></center>

<br><br>

<details>
    <summary>Un mot sur chacune des entités</summary>
    <p>1. L'entitée Google est composée de <a href="https://huggingface.co/google">google</a>, <a href="https://huggingface.co/google-bert">google-bert</a>, <a href="https://huggingface.co/google-t5">google-t5</a> et <a href="https://huggingface.co/albert">albert</a>. Plus de 74% de ses téléchargements proviennent de « vieux modèles » à savoir 64% de BERT, 6,8% de T5 et 3,2% d'ALBERT.</p>
    <p>2. L'entitée Meta est composée de <a href="https://huggingface.co/FacebookAI">FacebookAI</a>, <a href="https://huggingface.co/facebook">facebook</a> et <a href="https://huggingface.co/meta-llama">meta-llama</a>. Observations similaires que pour Google mais dans une moindre mesure avec 48,3% des téléchargements provenant de ses RoBERTa contre 9% pour les Llamas.</p>
    <p>3. L'entitée Sentence-transformers (de l'<em>Ubiquitous Knowledge Processing Lab</em> à Darmstadt en Allemagne et plus précisément les travaux de Nils Reimers) complète le trio. Cette entitée est composée de <a href="https://huggingface.co/sentence-transformers">sentence-transformers</a> et <a href="https://huggingface.co/cross-encoder">cross-encoder</a>. Le compte <code>sentence-transformers</code> est d'ailleurs le plus téléchargé de tout Hugging Face.</p>
    <p>4. L'entitée Hugging Face est composée de <a href="https://huggingface.co/timm">timm</a> (52,4% des téléchargements), de <a href="https://huggingface.co/distilbert">distilbert</a> (44,6%), <a href="https://huggingface.co/llava-hf">llava-hf</a>, <a href="https://huggingface.co/HuggingFaceTB">HuggingFaceTB</a> et <a href="https://huggingface.co/HuggingFaceM4">HuggingFaceM4</a>.</p>
    <p>5. L'entitée OpenAI est composée d'<a href="https://huggingface.co/openai">openai</a> (72%) et <a href="https://huggingface.co/openai-community">openai-community</a> (28%). Bien qu'elle publie peu en open-source, l'entreprise est extrêmement impactante lorsqu'elle le fait (ses CLIP et Whisper sont particulièrement téléchargés).</p>
    <p>6. 99% des téléchargements du MIT proviennent de son modèle <a href="https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593">ast-finetuned-audioset-10-10-0.4593</a> qui est le deuxième modèle le plus téléchargé de tout le Hub.</p>
    <p>7. Microsoft est une autre Big Tech populaire. Le graphique de la section sur les modalités montre que c'est l'organisation la plus diversifiée en termes de modalité adressée (là où, à l'exception de quelques autres, la plupart des entités du top 50 sont hyper spécialisées sur une modalité donnée).</p>
    <p>8. Jonatas Grosman est l'individuel le plus téléchargé. Avec à peine 300 followers sur le Hub, il est une illustration du fait que ce ne sont pas forcément les entités les plus suivies / likées qui sont les plus impactantes. Il s'est un temps spécialisé dans le finetuning de wav2vec2 mais ne publie plus de nouveaux modèles depuis 3 ans maintenant.</p>
    <p>9. Pyannote est spécialisée dans des petits modèles de segmentation et diarization d'audio.</p>
    <p>10. 99% des téléchargements de Falcons.ai (350 followers) proviennent de son modèle <a href="https://huggingface.co/Falconsai/nsfw_image_detection">nsfw_image_detection</a> qui est le septième modèle le plus téléchargé de tout le Hub.</p>
    <p>11. BAAI est l'entitée chinoise la plus téléchargée sur Hugging Face notamment via ses modèles <a href="https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d">bge</a>. C'est aussi l'entitée non-profit la plus téléchargée.</p>
    <p>12. L'entitée Alibaba est composée de <a href="https://hf.co/Qwen">Qwen</a> (81,5%), <a href="https://hf.co/Alibaba-NLP">Alibaba-NLP</a> (9,5%), <a href="https://hf.co/thenlper">thenlper</a> (7,7%), <a href="https://hf.co/Wan-AI">Wan-AI</a>, <a href="https://hf.co/AIDC-AI">AIDC-AI</a>, <a href="https://hf.co/alibaba-pai">alibaba-pai</a>, <a href="https://hf.co/alibaba-damo">alibaba-damo</a>. Elle est ainsi principalement téléchargée pour ses modèles Qwen (surtout le <a href="https://hf.co/Qwen/Qwen2.5-1.5B-Instruct">Qwen2.5-1.5B-Instruct</a> qui représente 20% de ses téléchargments et est le LLM le plus téléchargé du Hub).</p>
    <p>13. L'entitée Amazon est composée d'<a href="https://hf.co/amazon">amazon</a> (80,1%) et d'<a href="https://hf.co/autogluon">autogluon</a> (19,9%). C'est la seule entitée dont les téléchargements portent massivement sur les séries temporelles.</p>
    <p>14. Dima806 est principalement téléchargé pour son modèle <a href="https://huggingface.co/dima806/fairface_age_image_detection">dima806/fairface_age_image_detection</a>. C'est un individuel encore actif en 2025.</p>
    <p>15. Cardiffnlp est téléchargée pour ses nombreux modèles de classification de sentiment.</p>
    <p>16. L'entitée Stability AI est composée de <a href="https://hf.co/stabilityai">stabilityai</a> (80%) et de <a href="https://hf.co/stable-diffusion-v1-5">stable-diffusion-v1-5</a> (20%). Elle est principalement téléchargée pour ses différentes version de Stable diffusion.</p>
    <p>17. L'entitée Maziyar Panahi est composée de <a href="https://hf.co/MaziyarPanahi">MaziyarPanahi</a> (80,7%) qui est son compte individuel où il propose des version GGUF de LLM, et d'<a href="https://hf.co/OpenMed">OpenMed</a> (19,3%) qui est une organisation non-profit qu'il a créé et dédiée aux modèles médicaux. C'est un individuel encore actif en 2025.</p>
    <p>18. Helsinki-NLP est téléchargée pour ses nombreux modèles de traduction automatique.</p>
    <p>19. Laion est principalement téléchargée pour ses nombreux modèles reproduisant <a href="https://huggingface.co/collections/laion/openclip-laion-2b-64fcade42d20ced4e9389b30">CLIP</a>.</p>
    <p>20. Juan Manuel Pérez via l'organsiation <code>pysentimiento</code> est principalement téléchargé pour ses modèles de classification de sentiment en espagnol. Il n'a plus publié de modèles depuis 2023.</p>
    <p>21. Bingsu est principalement téléchargé pour les différents yolo contenus dans <a href="https://huggingface.co/Bingsu/adetailer">Bingsu/adetailer</a>. C'est un individuel encore actif en 2025.</p>
    <p>22. La moitié des téléchargements d'AllenAI provient de son <a href="https://huggingface.co/allenai/longformer-base-4096">longformer</a>.</p>
    <p>23. Tohoku-nlp est un groupe universitaire téléchargé majoritairement pour <a href="https://huggingface.co/tohoku-nlp/bert-base-japanese">sa version japonaise de BERT</a>.</p>
    <p>24. Manuel Romero est massivement téléchargé pour ses finetunings de t5 mais surtout son modèle financier <a href="https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis">distilroberta-finetuned-financial-news-sentiment-analysis</a>. C'est un individuel encore actif en 2025.</p>
    <p>25. Mistral AI est principalement téléchargé pour les versions 7B instruct de ses modèles.</p>
    <p>26. Les téléchargements de Prajjwal1 reposent sur des convertions en Pytorch de bert-tiny (4M de paramètres) et bert-small (29M). Il n'a plus publié de modèles depuis 2023.</p>
    <p>27. Deepset est téléchargé pour son modèle de <a href="https://huggingface.co/deepset/roberta-base-squad2">QA en anglais</a>.</p>
    <p>28. Salesforce connait du succès avec ses différentes versions de son modèle <a href="https://huggingface.co/collections/Salesforce/blip-models-65242f40f1491fbf6a9e9472">BLIP</a>.</p>
    <p>29. Intfloat est considéré comme un individuel chinois puisque Liang Wang a décidé de publier son travail sous son nom sur Hugging Face. En pratique, il s'agit des modèles <a href="https://huggingface.co/collections/intfloat/multilingual-e5-text-embeddings-67b2b8bb9bff40dec9fb3534">e5</a> créés dans le cadre de son travail à Microsoft Asia. C'est un individuel encore actif en 2025.</p>
    <p>30. TheBloke est connu pour proposer des versions quantifiées de modèles. Sa version ayant eu le plus de succès est le <a href="https://huggingface.co/TheBloke/phi-2-GGUF">phi-2-GGUF</a>. Il n'a plus publié de modèles depuis 2024.</p>
    <p>31. CompVis est populaire pour son modèle <a href="https://huggingface.co/collections/CompVis/stable-diffusion-safety-checker">stable-diffusion-safety-checker</a>.</p>
    <p>32. CIDAS est très utilisé pour son modèle de segmentation <a href="https://huggingface.co/collections/CIDAS/clipseg-rd64-refined">clipseg-rd64-refined</a>.</p>
    <p>33. Emily Alsentzer a connu un certain succès avec son <a href="https://huggingface.co/collections/emilyalsentzer/Bio_ClinicalBERT">Bio_ClinicalBERT</a>. Elle n'a plus publié de modèles depuis 2020.</p>
    <p>34. NVIDIA est extrêmement équilibrée (ce n'est pas un unique modèle qui tire tous les téléchargements de l'entité). Son modèle <a href="https://huggingface.co/collections/nvidia/speakerverification_en_titanet_large">speakerverification_en_titanet_large</a> se démarque légèrement des autres.</p>
    <p>35. LM Studio est composée de <a href="https://huggingface.co/lmstudio-community">lmstudio-community</a> (44,1%), <a href="https://huggingface.co/bartowski">bartowski</a> (55,8%) et <a href="https://huggingface.co/lmstudio-ai">lmstudio-ai</a>.</p>
    <p>36. David S. Lim est téléchargé pour ses modèles de NER en anglais. Il n'a plus publié de modèles depuis 2024.</p>
    <p>37. Unsloth est principalement téléchargé pour ses versions quantifiées de LLM.</p>
    <p>38. mradermacher est principalement téléchargé pour ses versions quantifiées de LLM. C'est un groupe d'individuels encore actif en 2025.</p>
    <p>39. Moritz Laurer est principalement téléchargé pour ses modèles de NLI, notamment <a href="https://hf.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli">DeBERTa-v3-base-mnli-fever-anli</a>. Il n'a plus publié de modèles depuis 2024.</p>
    <p>40. Jean-Baptiste Polle est téléchargé pour ses modèles de NER en français. Il n'a plus publié de modèles depuis 2023.</p>
    <p>41. HFL est un laboratoire hybride (université x entreprise) téléchargé majoritairement pour <a href="https://huggingface.co/hfl/chinese-macbert-base">sa version chinoise de BERT</a>.</p>
    <p>42. Deepseek est assez équilibrée avec une petite prévalence des modèles <a href="https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d">R1</a>.</p>
    <p>43. Big Science est un projet collaboratif international (porté par Hugging Face et le CNRS) par ses modèles bloom, bloomz et t0.</p>
    <p>44. Flair est principalement téléchargé pour ses modèles de NER monolingue dans plusieurs langues.</p>
    <p>45. Sam Lowe est principalement téléchargé pour son modèle de classification d'emotions. Il n'a plus publié de modèles depuis 2023.</p>
    <p>46. Patrick John Chia, dont nous n'avons pas pu déterminer la nationalité, est connu pour son modèle <a href="https://huggingface.co/patrickjohncyh/fashion-clip">fashion-clip</a>. Il n'a plus publié de modèles depuis 2023.</p>
    <p>47. Almanach est un groupe universitaire téléchargé majoritairement pour <a href="https://huggingface.co/almanach/camembert-base">sa version française de BERT</a>.</p>
    <p>48. Supabase est principalement téléchargé pour son modèle <a href="https://huggingface.co/Supabase/gte-small">gte-small</a>.</p>
    <p>49. Jina AI est principalement téléchargé pour ses modèles d'embeddings.</p>
    <p>50. colbert-ir est principalement téléchargé pour son modèle <a href="https://huggingface.co/colbert-ir/colbertv2.0">colbertv2.0</a>.</p>
</details>

<br><br><br>

<h2>Vision par pays d'origine du compte</h2>
<p>Pour le pays d'origine, nous nous intéressons dans cette version, aux localisations de l'individu et du siège social de son entreprise. 
Le but ici est d'estimer le nombre de pays disposant d'un environnement permettant de créer les modèles les plus téléchargés.</p>

<br>

<h3>Vision par pays (individuels)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_countries.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 5 : Total des téléchargements par pays (pays individuels)</i></center>

<br><br>

<p>
Avec plus de 20,6 milliards de téléchargements, les États-Unis représentent 56,4% des téléchargements des 50 entités open-source les plus téléchargées sur Hugging Face. Ils sont notamment portés par les Big Tech et leur densité importante regroupant 18 entités sur les 50.<br>
Quatre fois moins importante, l'Allemagne est deuxième avec 13,2% du total avec 4,8 milliards de téléchargements porté à 79% par sentence-transformers. Elle regroupe 8 entités sur les 50.<br>
La France est troisième avec 3,4 milliards de téléchargements représentant 9,3% du total. Présente avec 5 entités, sa répartition des contributions des différents acteurs est un peu plus équilibrée.<br>
En quatrième position, la Chine représente 1,9 milliard de téléchargements, soit 5,2% du total, présente elle aussi avec 5 entités (4 si on considère `intfloat` comme lié à Microsoft).<br>
A l'exception du Royaume-Uni composé également de 4 entités, tous les autres pays sont représentés par une seule entité.  
</p>

<br>

<div class="note-block">
<div class="note-content">
    Il peut être étonnant d'observer que la Chine n'est que quatrième en termes de téléchargements. Cela peut s'expliquer par le fait qu'Hugging Face n'y est pas accessible, privant les entités de ce pays d'une comptabilisation des usagers locaux en plus des internationaux.
    <br><br>
    Dans la suite du post, l'analyse des chiffres collectés permet de mettre en avant d'autres explications possibles à cette observation (voir la section sur la taille des modèles ou celle sur les modalités).
</div>
</div>

<br><br>

<h3>Vision par pays (UE regroupée)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_eu.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 6 : Total des téléchargements par pays (UE regroupée)</i></center>

<br><br>

<p>En regroupant les pays de l'Union Européenne (13 entités), leur part dans le total des téléchargements passe alors à 24 %.</p>

<br><br>

<h3>Vision par pays (continents)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_continental.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 7 : Total des téléchargements par pays (continents)</i></center>

<br><br>

<p>Une comparaison au niveau des continents permet de mettre en évidence que l'Amérique du Nord concentre 56,7% des téléchargements des modèles open-source du top 50 (et 19 entités), l'Europe 29% (20 entités), l'Asie 8,9% (8 entités) et l'Amérique du Sud 4,9% (2 entités). Le reste étant indéterminé ou composé d'initiatives internationales (2 entités).</p>

<br><br>

<h2>Vision par type d'entités</h2>

<br>

<p>
    Le type de l'entité est déterminé via ce qu'elle a elle-même choisi parmi les possibilités offertes par Hugging Face lors de la <a href="https://huggingface.co/organizations/new" target="_blank">création d'une organisation</a> (<em>Compagny</em>, <em>University</em>, <em>Classroom</em>, <em>Non-profit</em>, <em>Governement</em>, <em>Community</em>). Nous y avons ajouté <em>Individual</em> pour désigner les individus proposant des modèles sous leur propre nom en dehors d'une organisation. À noter également un particulier, HFL, qui est un laboratoire commun entre une université et une entreprise. Nous avons donc dû créer une catégorie <em>Hybrid Lab</em> non disponible sur Hugging Face.
</p>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_entity_type.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 8 : Total des téléchargements par type d'entité (continents)</i></center>

<br><br>

<p>Les entreprises (20 entités sur les 50) représentent 63,2% des téléchargements de modèles open-source du top 50, les universités (10 entités) 20,7%, les individuels (16 entités sur) 12,1%, les structures non-profit (4 entités) 3,8% et les laboratoires hybrides 0,3% (1 entité).</p>

<div class="note-block">
<div class="note-content">
En consultant l'activité des différentes entités sur le Hub, on peut constater que globalement la plupart d'entre elles sont encore actives en 2025. Une exception notable est la catégorie <i>Individual</i>. En effet, sur les 16 entités de ce type, seulement 6 ont publié de nouveaux modèles en 2025.<br>
Il apparaît donc que contrairement aux autres catégories de types, la contribution d'individuels à l'open-source n'est pas une activité pérenne dans le temps. Ce phénomène peut être compensé par un renouvellement des contributeurs, néanmoins il serait intéressant de connaître les raisons pour lesquelles ces personnes se détournent de l'open source pour trouver une façon de les soutenir et remédier à ce problème.<br><br>
La forte part des entreprises dans le modèle open-source pourrait être aussi une vulnérabilité du modèle si ces dernières décident à l'avenir de se retirer et de ne plus y contribuer comme certaines en ont fait le choix par le passé.
</div>
</div>

<br><br>

<h2>Croisement type x pays</h2>

<br>

<h3>Vision par pays (individuels)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_entities_countries_individuals.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 9 : Total des téléchargements par type d'entité (pays individuels)</i></center>

<br><br>

<p>
Les États-Unis sont présents dans les 4 types de catégories : premier de très loin chez les entreprises avec 76,3% des téléchargements, deuxième chez les universités avec 30% de la catégorie, troisième à 12,5% des individuels et troisième des non-profit à 15,3%.<br>
L'Allemagne est également dans 4 types de catégories : sixième chez les entreprises avec 1,4% des téléchargements, première chez les universités avec 54,1%, à 2,5% chez les individuels et deuxième à 20,3% chez les non-profit.  <br>
La Chine est également dans 4 types de catégories : quatrième des entreprises à 3,7%, absente des universités (seules) mais seul cas de laboratoire hybride université/entreprise dans ce top 50, à 3,6% chez les individuels (le cas `intfloat`) et enfin première chez les non-profit à 57,5%.  <br>
La France est présente dans 3 types de catégories : deuxième des entreprises avec 12,7%, sixième des universités à 1,2% et quatrième des individuels à 8,9%.  <br>
Le Royaume-Uni est classé dans 3 types de catégories : cinquième chez les entreprises avec 1,9% des téléchargements, troisième chez les universités avec 7,4% et à 5,4% chez les individuels.   <br>
Tous les autres pays ne sont représentés qu'une seule fois dans une catégorie spécifique.
</p>

<br>

<div class="note-block">
<div class="note-content">
    Non appliqué dans cette première version, un système de pondération des différents montants avancés serait à appliquer. En effet, le nombre de téléchargements d'un pays peut être influencé par la taille de sa population, le nombre d'entreprises dans le pays, le taux d'usage de l'IA dans la population, etc.  
    <br><br>
    C'est pourquoi, par exemple, nous proposons en plus des graphiques individuels, des graphiques au niveau de l'UE ou des continents. 
    <br><br>
    De plus, des échanges ont lieu avec les équipes d'Hugging Face pour essayer de connaître la localisation des téléchargements des modèles, pour ainsi déterminer ce qui révèle de la « consommation intérieure » et de l'« export ».
</div>
</div>

<br>

<h3>Vision par pays (UE regroupée)</h3>

<br>


<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_entities_countries_eu.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 10 : Total des téléchargements par type d'entité (UE regroupée)</i></center>

<br><br>

<p>Dans cette configuration, l'Union Européenne est deuxième chez les entreprises open-source à 14,1%, première chez les universités à 60%, troisième chez les individuels à 15,6% et deuxième chez les non-profit à 20,3%.</p>

<br>

<h3>Vision par pays (continents)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_entities_countries_continents.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 11 : Total des téléchargements par type d'entité (continents)</i></center>

<br><br>

<p>
Au niveau des continents, le positionnement de l'Amérique du Nord est sensiblement le même que celui listé pour les États-Unis. Seule la part des individuels passe de 12,5 à 14,4% en comptabilisant le Canada.  <br>
L'Europe bénéficie alors des chiffres du Royaume-Uni et de l'Ukraine, donnant, pour les entreprises deuxième à 16%, première des universités à 67,4%, deuxième des individuels à 34,3% et deuxième des non-profit toujours à 20,3%.  <br>
L'Asie est troisième des entreprises à 7,7%, troisième des universités à 2,6%, quatrième des individus à 8,6%, première chez les non-profit à 57,4% et seule sur le segment des laboratoires hybrides.    <br>
Notons finalement que l'Amérique du Sud se démarque par des contributions individuelles où elle est première à 40,7% du total.
</p>


<br><br>

<h2>Vision par modalité</h2>

<br>

<h3>Vision globale</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_treemap_modalities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 12 : Contributions des entités aux modalités</i></center>

<br><br>

<p>
On peut constater que le NLP est la modalité la plus téléchargée parmi ce top 50 avec 58,1% des téléchargements, suivie par la CV à 21,2% et l'audio à 15,1%.<br>  
La modalité « <i>Unknow</i> » regroupe tous les modèles dont le tag de langue n'était pas spécifié et n'a pas pu être corrigé.</p>

<div class="note-block">
<div class="note-content">
Le nombre de téléchargements d'une modalité semble lié au fait ou non qu'Hugging Face intègre également un modèle de la modalité en question dans sa librairie <code>transformers</code>. 
    <br><br>
Le NLP apparaît ainsi plutôt favorisé (Hugging Face étant à la base connu pour cette modalité). Il n'est pas évident que les praticiens des autres modalités utilisent cette librairie pour leur cas d'usage. Par exemple, il existe plusieurs alternatives en vision avec les outils d'open-mmlab, de roboflow, etc. Dans la modalité vision, le modèle le plus téléchargé étant CLIP, intégré dans <code>transformers</code>.
</div>
</div>

<br>

<h3>Vision par entité</h3>

<br>

<p>Dans le graphique suivant, nous affichons pour chacune des entités, la part de chacune des modalités dans ses téléchargements.</p>

<iframe
    class="full-width-iframe"
    src="{{ '/assets/images/hf_models_stats/huggingface_downloads_modalities_breakdown.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 13 : Top 50 des entités Hugging Face par nombre total de téléchargements avec décomposition par modalité</i></center>

<br>

<p>
Nous observons que peu d'entités sont diversifiées. Chacune semble avoir une spécialité.<br> 
32 font majoritairement du NLP, 10 de la vision (paradoxalement Hugging Face avec le rachat de <code>timm</code> et OpenAI par conséquence de n'avoir mis de modèles de NLP en open-source depuis le gpt2 avant août 2025), 4 d'audios, 2 de multimodal nlp/vision, 1 de séries temporelles et 1 non déterminé (`mradermacher` proposant des versions quantifiées de modèles, cela serait vraisemblablement du NLP).
</p>

<br>

<h3>Vision par sous-comptes</h3>

<br>

<p>Le graphique ci-dessous donne un peu plus de détails en affichant les répartitions par sous-comptes de chacune des entités.</p>

<iframe
    class="full-width-iframe"
    src="{{ '/assets/images/hf_models_stats/huggingface_downloads_modalities_breakdown_sub_accounts.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 14 : Top 50 des entités Hugging Face par nombre total de téléchargements avec décomposition par modalité (et par sous-comptes)</i></center>

<br>

<h3>Vision par pays (individuels)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_individuals_to_modalities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 15 : Contribution des pays (individuels) aux modalités</i></center>

<br>

<p>
On peut remarquer que les États-Unis sont premiers dans la totalité des modalités. Excepté en vision où ils n’obtiennent « que » 46,6% de la modalité, ils captent la majorité dans l'ensemble de celles-ci.  <br>
La France est quant à elle placée en deuxième ou troisième position des modalités NLP, vision et audio.<br>
La Chine pour sa part est absente des modalités CV et audio. Cela pourrait expliquer l'observation faite précédemment. Elle n'est « que » quatrième dans l'ensemble des téléchargements ou plutôt que l'Allemagne (présente en NLP et vision) et la France comptabilisent plus de téléchargements.
</p>

<br>

<h3>Vision par pays (UE regroupée)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_eu_to_modalities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 16 : Contribution des pays (UE regroupée) aux modalités</i></center>

<br>

<p>L'Union Européenne se positionne alors en deuxième ou troisième position des modalités NLP, vision et audio.</p>

<br>

<h3>Vision par pays (continents)</h3>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_continental_to_modalities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 17 : Contribution des pays (continents) aux modalités</i></center>

<br><br>

<h2>Vision par tâche</h2>

<br>

<p>Les graphiques des modalités ont pu être obtenus à partir des pipelines de tags en utilisant les associations disponibles dans le dictionnaire suivant.</p>

<details>
  <summary>Afficher le dictionnaire</summary>
  <pre><code>
pipeline_tag_to_modality = {
    'none': 'unknown',
    'feature-extraction': 'natural language processing',
    'text-ranking': 'natural language processing',
    'text2text-generation': 'natural language processing',
    'text-generation': 'natural language processing',
    'text-classification': 'natural language processing',
    'summarization': 'natural language processing',
    'image-classification': 'computer vision',
    'token-classification': 'natural language processing',
    'fill-mask': 'natural language processing',
    'sentence-similarity': 'natural language processing',
    'image-text-to-text': 'multimodal-nlp/cv',
    'video-text-to-text': 'multimodal-nlp/cv',
    'image-to-text': 'multimodal-nlp/cv',
    'audio-text-to-text': 'multimodal-nlp/audio',
    'text-to-speech': 'multimodal-nlp/audio',
    'text-to-audio': 'multimodal-nlp/audio',
    'text-to-image': 'multimodal-nlp/cv',
    'visual-question-answering': 'multimodal-nlp/cv',
    'text-to-video': 'multimodal-nlp/cv',
    'visual-document-retrieval': 'multimodal-nlp/cv',
    'automatic-speech-recognition': 'audio',
    'any-to-any': 'multimodal',
    'video-to-video': 'computer vision',
    'image-to-image': 'computer vision',
    'translation': 'natural language processing',
    'image-to-video': 'computer vision',
    'video-to-video': 'computer vision',
    'image-feature-extraction': 'computer vision',
    'zero-shot-classification': 'natural language processing',
    'zero-shot-image-classification': 'computer vision',
    'zero-shot-object-detection': 'computer vision',
    'question-answering': 'natural language processing',
    'voice-activity-detection': 'audio',
    'audio-classification': 'audio',
    'audio-to-audio': 'audio',
    'text-to-3d': 'multimodal-nlp/cv',
    'reinforcement-learning': 'robotic',
    'robotics': 'robotic',
    'image-segmentation': 'computer vision',
    'unconditional-image-generation': 'computer vision',
    'mask-generation': 'computer vision',
    'object-detection': 'computer vision',
    'depth-estimation': 'computer vision',
    'keypoint-detection': 'computer vision',
    'video-classification': 'computer vision',
    'tabular-classification': 'tabular',
    'tabular-regression': 'tabular',
    'time-series-forecasting': 'tabular',
    'image-to-3d': 'computer vision',
    'multiple-choice': 'natural language processing',
    'table-question-answering': 'natural language processing',
    'video-text-to-text': 'multimodal-nlp/cv',
}
</code></pre>
</details>

<br>

<p>Dans cette section, nous affichons les différentes tâches par modalité afin d’obtenir un meilleur aperçu des plus populaires.</p>

<br>

<h3>Vision globale</h3>

<br>

<p>Les tâches de NLP sont différentes variantes de bleu, celles de vision des variantes de jaune, et celles d'audio des variantes de rouge.<br>
Sur conseil de Tom Aarseen, nous avons fusionné les tags de pipeline <code>sentence-similarity</code> et <code>feature-extraction</code>.<br>
Enfin nous n'avons pas regroupé <code>text2text-generation</code> avec <code>text-generation</code>. Le premier tag est surtout utilisé par les modèles encodeurs-décodeurs générant du texte (T5, BART) alors que le second est utilisé par les modèles uniquement décodeur. L'objectif ici était de montrer que ces encodeurs-décodeurs représentent autant de téléchargements que les Qwen d'Alibaba et davantage que les Llamas de Meta.</p>

<br>

<iframe
    class="full-width-iframe-treemap"
    src="{{ '/assets/images/hf_models_stats/huggingface_entities_to_tasks.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 18 : Contribution des entités aux tâches</i></center>

<br>

<p>Les modèles de base purement encodeurs visibles via la tâche <code>fill-mask</code> sont de loin les modèles les plus téléchargés avec 22,3%. Leur finetunings représentent 22,7% des téléchargements, principalement sur la tâche de <code>sentence-similarity</code> (à laquelle nous avons hésité de fusionner aussi celle de <code>text-ranking</code>) puis de <code>text-classification</code>, de <code>token-classification</code>, <code>zero-shot-classication</code> et enfin de <code>question-answering</code>.<br>
Les modèles de génération purement décodeur représentent 9,5% des téléchargements.<br>
Les modèles encodeur-décodeur représentent 1,4% auxquels on peut ajouter la tâche de <code>traduction</code> à 1,6% pour lesquels ils sont principalement utilisés.</p><br>

<p>Viens ensuite la vision avec 11,1% des téléchargements en <code>classification</code>, 6,4% en <code>zero-shot-classification</code> (les CLIP et dérivés), 1,8% en <code>image-feature-extraction</code>, 0,9% en <code>détection d'objet</code> et 0,5% en <code>segmentation d'objet</code> (le reste des tâches n'étant pas significatives).</p><br>

<p>Pour l'audio, on a 7% d'<code>ASR</code>, 6% de <code>classification</code> et 1,2% de <code>détection d'activité</code>.</p>

<br>

<h3>Vision par entité</h3>

<br>

<iframe
    class="full-width-iframe"
    src="{{ '/assets/images/hf_models_stats/huggingface_downloads_by_pipelinetag_breakdown.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 19 : Top 50 des entités Hugging Face par nombre total de téléchargements avec décomposition par tâche</i></center>

<br>

<p>Les graphique par sous-comptes etpar pays, ne sont pas affichés. Ils sont en effet totalement illisibles avec plus de 50 tâches comptabilisées au total.</p>

<br><br>

<h2>Vision par langue</h2>

<br>

<p>Dans cette section nous nous intéressons qu'aux modèles liés à des tâches où l'utilisation d'une langue est applicable (les tâches de NLP, l'ASR, text-to-image, etc.). Cela représente 24 592 908 565 téléchargements sur les 36 450 707 797 initiaux du top 50 soit 67,47%.
En pratique il s'avère que pour 14,42% de ces 24,6 milliards de téléchargements, le tag de langue n'est pas renseigné pour ces modèles.<br>

Après analyse, 184 langues sont référencées dans ce top 50 (224 autres valeurs ont été trouvées mais ne sont pas des tags ISO639-1 ou ISO639-3). Ici aussi, pour des questions de visibilité, seules les 20 premières langues sont affichées dans le graphique suivant. L'ensemble des chiffres est retrouvable dans le jeu de données disponible <a href="https://huggingface.co/datasets/lbourdois/huggingface_languages_October_2025">ici</a>.</p>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_language.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 20 : Nombre total de téléchargements par langues</i></center>

<br>

<p>On peut ainsi remarquer que parmi tous les modèles disponibles dans le top 50 reposant sur une langue, l'anglais représente plus de 79,46% des téléchargements des modèles (monolingues ou multilingues) utilisant une langue (et même 92,85% des modèles possédant un tag de langue). Elle est loin devant les autres langues. Par exemple, le français qui arrive en deuxième position ne l'est qu'avec 17,48% (20,43% des modèles possédant un tag de langue).  
<br>
Sur ce top 20, les langues à alphabet latin accaparent les premières positions mais ne sont que 8 sur 20, montrant que les modèles multilingues sont plutôt divers.
</p>

<br><br>

<h2>Vision par taille de modèle</h2>

<br>

<p>Dans cette section, nous nous intéressons uniquement aux 11 263 modèles pour lesquels nous avons pu déterminer une taille au modèle.
Ils représentent 35 333 543 289 téléchargements sur 36 450 707 797, soit 96,94 ou 77,76% du total.</p>

<br>

<h3>Vision globale</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_5bins.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 21 : Nombre total de téléchargements en fonction de la taille du modèle</i></center>

<br>

<p>On peut constater que :<br>   
- 92,48% des téléchargements portent sur des modèles de moins d'un milliard de paramètres,<br> 
- 86,33% sur des modèles de moins de 500 millions de paramètres,  <br> 
- 69,83% sur des modèles de moins de 200 millions de paramètres,  <br> 
- 40,17% sur des modèles de de moins de 100 millions de paramètres.<br> <br> 

En détaillant davantage : 
</p>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_10bins.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 22 : Nombre total de téléchargements en fonction de la taille du modèle (plus de strates)</i></center>

<br>

<div class="note-block">
<div class="note-content">
Les LLM ne sont ainsi pas des modèles massivement téléchargés (en open-source du moins).<br>    
Une hypothèse est que l'explication pourrait venir du profil des utilisateurs d'Hugging Face (notamment de leur capacité de calcul pouvant être renseignée <a href="https://huggingface.co/settings/local-apps">ici</a> mais nous n'avons pas trouvé de moyen de récupérer ces informations de manière automatique.<br>
Quoi qu'il en soit, une entité souhaitant être impactante en open-source doit probablement proposer des modèles inférieurs à 500M de paramètres voire probablement plutôt 200M pour être téléchargée par une grande cible d'utilisateurs.<br>
Etant donné que 92,5% des téléchargements portent sur les modèles de moins d'un milliard de paramètres, il serait intéressant qu'Hugging Face permette d'affiner les possibilités de filtrage.<br>
<center><figure>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/613b0a62a14099d5afed7830/UhEqosU0E8wn477iJ1yIh.png" 
       alt="Diagram example" 
       style="max-width:50%; height:auto;">
</figure></center></div>
</div>

<br>

<h3>Vision par type d'entités</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_type_entities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 23 : Nombre total de téléchargements en fonction de la taille du modèle et du type de l'entité</i></center>

<br><br>

<p>
Les entreprises sont présentes dans toutes les tranches de tailles de modèles. Elles représentent une part importante pour les modèles de moins de 5 millions de paramètres (94% des téléchargements de la tranche) et au-delà de 500M de paramètres (77 à 85% des téléchargements).<br>
Entre 5M de paramètres et 500M (plutôt 200M), les modèles des universités peuvent se présenter comme des alternatives à ceux des entreprises. 
</p>

<br>

<h3>Vision par modalité</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_modalities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 24 : Nombre total de téléchargements en fonction de la taille du modèle et de la modalité</i></center>

<br><br>

<p>
Le NLP est présent dans toutes les tranches de tailles, notamment de 100 à 500M de paramètres.  <br>
La CV est principalement portée par des modèles de moins de 100 de paramètres bien qu'également présente entre 100 et 500M.  <br>
L'audio est principalement réparti de <5 à 500M de paramètres mais étonnement absent sur la tranche 100 à 200M.  <br>
Les séries temporelles pour les modèles de moins de 50M de paramètres.
</p>

<br>

<h3>Vision par tâche</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_tasks.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 25 : Nombre total de téléchargements en fonction de la taille du modèle et de la tâche</i></center>

<br><br>

<p>
Nous disposons d'un peu plus de détails au niveau des tâches (par exemple pour la vision on peut s’apercevoir que les modèles <100M sont plutôt de la classification alors qu'entre 100 et 500M il s'agit plutôt des CLIP).<br>
Le lecteur peut faire ses propres analyses en se basant sur le fait que les tâches de NLP sont différentes variantes de bleu, celles de vision des variantes de jaune, et celles d'audio des variantes de rouge.
</p>

<br>

<h3>Vision par entité</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_entities.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 26 : Nombre total de téléchargements en fonction de la taille du modèle et de l'entité</i></center>

<br><br>

<p>
Nous invitons le lecteur à cliquer sur la légende pour ne garder que les entités qui l'intéressent. Cela permet à une entité donnée de voir la répartition des téléchargements de ses modèles en fonction des différentes tranches et ainsi faire apparaître son profil, ou encore de comparer <i>n</i> entités différentes.
</p>

<br>

<figure>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/613b0a62a14099d5afed7830/-FJAdzQE60DqvNuo0CZGt.png" 
       alt="Diagram example" 
       style="max-width:100%; height:auto;">
</figure>
<center><i>Figure 27 : Nombre total de téléchargements en fonction de la taille du modèle et de l'entité (focus sur Google et Meta)</i></center>

<br>

<p>Par exemple, si l'on compare Google et Meta qui sont les deux entités open-source les plus téléchargées, Google domine sur les modèles <200M de paramètres (et est extrêmement fort sur la tranche 100-200M), là où sur les modèles de 200M et plus, c'est Meta qui domine les propos.</p>

<br>

<p>Il est aussi possible de zoomer :</p>

<figure>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/613b0a62a14099d5afed7830/D5w_p2ZXuwSbv6zG-65YR.png" 
       alt="Diagram example" 
       style="max-width:100%; height:auto;">
</figure>
<center><i>Figure 28 : Nombre total de téléchargements en fonction de la taille du modèle et de l'entité (modèles de plus d'un milliard de paramètre)</i></center>

<br><br>
      
<p>Dans cet exemple se focalisant uniquement sur les acteurs de LLM du top 50 qui dépassent les 5% de téléchargements sur l'une des tranches supérieures à 1B de paramètres (à savoir Google, Meta, Microsoft, Alibaba, Mistral, Unsloth, Deepseek + les individuels proposant les versions quantifiées des modèles comme TheBloke et Maziyar Panahi), on peut constater :  <br>
- sur la tranche 1B-3B :  <br>
  Alibaba est le plus téléchargé sur la tranche avec 20,2%, devant Meta à 16,3%, Google à 11,7%, TheBlock à 11,4% et Maziyar Panahi à 6,3%<br>
- sur la tranche 3B-7,5B :  <br>
  Meta est le plus téléchargé sur la tranche avec 31,4%, devant Mistral à 14,6%, Alibaba à 12,3%, Maziyar Panahi à 9,7%, Microsoft à 7,6% et Unsloth à 7,2%<br>
- sur la tranche 7,5B et + :  <br>
  Alibaba est le plus téléchargé sur la tranche avec 24,6%, devant Meta à 23,2%, Maziyar Panahi à 15,5%, Mistral à 7,1%, Deepseek à 6,9% et Google à 5,9%<br>
<br><br>
Au global (non affiché sur ce graphique), sur la tranche 1B et + :  <br>
Meta est le plus téléchargé avec 23,2% de la tranche, devant Alibaba à 20%, Maziyar Panahi à 11,1%, Google à 7%, Mistral à 6,8%, TheBloke à 4,5%, Deepseek à 3,8% et Microsoft à 3,3%.
</p>

<div class="note-block">
<div class="note-content">
15,6% de la tranche est captée par des individus qui proposent les poids quantifiés des modèles de base. Les entités à l'origine de ces modèles pourraient capter ces téléchargements si lors des mises en ligne, elles mettaient directement ces versions quantifiées à disposition plutôt que laisser cela à la communauté. <br>
Sans sortie de nouveaux modèles de la part de Meta en open-source prochainement, d'après une estimation très naïve (extrapolation linéaire à partir des téléchargements des comptes de <code>Qwen</code> et de <code>meta-llama</code> entre le 21 septembre et 1er octobre 2025), Alibaba devrait devenir premier sur cette tranche d'ici la fin du mois de novembre. Son modèle <code>Qwen/Qwen2.5-1.5B-Instruct</code> étant déjà le LLM textuel le plus téléchargé devant <code>meta-llama/Llama-3.1-8B-Instruct</code> (les modèles les plus petits étant les plus téléchargés). <br>
Un acteur chinois serait alors premier sur cette métrique (et sur le segment des LLM open-source), bien que les États-Unis dans leur ensemble ont encore de la marge par rapport aux acteurs chinois dans leur ensemble (49,8% pour les États-Unis sur la tranche 1B et + contre 24,2% pour la Chine et 43,1% vs 31,8% sur la tranche 7,5B et +).
</div>
</div>

<br>

<h3>Vision par pays (individuels)</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_countries_individuals.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 29 : Nombre total de téléchargements en fonction de la taille du modèle et du pays</i></center>

<br>

<h3>Vision par pays (UE regroupée)</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_countries_eu.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 30 : Nombre total de téléchargements en fonction de la taille du modèle et du pays (UE regroupée)</i></center>

<br><br>

<p>
Les États-Unis et l'Union Européenne sont présents sur l'ensemble des tranches de tailles de modèles.  <br>
L'UE est majoritaire sur les modèles de moins de 25M de paramètres, là où les États-Unis sont premiers sur toutes les autres.  <br>
À noter que la Chine est absente (à peine 130M de téléchargements) des modèles de moins de 100M de paramètres. Sachant qu'il s'agit de ceux-ci qui sont les plus téléchargés, cela explique l'observation faite au début de l'article comme quoi le pays n'est que quatrième des téléchargements globaux derrière l'Allemagne et la France, alors qu'elle est deuxième sur les modèles d'un milliard de paramètres et plus, qui sont peu téléchargés en open-source.
</p>

<br>

<h3>Vision par pays (continents)</h3>

<br>

<iframe
    class="full-width-iframe-model_size"
    src="{{ '/assets/images/hf_models_stats/huggingface_totaldownloads_modelsize_countries_continents.html' | relative_url }}" 
    allowfullscreen
></iframe>
<center><i>Figure 31 : Nombre total de téléchargements en fonction de la taille du modèle et du pays (Continents)</i></center>

<br><br><br>

<h1>Résumé</h1>
<p>L'analyse des 50 entités les plus téléchargées sur le Hub d'Hugging Face (80,22% du total des téléchargements du Hub) montre que :</p>

<ol>
    <li>Sur l'ensemble des modèles open-source où il est possible de connaître la taille du modèle (96,94% du top 50 et 77,76% du Hub), les petits modèles sont ceux qui sont de loin les plus téléchargés :
        <ul>
            <li>92,48% des téléchargements portent sur des modèles de moins d'un milliard de paramètres,</li>
            <li>86,33% sur des modèles de moins de 500 millions de paramètres,</li>
            <li>69,83% sur des modèles de moins de 200 millions de paramètres,</li>
            <li>40,17% sur des modèles de de moins de 100 millions de paramètres.</li>
        </ul>
    </li>
    <li>Les téléchargements de modèles concernent en premier lieu le NLP (58,1%), suivi par la CV à 21,2%, l'audio à 15,1%, les différentes formes de multimodalité 3,3%, les séries temporelles 1,7% et le reste est indéterminé faute de métadonnées correctement annotées.</li>
    <li>Les encodeurs textuels représentent (entre les modèles de base et leur finetuing sur une tâche particulière), plus de 45% des téléchargements totaux (ou 77,5% de la modalité NLP), contre seulement 9,5% pour les décodeurs (16,5% de la modalité) et 3% pour les encodeurs-décodeurs (5% de la modalité).<br>
    Ainsi, contrairement à l'effervescence autour de ces modèles, les LLM ne sont pas massivement téléchargés en open-source. Leur usage dans le monde réel serait-il alors plutôt du côté des API privées ?</li>
    <li>L'anglais représente plus de 79,46% des téléchargements des modèles (monolingues ou multilingues) utilisant une langue (et même 92,85% si on ne considère que les modèles possédant un tag de langue). Cette langue est loin devant les autres. Par exemple, le français qui arrive en deuxième position ne l'est qu'avec 17,48% (20,43% des modèles possédant un tag de langue).</li>
    <li>Les entreprises sont le contributeur majoritaire à l'open-source avec 63,2% des téléchargements (20 entités sur les 50), suivies à 20,7% par les universités (10 entités), puis par des individuels à 12,1% (16 entités), par des structures non-profit à 3,8% (4 entités) et enfin par les laboratoires hybrides à 0,3% (1 entité).</li>
    <li>Les États-Unis sont présents partout, maillant toutes les modalités (NLP, vision, audio, séries temporelles, multimodalités) et toutes les tailles de modèles (de moins de 5M de paramètres aux dizaines/centaines de milliards). Ils sont notamment portés par leurs entreprises open-source (écrasant toute concurrence sur ce segment) mais disposent également d'atouts dans tous les types d'organisation existantes (hors laboratoires hybrides) puisqu'ils sont représentés à 18 reprises dans ce top 50.<br>
    L'Europe (l'Allemagne, la France et le Royaume-Uni notamment) se positionne également dans tous les types d'organisation existantes hors laboratoires hybrides (présente à 20 reprises) mais se démarque par l'impact de ses universités spécialisées sur des modèles de petites tailles (&lt;200M de paramètres). Elle est aussi présente sur toutes les modalités à l'exception des séries temporelles.<br>
    La Chine (représentée par 5 entités) est bien présente sur le segment des grands modèles open-source (31,8% vs 43,1% pour les États-Unis et 24% pour l'Europe sur les modèles de plus de 7,5 milliards de paramètres). Elle est cependant mal placée sur toutes les autres tranches de tailles des modèles (seulement 130M de téléchargements de modèles de moins de 100M de paramètres contre 7,05 milliards pour les États-Unis et 5,3 milliards pour l'Europe). Son non-positionnement sur la vision (à peine 4 millions de téléchargements) et l'audio (0 téléchargements) la pénalise également. Il ne s'agit pas de modalités sur lesquelles elle est connue pour avoir du retard, mais force est de constater qu'elle ne produit pas, à l'heure actuelle, de contenus open-source dans ces modalités sur Hugging Face (plateforme non accessible dans le pays). Elle domine le secteur des organisations non-profit et est seule sur celui des laboratoires hybrides universités/entreprises.<br>
    Enfin, les autres pays ne bénéficient uniquement d'un acteur spécialisé sur une modalité donnée.</li>
</ol>


<br><br><br>

<h1>Suite</h1>

<p>Dans une actualisation de ce travail (qui nécessitera d'analyser un nombre important de publications), nous souhaiterions rajouter deux strates supplémentaires à savoir la ville où se situent les auteurs du modèle ainsi que leur nationalité. L’objectif sera de déterminer dans quelles villes les modèles les plus téléchargés se développent ainsi que les pays avec le système éducatif générant les auteurs les plus téléchargés.<br>
Par exemple, les téléchargements du LLaMA 1 ne seraient pas comptabilisés à 100% comme américain mais à 12/14 français (Paris), 1/14 américain (San Francisco) et 1/14 pakistanais (Londres). Plusieurs systèmes de pondérations pourraient être appliqués comme telle une pondération plus forte sur les auteurs principaux par exemple).<br><br>
Nous souhaiterions également proposer une vue permettant de distinguer l'impact d'un modèle dans l'écosystème, c'est-à-dire en plus des téléchargements propres à un modèle donné y ajouter tous les téléchargements issus des finetunings/merges/adaptateurs/quantifications. Pour cela nous avons déjà procédé à quelques expériences en utilisant le <em>Model tree</em> mais il s'avère que celui-ci est souvent incomplet, surtout pour les modèles les plus anciens.<br>
Une méthode consisterait à analyser le nom des têtes de finetuning utilisées par les modèles, si ce n'est le modèle de base, au moins son architecture de base.<br>
L'objectif est alors de déterminer quels modèles de base sont les plus impactants (cet article de <a href="https://huggingface.co/spaces/transformers-community/Transformers-tenets">blog</a> par les équipes d'Hugging Face et qui vient de sortir serait une bonne base).</p>

<figure>
  <center>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/613b0a62a14099d5afed7830/-gczIrRFbuQ-0RJ7qd_HQ.png" 
       alt="Diagram example" 
       style="max-width:60%; height:auto;"></center>
</figure>
<center><i>Figure 32 : Model tree de google-bert/bert-base-uncased</i></center>


<br>
<p>
Comme évoqué en introduction, les téléchargements de certains modèles sont mal comptabilisés actuellement sur le Hub. Cela pourrait ainsi dévaluer certaines entités. Nous sommes actuellement en contact avec les équipes d'Hugging Face pour corriger du mieux possible ce point.<br><br>

Enfin, nous envisageons de réaliser un article similaire à celui pour les jeux de données en place des modèles.</p>
</div>




















