---
title: "Ã‰VOLUTION DES STATE SPACE MODELS (SSM) EN 2022"
categories:
  - SSM
tags:
  - Introduction aux SSM 
excerpt : SSM - Revue de littÃ©rature des SSM parus lors de l'annÃ©e 2022
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/NLP_radom_blog.png"
    teaser: "https://github.com/lbourdois/blog/assets/58078086/cb2dca34-9a3e-481a-8773-2360a1ceaa1c"
author_profile: false
sidebar:
    nav: sidebar-ssm
classes: wide
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

âš  **WIP** âš  


# <span style="color: #FF0000"> **Avant-propos** </span>
Dans lâ€™[article prÃ©cÃ©dent](https://lbourdois.github.io/blog/ssm/introduction_ssm/), nous avons dÃ©fini ce quâ€™est un State Space Model (SSM) Ã  lâ€™aide dâ€™un systÃ¨me en temps continu. Nous lâ€™avons discrÃ©tisÃ© pour faire apparaÃ®tre sa vue rÃ©currente ainsi que sa vue convolutive. Lâ€™intÃ©rÃªt est alors de pouvoir entraÃ®ner notre modÃ¨le de maniÃ¨re convolutive puis de rÃ©aliser lâ€™infÃ©rence de maniÃ¨re rÃ©currente sur de trÃ¨s longues sÃ©quences.  
<br>

| ![image](https://github.com/lbourdois/blog/assets/58078086/12bbe1cf-3911-4bad-9a3b-3f427bc6bc82)|
|:--:|
| Figure 1 : *Image provenant de l'article de blog Â« [Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3) Â» d'Albert GU et al. (2022)*|

<br>
Cette vision a Ã©tÃ© introduite par Albert GU dans ses papiers LSSL et S4 parus en 2021. Le S4 Ã©tant lâ€™Ã©quivalent du Â« *Attention is all you need* Â» pour les transformers.
Dans cet article, nous allons faire une revue de littÃ©rature des SSM parus durant lâ€™annÃ©e 2022. Ceux apparus en 2023 seront listÃ©s dans le prochain article.
Lâ€™objectif est de montrer les diffÃ©rentes Ã©volutions de ces types de modÃ¨les au cours des mois, tout en restant Ã  une vue globale des choses (i.e. je ne vais pas rentrer dans tous les dÃ©tails des papiers listÃ©s). Lors de cette annÃ©e 2022, les diffÃ©rentes avancÃ©es se sont focalisÃ©es Ã  appliquer des algorithmes de discrÃ©tisations diffÃ©rents, ainsi quâ€™Ã  remplacer la matrice HiPPO par une plus simple.
<br><br><br>

## <span style="color: #FFBF00"> **ModÃ¨les thÃ©oriques** </span>

Dans cette section, nous allons passer en revue des travaux thÃ©orique, au sens oÃ¹ ils sont des propositions dâ€™amÃ©lioration de lâ€™architecture du S4. Nous aborderons ensuite dans une section diffÃ©rente, des applications concrÃ¨tes sur diffÃ©rentes tÃ¢ches (audio, vision, etc.).
<br><br>

### <span style="color: #51C353"> **S4 V2** </span>

CommenÃ§ons par une partie que le lecteur peut considÃ©rer comme facultative car dÃ¨s la section suivante nous verrons que ce qui est dÃ©crit dans celle-ci se retrouve caduc du fait du remplacement de la matrice HiPPO par une plus simple. Nous rÃ©digeons cette section afin de rendre compte de lâ€™ensemble des travaux effectuÃ©s sur la matrice HiPPO avant son abandon.   
  
Le 4 mars 2022, les auteurs du S4 ont actualisÃ© leur papier afin dâ€™y intÃ©grer une section sur lâ€™importance de la matrice HiPPO. Il sâ€™agit de la section 4.4 dans la version la plus rÃ©cente du papier.  
Pour rÃ©sumer, elle consiste Ã  rapporter les rÃ©sultats observÃ©s Ã  la suite de la rÃ©alisation dâ€™ablations sur le jeu de donnÃ©es sÃ©quentielles CIFAR-10. Au lieu d'utiliser un SSM avec la matrice HiPPO, les auteurs ont essayÃ© d'utiliser diverses paramÃ©trisations comme une matrice dense alÃ©atoire et une matrice diagonale alÃ©atoire. 

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Hippo.png) |
|:--:|
| Figure 2 : *Accuracy sur l'Ã©chantillon de validation de CIFAR-10, tirÃ©e de la figure 3 du papier du S4* |

<br>
HiPPO est donc importante, mais est-ce que ces performances sont du spÃ©cifiquement Ã  cette matrice ou bien est-ce que nâ€™importe quelle matrice normale Ã  faible rang (NPLR pour normal plus low-rank) suffirait ?

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/NPLR.png) |
|:--:|
| Figure 3 : *Accuracy sur l'Ã©chantillon de validation de CIFAR-10 avec diffÃ©rentes initialisations et parameterisations, tirÃ©e de la figure 4 du papier du S4* |

<br>
Initialiser une matrice NPLR avec HiPPO augmente considÃ©rablement les performances.
Ainsi, dâ€™aprÃ¨s ces expÃ©riences, la matrice HiPPO est primordiale afin dâ€™obtenir un modÃ¨le performant.

Les auteurs du S4 ont approfondi leurs travaux et le 24 juin 2022, ont publiÃ© lâ€™article [*How to Train Your HiPPO*](https://arxiv.org/abs/2206.12037). Il sâ€™agit dâ€™un papier extrÃªmement dÃ©taillÃ© de plus de 39 pages.  
Pour rÃ©sumer, dans ce travail, les auteurs se concentrent sur une interprÃ©tation plus intuitive des SSM en tant que modÃ¨le convolutif oÃ¹ le noyau de convolution est une combinaison linÃ©aire de fonctions de base particuliÃ¨res, ce qui conduit Ã  plusieurs gÃ©nÃ©ralisations et Ã  de nouvelles mÃ©thodes.  
Ainsi, ils prouvent que la matrice A du S4 produit des polynÃ´mes de Legendre Ã  Ã©chelle exponentielle (LegS). Cela confÃ¨re au systÃ¨me une meilleure capacitÃ© Ã  modÃ©liser les dÃ©pendances Ã  long terme via des noyaux trÃ¨s lisses.  
Les auteurs dÃ©rivent Ã©galement un nouveau SSM qui produit des approximations de fonctions de Fourier tronquÃ©es (FouT). Cette mÃ©thode gÃ©nÃ©ralise les [transformÃ©es de Fourier Ã  court terme]( https://fr.wikipedia.org/wiki/Transform%C3%A9e_de_Fourier_%C3%A0_court_terme) et les convolutions locales (c'est-Ã -dire un ConvNet standard), et peut Ã©galement coder des fonctions de pointe pour rÃ©soudre des tÃ¢ches de mÃ©morisation classiques.  
A noter que câ€™est surtout HiPPO-FouT qui est introduit dans ce papier. HiPPO-LegS ayant Ã©tÃ© introduit dans le papier original dâ€™HiPPO deux ans plus tÃ´t. De mÃªme quâ€™HiPPO-LegT (polynÃ´mes de Legendre tronquÃ©s).  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master//assets/images/SSM_2022/Hippo_LegS_LegT_FouT.png) |
|:--:|
| Figure 4 : *Les diffÃ©rentes variantes d'Hippo* |

<br>
Les couleurs reprÃ©sentent les 4 premiÃ¨res fonctions de base $$K_n(t)$$ (le noyau de convolution) pour chacune des mÃ©thodes (nous invitons le lecteur Ã  regarder le tableau 1 du papier pour savoir Ã  quoi Ã©quivaut $$K_n(t)$$ dans chacune des mÃ©thodes).

De plus, les auteurs travaillent Ã©galement sur le pas de temps $$âˆ†$$, qui indÃ©pendamment d'une notion de discrÃ©tisation, peut Ãªtre interprÃ©tÃ© simplement comme contrÃ´lant la longueur des dÃ©pendances ou la largeur des noyaux du SSM. Les auteurs dÃ©taillent aussi comment choisir une bonne valeur de $$âˆ†$$ pour une tÃ¢che donnÃ©e.

Les travaux menÃ©s permettent dâ€™amÃ©liorer les rÃ©sultats du S4 sur le benchmark LRA de plus de 6 points :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4v2_results.png) |
|:--:|
| Figure 5 : *RÃ©sultats du S4 v2 sur le benchmark* |

<br>
Le modÃ¨le rÃ©sultant de ce papier est gÃ©nÃ©ralement appelÃ© Â« S4 V2 Â» ou Â« S4 updated Â» dans la littÃ©rature Ã  opposer au Â« S4 original Â» ou Â« S4 V1 Â».

<br><br>

### <span style="color: #51C353"> Le DSS : *Diagonal State Spaces* </span>

Le 27 mars 2022, Ankit GUPTA introduit dans son papier [*Diagonal State Spaces are as Effective as Structured State Spaces*](https://arxiv.org/abs/2203.14343) les *Diagonal State Spaces* (DSS).  
Il semble que suite Ã  ce papier, Albert GU et lui se sont mis Ã  travailler ensemble dâ€™une part sur une version actualisÃ©e de ce papier (GU apparaissant par la suite comme co-auteur dans les v2 et v3 de lâ€™article) et dâ€™autre part dans le cadre du S4D (voir section suivante).  
La principale chose Ã  retenir est que cette approche est sensiblement plus simple que le S4. En effet, le DSS repose sur des matrices d'Ã©tat diagonales (donc sans la correction de rang faible du S4, i.e. sans la matrice HiPPO) qui, si sont initialisÃ©es de maniÃ¨re appropriÃ©e, fonctionne mieux que le S4 original. Lâ€™usage dâ€™une matrice diagonale Ã  la place de la matrice HiPPO pour A est depuis devenu une norme.  

ArrÃªtons-nous nÃ©anmoins sur les quelques complexitÃ©s/limites que contiennent ce papier. En effet, câ€™est en les listant que nous pourrons comprendre les apports des mÃ©thodes suivantes qui visent Ã  simplifier davantage les choses.

**1. La discrÃ©tisation**  
Le DSS utilise le mÃªme systÃ¨me dâ€™Ã©quations diffÃ©rentielles que le S4 : 

$$
  \begin{aligned}
    x' &= \mathbf{A}x + \mathbf{B}u \\
    y &= \mathbf{C}x
  \end{aligned}
$$

Cependant il utilise une discrÃ©tisation diffÃ©rente afin dâ€™aboutir aux vues convolutives et rÃ©currentes, Ã  savoir la discrÃ©tisation [*zero-order hold*](https://en.wikipedia.org/wiki/Zero-order_hold) (ZOH) ou bloqueur d'ordre zÃ©ro en franÃ§ais, au lieu de la discrÃ©tisation bilinÃ©aire, qui suppose que le signal que nous Ã©chantillonnons est constant entre chaque point d'Ã©chantillonnage.  
Ci-dessous un tableau comparatif des valeurs de $$A$$, $$B$$ et $$C$$ pour chacunes des deux discrÃ©tisations dans la vue rÃ©currente, ainsi que lâ€™expression du noyau de convolution dans la vue convolutive :

| DiscrÃ©tisation | BilinÃ©aire | ZOH                    |
| -------------- | ---------- | ----------------------- |
| RÃ©currence     | $$\mathbf{\bar{A}} = (\mathbf {I} - \frac{\Delta}{2} \mathbf{A})^{-1}(\mathbf {I} + \frac{\Delta}{2} \mathbf{A})$$ <br> $$\mathbf {\bar{B}} = (\mathbf{I} - \frac{\Delta}{2}  \mathbf{A})^{-1} \Delta \mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$   | $$\mathbf{\bar{A}} = e^{\mathbf{A}\Delta}$$ <br> $$\mathbf{\bar{B}} = (\mathbf{\bar{A}} - I)\mathbf{A}^{-1}\mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$  |
| Convolution    | $$\mathbf{\bar{K}}_k = (\mathbf{\bar{C}}  \mathbf{\bar{B}}, \mathbf{\bar{C}} \mathbf{\bar{A}}  \mathbf{\bar{B}}, â€¦, \mathbf{\bar{C}}  \mathbf{\bar{A}}^{k} \mathbf{\bar{B}})$$       | $$\mathbf{\bar{K}} = (\ \mathbf{C} e^{\mathbf{A}\cdot k\Delta} (e^{\mathbf{A}\Delta} - I)\mathbf{A}^{-1}\mathbf{B}\ )_{0 \leq k < L}$$ |

<br>

Pour la ZOH, aprÃ¨s avoir dÃ©roulÃ© les calculs, on obtient en fin de compte $$y_k = \sum_{j=0}^k \bar{C}\bar{A}^j\bar{B}\cdot u_{k-j} = \sum_{j=0}^k \bar{K}_j\cdot u_{k-j}$$.

Calculer $$y$$ Ã  partir de $$u$$ et $$\bar{K}$$ sâ€™effectue alors par Transformation de Fourier rapide en $$O(L~log(L))$$ avec $$L$$ la longueur de la sÃ©quence en calculant simultanÃ©ment la multiplication de deux polynÃ´mes de degrÃ©s $$L-1$$.
<br>

**2. DSSsoftmax et DSSexp**  

<u>Version courte</u>  

GUPTA formule une proposition pour obtenir des DSS qui soient aussi expressifs que le S4, ce qui en pratique abouti Ã  la formulation de deux DSS diffÃ©rents : le DSSexp et le DSSsoftmax. Les informations Ã  retenir les concernant peuvent se rÃ©sumer au tableau suivant :

| Approche        | DSSexp          | DSSsoftmax      |
| --------------- | --------------- | --------------- |
| Vue convolutive | $$K = \bar{K}_{\Delta, L}(\Lambda,\mathbb{I}_{1 \leq i \leq N},\ \widetilde{w})\\ = \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$ | $$K = \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\\ = w \cdot \Lambda^{-1} \cdot \text{row-softmax}(P)$$  |
| Vue rÃ©currente  | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$ | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$ |
| InterprÃ©tation  | Agit comme la porte d'oubli d'une LSTM  | Si $$\Re(\lambda)<<0$$ : conserve l'information locale,<br> si $$\Re(\lambda)>>0$$ : peut capturer des informations Ã  trÃ¨s longues distances                                                                                        |

Nous travaillons donc ici sur $$â„‚$$ et non pas $$â„$$.  

<br>

<u>Version longue</u>  

GUPTA formule la proposition suivante pour obtenir des DSS qui sont aussi expressifs que le S4 :

Let $$K \in \mathbb{R}^{1\times L}$$ be the kernel of length $$L$$ of a given state space $$(A, B, C)$$ and sample time $$\Delta > 0$$, where $$A \in \mathbb{C}^{N \times N}$$ is diagonalizable over $$\mathbb{C}$$ with eigenvalues $$\lambda_1,\ldots,\lambda_N$$ and $$\forall i$$, $$\lambda_i \neq 0$$ and $$e^{L\lambda_i\Delta} \neq 1$$. Let $$P \in \mathbb{C}^{N \times L}$$ be $$P_{i,k} = \lambda_i k\Delta$$ and $$\Lambda$$ be the diagonal matrix with $$\lambda_1,\ldots,\lambda_N$$. Then there exist $$\widetilde{w}, w \in \mathbb{C}^{1\times N}$$ such that

- (a) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ (1)_{1 \leq i \leq N},\ \widetilde{w})\ \ =\ \ \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$
- (b) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\ \ =\ \ w \cdot \Lambda^{-1} \cdot  \text{row-softmax}(P)$$

(a) suggÃ¨re que nous pouvons paramÃ©trer les espaces d'Ã©tat via $$\Lambda, \widetilde{w} \in \mathbb{C}^N$$ et calculer le noyau comme indiquÃ©. Malheureusement, dans la pratique, la partie rÃ©elle des Ã©lÃ©ments de $$Î›$$ peut devenir positive pendant l'apprentissage, ce qui rend l'apprentissage instable pour les entrÃ©es longues. Pour rÃ©soudre Ã§a, les auteurs proposent deux mÃ©thodes : DSSexp et DSSsoftmax.

2.1 Convolution  
Dans DSSexp, les parties rÃ©elles de $$Î›$$ doivent Ãªtre nÃ©gatives. On a alors $$\Lambda = - \text{elementwise-exp}(\Lambda_\mathrm{re}) + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$. $$K$$ se calcule alors comme dans la formule indiquÃ©e dans la partie (a) de la proposition.  
Dans DSSsoftmax, on normalise chaque ligne de $$Î›$$ par la somme de ces Ã©lÃ©ments. On a alors $$\Lambda = \Lambda_\mathrm{re} + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$.  
$$K$$ se calcule alors comme dans la formule indiquÃ©e dans la partie (b) de la proposition.  
A noter que softmax sur $$\mathbb{C}$$ pouvant par exemple ne pas Ãªtre dÃ©fini lors de sofmax $$(0, i \pi)$$, les auteurs utilisent une version corrigÃ©e du softmax (dÃ©taillÃ©e dans lâ€™annexe A.2. du papier) pour prÃ©venir ce problÃ¨me.

2.2 RÃ©currence  
Dans DSSexp, en utilisant la formule de la rÃ©currence dans le tableau ci-dessus, on obtient $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$, oÃ¹ dans les deux Ã©galitÃ©s, $$\lambda_i$$ est la iÃ¨me valeur propre de Lambda.  
Etant donnÃ© que $$\bar{A}$$ est diagonale, il est possible de calculer les $$x_k$$ indÃ©pendamment de la maniÃ¨re suivante :  $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + \lambda_i^{-1} (e^{\lambda_i\Delta} - 1)u_k$$.  
Et il est alors possible de dÃ©duire que, si $$|\lambda_i|\Delta \approx 0$$, nous avons $$x_{i,k} \approx x_{i,k-1}$$ et nous copions donc l'histoire sur de nombreux pas de temps. En revanche, si $$\mathrm{Re}(\lambda_i)\Delta \ll 0$$, alors $$_{i,k} \approx -\lambda_i^{-1}u_k$$ et donc l'information des pas de temps prÃ©cÃ©dents est oubliÃ©e, similaire Ã  une porte Â« forget Â» dans les LSTMs.

Dans DSSsoftmax, en utilisant la formule de la rÃ©currence dans le tableau ci-dessus, on obtient : $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$.  
Ce qui fait que $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + {u_k(e^{\lambda_i\Delta} - 1) \over \lambda_i (e^{\lambda_i\Delta L} - 1)}$$.  
A noter que $$e^{\lambda_i\Delta}$$ peut Ãªtre instable, et on calcule alors deux cas diffÃ©rents en fonction du signe de $$\mathrm{Re}(\lambda)$$ en introduisant un Ã©tat intermÃ©diaire $$\widetilde{x}_{k}$$.  
-	Si $$\mathrm{Re}(\lambda) \leq 0$$ : $$\widetilde{x}_{k} = e^{\lambda\Delta}\cdot \widetilde{x}_{k-1} + u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {(e^{\lambda\Delta} - 1) \over \lambda (e^{\lambda\Delta L} - 1) }$$

Et notamment si $$\mathrm{Re}(\lambda) \ll 0$$ alors $$\widetilde{x}_k \approx u_k$$ et $$x_k \approx u_k / \lambda$$, ce qui fait quâ€™on se focalise sur une information locale (les pas prÃ©cÃ©dents sont ignorÃ©s)

-	Si $$\mathrm{Re}(\lambda) > 0$$ : 
$$\widetilde{x}_{k} = \widetilde{x}_{k-1} + e^{-k\lambda\Delta} \cdot u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {e^{\lambda\Delta (k-(L-1))} \over \lambda}\cdot {e^{-\lambda\Delta}-1 \over e^{-\lambda\Delta L} - 1 }$$

Et notamment si $$\mathrm{Re}(\lambda) \gg 0$$ alors $$\widetilde{x}_0 \approx u_0$$ et $$\widetilde{x}_k \approx \widetilde{x}_{k-1} \approx u_0$$, $$x_{k < L-1} \approx 0$$ et $$x_{L-1} \approx u_0 / \lambda$$, ce qui fait que le modÃ¨le peut capturer des informations Ã  trÃ¨s longues distances.
En pratique, les auteurs du S4D indiquant que $$\mathrm{Re}(\lambda) \gg 0$$ ne fonctionne pas si $$L$$ est trÃ¨s grande (= Ã§a explose quand $$t \rightarrow \infty$$ dans $$K(t) = C \exp(A^\intercal).B)$$.  

**3. Initialisation**  
Les parties rÃ©elles et imaginaires de w sont initialisÃ©es Ã  partir de $$\mathcal{N}(0,1)$$, les Ã©lÃ©ments de $$\Delta_{\log}$$ Ã  partir de $$\exp(\mathcal{U}~(log(0.001), log(0.1)))$$, et $$\Delta$$ via les valeurs propres de la matrice HiPPO. Les auteurs se demandent sâ€™il ne serait pas possible de trouver une initialisation plus simple pour $$\Delta$$. Ils notent nÃ©anmoins quâ€™une alÃ©atoire conduit Ã  de moins bons rÃ©sultats.  

Concernant les rÃ©sultats, DSS a Ã©tÃ© testÃ© sur LRA et Speech Commands :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results.png) |
|:--:|
| Figure 6 : *RÃ©sultats du DSS sur LRA* |

<br>
Le DSS (en version softmax ou en version exp) obtient de meilleurs rÃ©sultats moyens que ce du S4 original sur ce benchmark. Le DSSsoftmax semble performer lÃ©gÃ¨rement mieux que le DSSexp. Un intÃ©rÃªt Ã©galement de ce papier est quâ€™il est le premier Ã  reproduire les rÃ©sultats du S4 et donc confirmer que les SSM passent ce benchmark.


| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results2.png) |
|:--:|
| Figure 7 : *RÃ©sultats du DSS sur Speech Commands* |

<br>
Sur Speech Commands, le S4 garde lâ€™avantage sur les DSS.

<br><br>

### <span style="color: #51C353"> **S4D** </span>
Le 23 juin 2022, GU, GUPTA et al. introduisent le S4D dans leur article [*On the Parameterization and Initialization of Diagonal State Space Models*](https://arxiv.org/abs/2206.11893).  
L'initialisation de la matrice d'Ã©tat $$A$$ du DSS repose sur une approximation particuliÃ¨re de la matrice HiPPO du S4. Si la matrice du S4 a une interprÃ©tation mathÃ©matique pour traiter les dÃ©pendances Ã  longue portÃ©e, l'efficacitÃ© de l'approximation diagonale reste thÃ©oriquement inexpliquÃ©e.  
Avec le S4D, les auteurs introduisent un SSM diagonal combinant meilleur du calcul et de la paramÃ©trisation de S4 et de l'initialisation de DSS, ce qui donne une mÃ©thode extrÃªmement simple, thÃ©oriquement fondÃ©e et empiriquement efficace.  

Une comparaison des trois mÃ©thodes est donnÃ©e par le tableau 1 du papier :  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4D_table1.png) |
|:--:|
| Figure 8 : *Comparaison du S4, du DSS et du S4D* |

<br>
Le S4D peut utiliser la discrÃ©tisation bilinÃ©aire du S4 ou bien la discrÃ©tisation ZOH du DSS.  

Dans le S4D, le noyau de convolution $$K$$ de lâ€™Ã©quation 
$$y = u \ast \mathbf{\overline{K}}$$ oÃ¹ $$\mathbf{\overline{K}} = (\mathbf{C}\mathbf{\overline{B}}, \mathbf{C}\mathbf{\overline{A}}\mathbf{\overline{B}}, \dots, \mathbf{C}\mathbf{\overline{A}}^{L-1}\mathbf{\overline{B}})$$, se calcule de la faÃ§on suivante : 
$$\mathbf{\overline{K}}_\ell = \sum_{n = 0}^{N-1} \mathbf{C}_n \mathbf{\overline{A}}_n^\ell \mathbf{\overline{B}}_n \implies \mathbf{\overline{K}} = (\mathbf{\overline{B}}^\top \circ \mathbf{C}) \cdot \mathcal{V}_L(\mathbf{\overline{A}})$$  oÃ¹ :  
â€¢ $$\circ$$ reprÃ©sente le [produit matriciel dâ€™Hadamard]( https://fr.wikipedia.org/wiki/Produit_matriciel_de_Hadamard),  
â€¢ $$\cdot$$ un produit matriciel classique,  
â€¢ $$\mathcal{V}_L$$ est la [matrice de Vandermonde]( https://fr.wikipedia.org/wiki/Matrice_de_Vandermonde) câ€™est-Ã -dire : $$\mathcal{V} = 
\begin{bmatrix}
1&\alpha _{1}&{\alpha _{1}}^{2}&\dots &{\alpha _{1}}^{n-1}\\
1&\alpha _{2}&{\alpha _{2}}^{2}&\dots &{\alpha _{2}}^{n-1}\\
1&\alpha _{3}&{\alpha _{3}}^{2}&\dots &{\alpha _{3}}^{n-1}\\
vdots &\vdots &\vdots & \vdots \\
1&\alpha _{m}&{\alpha _{m}}^{2}&\dots &{\alpha _{m}}^{n-1}
\end{bmatrix}$$.

Autrement dit, pour tous $$i$$ et $$j$$, le coefficient en ligne $$i$$ et colonne $$j$$ est $$\displaystyle V_{i,j}={\alpha _{i}}^{j-1}$$.  

Au final, dans le S4D, 

$$
  \mathbf{\overline{K}} =
  \begin{bmatrix}
    \mathbf{\overline{B}}_0 \mathbf{C}_0 & \dots & \mathbf{\overline{B}}_{N-1} \mathbf{C}_{N-1}
  \end{bmatrix}
  \begin{bmatrix}
    1      & \mathbf{\overline{A}}_0     & \mathbf{\overline{A}}_0^2     & \dots  & \mathbf{\overline{A}}_0^{L-1}     \\
    1      & \mathbf{\overline{A}}_1     & \mathbf{\overline{A}}_1^2     & \dots  & \mathbf{\overline{A}}_1^{L-1}     \\
    \vdots & \vdots                 & \vdots                    & \ddots & \vdots                        \\
    1      & \mathbf{\overline{A}}_{N-1} & \mathbf{\overline{A}}_{N-1}^2 & \dots  & \mathbf{\overline{A}}_{N-1}^{L-1} \\
  \end{bmatrix}
\qquad \text{oÃ¹ } \mathcal{V}_L(\mathbf{\overline{A}})_{n, \ell} = \mathbf{\overline{A}}_n^\ell
$$.  
Le tout est calculable en $$\widetilde{O}(N+L)$$ opÃ©rations et espace.  

La paramÃ©trisation des diffÃ©rentes matrices est la suivante :
-	$$\mathbf{A} = -\exp(\Re(\mathbf{A})) + i \cdot \Im(\mathbf{A})$$.  
Les auteurs indiquent quâ€™il est possible de remplacer lâ€™exponentielle par nâ€™importe quelles fonctions positives.
-	$$\mathbf{B} = 1$$ puis est entraÃ®nÃ©
-	$$\mathbf{C}$$ alÃ©atoire avec un Ã©cart-type de 1 puis entraÃ®nÃ©.
  
Notons que le S4 prend en compte des rÃ©els alors que S4D des complexes en paramÃ©trant avec une taille dâ€™Ã©tat de $$N/2$$ et en ajoutant implicitement les paires conjuguÃ©es aux paramÃ¨tres. On a alors lâ€™Ã©quivalent de $$N$$ paramÃ¨tres rÃ©els assurant que la sortie est rÃ©elle.  

Concernant lâ€™initialisation, les auteurs en introduisent deux :    
â€¢ **S4D-Inv** qui est une approximation de S4-LegS : $$\quad \mathbf{A}_n = -\frac{1}{2} + i \frac{N}{\pi} \left( \frac{N}{2n+1}-1 \right)$$  
â€¢ **S4D-Lin** qui est une approximation de S4-FouT : $$\quad \mathbf{A}_n = -\frac{1}{2}\mathbf{1} + i \pi n$$

Nous invitons le lecteur Ã  consulter la partie 4 du papier pour plus de dÃ©tails sur dâ€™oÃ¹ sortent ces Ã©quations.  
Dâ€™un point de vue interprÃ©tabilitÃ©, la partie rÃ©elle de $$\mathbf{A}_n$$ contrÃ´le le taux de dÃ©croissance des poids. La partie imaginaire de $$\mathbf{A}_n$$ contrÃ´le quant Ã  elle les frÃ©quences dâ€™oscillations de la fonction de base $$K_n(t) = e^{t\mathbf{A}}\mathbf{B}$$.  

Enfin les auteurs avancent quelques rÃ©sultats :  
1)	Calculer le modÃ¨le avec une softmax au lieu de Vandermonde ne fait pas de grande diffÃ©rence  
2)	EntraÃ®ner B donne toujours de meilleurs rÃ©sultats  
3)	Pas de diffÃ©rences notables entres les deux discrÃ©tisations possibles  
4)	Restreindre la partie rÃ©elle de A donne de meilleurs rÃ©sultats (trÃ¨s lÃ©gers cependant)  
5)	Toutes les modifications testÃ©es pour lâ€™initialisation ont dÃ©gradÃ© les rÃ©sultats. A savoir appliquer un coefficient sur la partie imaginaire ou utiliser une partie imaginaire alÃ©atoire, ou utiliser une partie rÃ©elle alÃ©atoire, ou utiliser une partie imaginaire et une partie rÃ©elle alÃ©atoire.  

Cette mÃ©thode Ã©tant trÃ¨s facile Ã  implÃ©menter par rapport aux autres (Vandermade se limitant Ã  deux lignes de code), le S4D a remplacÃ© le S4 dans les usages (on peut dâ€™ailleurs observer des abus de langage oÃ¹ dans la table 6 du papier du Mamba par exemple, les auteurs utilisent le terme S4 pour dÃ©signer le S4D).
<br><br>

### <span style="color: #51C353"> **GSS** </span>

<br><br>

### <span style="color: #51C353"> **Liquid-S4** </span>

<br><br>

### <span style="color: #51C353"> **S5** </span>

<br><br>

### <span style="color: #51C353"> **Mega** </span>

Le 21 septembre 2022, MA, ZHOU et al., ont publiÃ© le [*Mega: Moving Average Equipped Gated Attention*](https://arxiv.org/abs/2209.10655).  
Le Mega est un transformer avec un mÃ©canisme d'attention Ã  une seule tÃªte, utilisant le systÃ¨me de portes du GAU, et est Ã©quipÃ© d'une moyenne mobile exponentielle (EMA) amortie pour incorporer le biais inductif positionnel.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega.png) |
|:--:|
| Figure X : *Vue dâ€™ensemble du Mega, figure conÃ§ue Ã  partir de ma comprÃ©hension du papier* |

<br>
Les auteurs proposent aussi une variante, Mega-chunk, qui divise efficacement l'ensemble de la sÃ©quence en plusieurs morceaux de longueur fixe. Cela offre une complexitÃ© linÃ©aire en termes de temps et d'espace tout en n'entraÃ®nant qu'une perte de qualitÃ© minimale.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega-chunk.png) |
|:--:|
| Figure X : *Le Mega chunk* |

<br>
Cela offre une complexitÃ© linÃ©aire appliquant simplement l'attention localement Ã  chaque morceau de longueur fixe.  
Plus prÃ©cisÃ©ment, on divise les sÃ©quences de requÃªtes, de clÃ©s et de valeurs dans en morceaux de longueur $$c$$. Par exemple, $$\mathbf{Q} = {\mathbf{Q}_1, ... , \mathbf{Q}_k}$$, oÃ¹ $$k = -\frac{n}{c}$$ est le nombre de morceaux. L'opÃ©ration d'attention est appliquÃ©e individuellement Ã  chaque bloc, ce qui donne une complexitÃ© linÃ©aire $$\mathcal{O}(kc^2) = \mathcal{O}(nc)$$ par rapport Ã  $$n$$.  
Cette mÃ©thode souffre d'une limitation critique, Ã  savoir la perte d'informations contextuelles provenant d'autres blocs. Mais la sous-couche EMA attÃ©nue ce problÃ¨me en capturant les informations contextuelles locales Ã  proximitÃ© de chaque token, dont les rÃ©sultats sont utilisÃ©s comme entrÃ©es dans la sous-couche d'attention. Ainsi, le contexte effectif exploitÃ© par l'attention au niveau du bloc peut aller au-delÃ  de la limite du bloc.  

Le Mega est extrÃªmement compÃ©titif puisquâ€™il devient alors le meilleur modÃ¨le sur le LRA :  
 
| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_Mega.png) |
|:--:|
| Figure X : *Les rÃ©sultats du Mega sur le benchmark LRA* |

<br>
Que fait donc un transformer dans un article de blog sur les SSM ? IntÃ©ressons-nous Ã  lâ€™EMA amortie pour comprendre le lien entre le Mega et le S4D.  

â€¢ <u>Rappel sur lâ€™EMA Â« classique Â»</u> :  
Lâ€™Ã©quation dâ€™une [EMA Â« classique Â»](https://fr.wikipedia.org/wiki/Moyenne_mobile#Moyenne_mobile_exponentielle)  est $$ğ²_t = ğœ¶ âŠ™ ğ±_t + (1âˆ’ğœ¶) âŠ™ ğ²_{tâˆ’1}$$ avec $$ğœ¶$$ in $$[0,1]^d$$ le coefficient de lâ€™EMA reprÃ©sentant le degrÃ© de diminution de la pondÃ©ration et âŠ™ le produit matriciel de Hadamard.  
Un ğœ¶ plus Ã©levÃ© dÃ©cote plus rapidement les observations le plus anciennes.  
On impose donc ici un biais inductif : le poids de la dÃ©pendance entre deux tokens diminue de maniÃ¨re exponentielle au fil du temps avec un facteur ğœ¶ agnostique Ã  lâ€™entrÃ©e. Cette propriÃ©tÃ© favorise les dÃ©pendances locales et limite les dÃ©pendances Ã  long terme.  
Le calcul de lâ€™EMA peut Ãªtre reprÃ©sentÃ© comme n convolutions individuelles pouvant Ãªtre calculÃ©es efficacement par FFT.  

â€¢ <u>EMA utilisÃ©e dans le Mega</u> :  
Le Mega utilise une EMA Â« amortie Â» multidimensionnelle. Câ€™est-Ã -dire que dans lâ€™Ã©quation de lâ€™EMA Â« amortie Â», $$ğ²_t = ğœ¶ âŠ™ ğ±_t + (1âˆ’ğœ¶ âŠ™ ğœ¹) âŠ™ ğ²_{tâˆ’1}$$ oÃ¹ un paramÃ¨tre $$ğœ¹$$ in $$[0,1]^d$$ est introduit qui reprÃ©sente le facteur dâ€™amortissement, $$x$$ est Ã©tendue Ã  $$h$$ dimensions via une matrice dâ€™expansion $$ğœ·$$ in $$R^{d \times h}$$.  
Lâ€™Ã©quation devient alors $$ğ²_{t,j} = ğœ¼_j^{\intercal} ğ¡_t^{(j)}$$ avec $$ğ¡_t^{(j)} = ğœ¶_j âŠ™ ğ®_t^{(j)}+ (1âˆ’ğœ¶_j âŠ™ ğœ¹_j) âŠ™ ğ¡_{tâˆ’1}^{(j)}$$ et $$ğœ¼ \in R^{d \times h}$$ est la matrice de projection qui renvoie lâ€™Ã©tat cachÃ© en $$h$$ dimension Ã  la sortie unidimensionnelle $$ğ²_{t,j} \in \mathbb{R}$$.  

Preuve que lâ€™ EMA Â« amortie Â» multidimensionnelle peut Ãªtre calculÃ©e comme une convolution et donc par FTT (en fixant $$d = 1$$ pour $$ğœ¶$$ et $$ğœ¹$$) :  
On a $$ğ²_t = ğœ¼^{\intercal} ğ¡_t$$ avec $$ğ¡_t = ğœ¶_j âŠ™ ğ®_t + (1âˆ’ğœ¶ âŠ™ ğœ¹) âŠ™ ğ¡_{tâˆ’1}$$. Notons $$Ï• = 1âˆ’ğœ¶ âŠ™ ğœ¹$$.  
Alors :  $$ğ¡_t = ğœ¶ âŠ™ ğ®_t + (1âˆ’ğœ¶ âŠ™ ğœ¹) âŠ™ ğ¡_{tâˆ’1} = ğœ¶ âŠ™ ğœ· ğ±_t + Ï• âŠ™ ğ¡_{tâˆ’1}$$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; et $$ğ²_t = ğœ¼^{\intercal} ğ¡_t = ğœ¼^{\intercal} (ğœ¶ âŠ™ ğœ· ğ±_t + Ï• âŠ™ ğ¡_{tâˆ’1})$$    
Ensuite, en dÃ©roulant les deux Ã©quations ci-dessus, on obtient explicitement :  
Etape 0 : $$ğ¡_1 = ğœ¶ âŠ™ ğœ·ğ±_1 + Ï• âŠ™ ğ¡_0$$  
Etape 1 : $$ğ¡_2 = ğœ¶ âŠ™ ğœ·ğ±_2 + Ï• âŠ™ ğ¡_1$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$= ğœ¶ âŠ™ ğœ· ğ±_2 + Ï• âŠ™ (Ï• âŠ™ ğ¡_0 + ğœ¶ âŠ™ ğœ· ğ±_1) = ğœ¶ âŠ™ ğœ· ğ±_2 + Ï•^2 âŠ™ ğ¡_0 + Ï• âŠ™ ğœ¶ âŠ™ ğœ· ğ±_1$$  
â€¦  

Et de mÃªme :  
Etape 0 : $$ğ²_1 = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_1 + Ï• âŠ™ğ¡0) = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_1 + ğœ¼^{\intercal} Ï• âŠ™ ğ¡_0$$    
Etape 1 : $$ğ²_2 = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_2 + ğœ¼^{\intercal} Ï• âŠ™ ğ¡_1$$ 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $$= ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_2 + ğœ¼^{\intercal} Ï• âŠ™ (ğœ¶ âŠ™ ğœ· ğ±_1 + Ï• âŠ™ ğ¡_0) = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_2 + ğœ¼^{\intercal} Ï• âŠ™ ğœ¶ âŠ™ ğœ· ğ±_1 + ğœ¼^{\intercal} Ï•^2 âŠ™ ğ¡_0$$  
â€¦  
Etape $$t$$ :  $$ğ²_t = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_t + â€¦ + ğœ¼^{\intercal} Ï•_{tâˆ’1} âŠ™ ğœ¶ âŠ™ ğœ· ğ±_{tâˆ’1} + ğœ¼^{\intercal} Ï•^t âŠ™ ğ¡_0$$.  

Et donc $$ğ² = ğ’¦ * ğ± + ğœ¼^{\intercal} Ï•^t âŠ™ ğ¡_0$$ avec $$ğ’¦ = (ğœ¼^{\intercal} (ğœ¶ âŠ™ ğœ·), ğœ¼^{\intercal} (Ï• âŠ™ ğœ¶ âŠ™ ğœ·), â€¦, ğœ¼^{\intercal}(Ï•^t âŠ™ ğœ¶ âŠ™ ğœ·) \in \mathbb{R}^n$$.  
$$ğ’¦$$ est calculÃ© dans le Mega via le produit de Vandermonde, ce qui nous rappelle la mÃ©thode utilisÃ©e dans le S4D.    
Pour plus de dÃ©tails sur les liens entre le Mega et le S4, le lecteur est invitÃ© Ã  consulter les messages Ã©changÃ©s entre Albert Gu et les auteurs du Mega trouvables en commentaires de la soumission du Mega sur [Open Review](https://openreview.net/forum?id=qNLe3iq2El). Mais en rÃ©sumÃ©, en Ã©tablissant un lien entre lâ€™Ã©tape de discrÃ©tisation des SSM et lâ€™EMA amortie, il est possible de voir le Mega vue comme un hybride SSM/Attention simplifiant le S4 pour qu'il soit Ã  valeur rÃ©elle plutÃ´t que complexe. 

<br><br>

### <span style="color: #51C353"> **SGConv** </span>

<br><br>

## <span style="color: #FFBF00"> **Applications des SSM** </span>


<br><br><br>

# <span style="color: #FF0000"> **Conclusion** </span>

<br><br><br>

# <span style="color: #FF0000"> **RÃ©fÃ©rences** </span>
