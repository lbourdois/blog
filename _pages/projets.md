---
permalink: /projets/
title: "Projets"
classes: wide
---

Sur cette page vous pouvez trouver les contenus sur lesquels j'ai travaill√© (√† titre personnel ou professionnel) mais qui sont r√©f√©renc√©s sur d'autres sites que mon blog personnel. Il s'agit principalement de traductions de cours, et des cr√©ations de jeux de donn√©es et de mod√®les.

<br><br>
# Vue d'ensemble

<iframe
	src="https://lbourdois-roadmap.static.hf.space"
	frameborder="0"
	width="950"
	height="1500"
></iframe>

<br><br><br>

# Mod√®les et jeux de donn√©es en fran√ßais

## FAT5
Le FAT5 est une impl√©mentation du [T5](https://arxiv.org/abs/1910.10683) en PyTorch avec un objectif [UL2](https://arxiv.org/abs/2205.05131) optimis√© pour GPGPU d√©velopp√© avec [Boris ALBAR](https://b-albar.github.io/portfolio/).  
Elle utilise des noyaux CUDA et Triton personnalis√©s ainsi que des optimisations sp√©cifiques pour augmenter le d√©bit et r√©duire l'utilisation de la m√©moire pour l'entra√Ænement et l'inf√©rence d'un facteur 2 par rapport √† l'impl√©mentation originale disponible dans Hugging Face.  
Nous l'avons appliqu√©e en pr√©-entra√Ænant un mod√®le en fran√ßais de 147M param√®tres en utilisant uniquement une A100. Nous estimons ainsi pouvoir ramener le prix de pr√©-entra√Ænement d'un tel mod√®le √† seulement 1200‚Ç¨ (estimation faite sur une instance Sesterce).   
Le code de pr√©-entrainement est disponible sur [GitHub](https://github.com/catie-aq/flashT5) sous licence Apache-2.0 et les poids du mod√®le entra√Æn√© sur le compte [Hugging Face du CATIE]([https://huggingface.co/CATIE-AQ](https://huggingface.co/collections/CATIE-AQ/catie-french-fat5-ul2-677697a35feea336389d6403)). Un article de blog d√©taillant notre m√©thodologie est disponible [ici](https://huggingface.co/spaces/CATIE-AQ/FAT5-rapport).

## NER
Les NERmemBERT constituent une famille de mod√®les de Reconnaissance d‚ÄôEntit√©s Nomm√©es en fran√ßais capable d‚Äô√©tiqueter jusqu‚Äô√† 4 entit√©s (Personnalit√©s, Lieux, Organisations, Divers tel que des noms d‚Äô≈ìuvre, de maladies, etc.). Ils sont disponibles en taille base (110M ou 136M de param√®tres) et large (336M), g√©rant des contextes allant de 512 √† 8192 tokens. Les poids sont disponibles gratuitement en open-source, tout comme les jeux de donn√©es ayant servis √† l‚Äôentra√Ænement. Le tout est disponible sur le compte [Hugging Face du CATIE](https://huggingface.co/collections/CATIE-AQ/catie-french-ner-pack-658aefafe3f7a2dcf0e4dbb4). Un article de blog d√©taillant la m√©thodologie adopt√©e est disponible [ici](https://lbourdois.github.io/blog/NER/).<br>
Ils ont √©t√© t√©l√©charg√©s plus de 180 000 fois depuis leur mise en ligne.

## Question Answering
Les QAmemBERT constituent une famille de r√©ponse aux questions en fran√ßais capable d‚Äôindiquer si la r√©ponse √† une question est pr√©sente ou pas dans un texte de contexte associ√©. Ils sont disponibles en taille base (110M ou 136M de param√®tres) et large (335M), g√©rant des contextes allant de 512 √† 8192 tokens. Les poids sont disponibles gratuitement en open-source, tout comme le jeu de donn√©es ayant servis √† l‚Äôentra√Ænement. Le tout est disponible sur le compte [Hugging Face du CATIE](https://huggingface.co/collections/CATIE-AQ/catie-french-qa-pack-650821750f44c341cdb8ec91). Un article de blog d√©taillant la m√©thodologie adopt√©e est disponible [ici](https://lbourdois.github.io/blog/QA/).<br>
Ils ont √©t√© t√©l√©charg√©s plus de 155 000 fois depuis leur mise en ligne.

## DFP
*Dataset of French Prompts* (DFP) contient 113 129 978 lignes portant sur 30 t√¢ches de NLP diff√©rentes.  
724 prompts ont √©t√© √©crits sous forme imp√©rative, de tutoiement et de vouvoiement afin de couvrir autant que possible les donn√©es de pr√©-entra√Ænement utilis√©es par le mod√®le qui utilisera ces donn√©es et qui nous sont inconnues.

Les colonnes *inputs* et *targets* suivent le m√™me format que l'ensemble de donn√©es [xP3](https://huggingface.co/datasets/bigscience/xP3) de Muennighoff et al.  
L'ensemble des d√©tails est disponible sur [Hugging Face](https://huggingface.co/datasets/CATIE-AQ/DFP).<br>
Il a √©t√© t√©l√©charg√© plus de 65 000 fois depuis sa mise en ligne.

## La marmite
Projet toujours en cours.  
Il s'agit de proposer un √©quivalent en fran√ßais du jeu de donn√©es [*the cauldron*](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) afin de pouvoir entra√Æner un VLM en fran√ßais.  
Ce jeu de donn√©es prendra en compte des donn√©es d'OCR (voir celles d√©j√† disponibles en ligne [ici](https://huggingface.co/collections/lbourdois/french-ocr-datasets-67c8d3152330f11227e0d108)), de captionning (voir celles d√©j√† disponibles en ligne [ici](https://huggingface.co/collections/lbourdois/french-caption-datasets-67c8d2227284a5daa00c50b9)), de VQA (voir celles d√©j√† disponibles en ligne [ici](https://huggingface.co/collections/lbourdois/french-vqa-datasets-67c8d1a162a23ef0e9a2bc89)) et de raisonnement.  
Les sous-jeux de donn√©es d√©j√† en ligne ont √©t√© t√©l√©charg√© plus de 15 000 fois depuis leur mise en ligne.

<br><br><br>

# Traductions
## Cours de Yann Le Cun et Alfredo Canziani de la NYU
Cette traduction a √©t√© la plus longue √† effectuer s'√©talant de 2020 √† 2022.  
Le contenu est structur√© en 19 unit√©s r√©parties sur 33 vid√©os üé• de cours (cours magistraux et travaux dirig√©s) d‚Äôune dur√©e totale d‚Äôenviron 45H, 74 pages web üåê r√©sumant les vid√©os via les notes prises par les √©tudiants pendant le cours, et 16 notebooks Jupyter üìì (en PyTorch) utilis√©s lors des TD. Enfin un jeu de donn√©es de plus de 3000 donn√©es parall√®les v√©rifi√©es manuellement a √©t√© cr√©√© pour entra√Æner un mod√®le de traduction.  
Vous pouvez retrouver toutes ces ressources sur le site internet d√©di√© qui a √©t√© con√ßu √† l'occasion : [https://lbourdois.github.io/cours-dl-nyu/](https://lbourdois.github.io/cours-dl-nyu/).

## Cours de Hugging Face ü§ó
### Cours de NLP
En 2022, j'ai traduit le cours de traitement automatique du langage de Hugging Face.  
Le contenu est structur√© en 10 chapitres comprenant un total de 76 vid√©os üé• d'une dur√©e totale d'environ 5H, de 78 pages web üåê et 61 notebooks Jupyter üìì  (en PyTorch et Tensorflow).  
Vous pouvez retrouver toutes ces ressources sur le site de [Hugging Face](https://huggingface.co/learn/nlp-course/fr/chapter1/1).

### Cours d'audio
En 2023, j'ai traduit le cours de traitement automatique du langage de Hugging Face.  
Le contenu est structur√© en 8 unit√©s r√©parties sur 46 pages web üåê.  
Vous pouvez retrouver toutes ces ressources sur le site de [Hugging Face](https://huggingface.co/learn/audio-course/fr/).

### Cours sur les mod√®les de diffusion
En 2023, j'ai traduit le cours sur les mod√®les de diffusion de Hugging Face.  
Le contenu est structur√© en 4 chapitres portant sur 17 pages web üåê et 8 notebooks Jupyter üìì (en PyTorch).  
Vous pouvez retrouver toutes ces ressources sur le GitHub de [Hugging Face](https://github.com/huggingface/diffusion-models-class/tree/main/units/fr) (le contenu n'ayant pas encore √©t√© propag√© sur le site officiel).

### Cours sur les agents IA
En 2025, j'ai particip√© avec [Kim NOEL](https://github.com/knoel99) √† la traduction du cours sur les agents IA de Hugging Face.  
Le contenu est structur√© en 4 unit√©s (+ 3 bonus) r√©parties sur 74 pages web üåê et 16 notebooks Jupyter üìì.  
Vous pouvez retrouver toutes ces ressources sur le site de [Hugging Face](https://huggingface.co/learn/agents-course/fr).
