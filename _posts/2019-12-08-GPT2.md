---
title: "ILLUSTRATION DU GPT2"
categories:
  - NLP
tags:
  - french GPT2 NLP
  - GPT2 expliqué en français NLP
  - GPT2 NLP français
  - Illustration du GPT2 en français NLP
  - GPT2 français
  - GPT2 french
excerpt : "NLP - Illustration du GPT2 de Radford et al."
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/NLP_radom_blog.png"
    teaser : "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-sizes.png"
author_profile: false
sidebar:
    nav: sidebar-sample
classes: wide
---


# <span style="color: #FF0000"> **Avant-propos** </span>
 
Cet article est une traduction de l’article de Jay Alammar : [The illustrated GPT-2 (Visualizing transformer language model)](https://jalammar.github.io/illustrated-gpt2/).<br>
Merci à lui de m'avoir autorisé à effectuer cette traduction.
<br><br><br>


# <span style="color: #FF0000"> **Introduction** </span>
Cette année (2019), le [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (*Generative Pretrained Transformer 2*) de Radford et al. a fait preuve d’une impressionnante capacité à rédiger des essais cohérents et passionnés dépassant ce qui était envisageable avec les modèles linguistiques jusqu'ici à notre disposition. Le GPT-2 n’est pas une architecture particulièrement nouvelle (le GTP-2 étant la version 2 du [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) dévoilé en juin 2018 par Radford et al.), elle est très similaire à celle du decodeur du *Transformer* de Vaswani et al. Le GPT-2 est cependant basé sur un *transformer* entraîné sur un corpus massif de données. Dans cet article, nous allons nous intéresser à l’architecture qui a permis au modèle de produire ces résultats.
<br>
Le but ici est également de compléter l’article sur les *transformer* avec plus de visuels expliquant leur fonctionnement interne et comment ils ont évolué depuis l’article original. 
<br><br><br>


# <span style="color: #FF0000"> **1. GPT-2 et modélisation du langage** </span>
## <span style="color: #FFBF00"> **1.1 Qu’est-ce qu’un modèle linguistique ?** </span>
Dans la partie 4 de l’article sur le [word embedding](https://lbourdois.github.io/blog/nlp/word_embedding/), nous avons examiné ce qu’est un modèle linguistique.
Essentiellement un modèle d’apprentissage automatique qui est capable de regarder une partie d’une phrase et de prédire le mot suivant. Les modèles les plus connus sont les claviers de smartphones qui suggèrent le mot suivant en fonction de ce que vous avez déjà tapé.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/swiftkey-keyboard.png">
</figure>
</center>

En ce sens, nous pouvons dire que le GPT-2 est fondamentalement une fonction de prédiction de mots d’une application de clavier mais qui est beaucoup plus sophistiqué que ce que votre téléphone a. Le GPT-2 a été entraîné sur un énorme jeu de données de 40 Go appelé WebText que les chercheurs d’OpenAI ont trouvé sur internet. Pour comparer en termes de taille de stockage, l’application clavier que j’utilise, SwiftKey, occupe 78MBs d’espace. La plus petite variante du GPT-2 occupe 500MBs de stockage pour stocker tous ses paramètres. La plus grande variante de GPT-2 est 13 fois plus grande, prenant ainsi plus de 6,5 Go d’espace de stockage.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-sizes.png">
</figure>
</center>

Une excellente façon d’expérimenter le  GPT-2 est d’utiliser [Write with Transformer](https://transformer.huggingface.co/doc/gpt2-large) d'Hugging face qui affiche dix prédictions possibles pour le mot suivant (avec leur score de probabilité). 
Vous pouvez sélectionner un mot puis voir la liste suivante des prédictions pour continuer à écrire votre extrait.
<br><br>


## <span style="color: #FFBF00"> **1.2 *Transformers* pour la modélisation du langage** </span>
Comme nous l’avons vu dans l’article sur le [*Transformer*](https://lbourdois.github.io/blog/nlp/Transformer/), le *transformer* basique est composé d’un encodeur et d’un décodeur (chacun est une pile de ce que nous pouvons appeler des « *transformer blocks* »). Cette architecture est appropriée pour la traduction automatique, un problème où les architectures encodeur-décodeur ont connu du succès dans le passé.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-encoder-decoder.png">
</figure>
</center>

Une grande partie des travaux de recherche apparus depuis a vu l’architecture se débarrasser de l’encodeur ou du décodeur et n’utiliser qu’une seule pile de « *transformer blocks* ». Empilant les piles aussi haut que possible, leur fournissant des textes en quantité massive pour l’entraînement.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt-2-transformer-xl-bert-3.png">
</figure>
</center>

A quelle hauteur peut-on empiler ces blocs ? Il s’avère que c’est l’un des principaux facteurs de distinction entre les différentes tailles de modèles du GPT2 :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-sizes-hyperparameters-3.png">
</figure>
</center>
<br><br>


## <span style="color: #FFBF00"> **1.3 Une différence par rapport à BERT** </span>
Le GPT-2 est construit à l’aide de blocs décodeurs. BERT, pour sa part, utilise des blocs d’encodeurs. Nous examinerons la différence dans une section suivante. Mais l’une des principales différences entre les deux est que le GPT-2, comme les modèles de langage traditionnels, produit un seul *token* à la fois. Invitons par exemple un GPT-2 bien entraîné à réciter la première loi de la robotique :  « *A robot may not injure a human being or, through inaction, allow a human being to come to harm* » (un robot ne peut pas blesser un être humain ou, par son inaction, permettre qu'un être humain soit blessé).
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/xlnet/gpt-2-output.gif">
</figure>
</center>

La façon dont fonctionnent réellement ces modèles est qu’après chaque *token* produit, le *token* est ajouté à la séquence d'entrée. Cette nouvelle séquence devient l’entrée du modèle pour l'étape suivante. Cette idée est appelée « autorégression » et a permis aux RNNs d’être efficaces.
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif">
</figure>
</center>
Le GPT2 et certains modèles plus récents comme [TransformerXL](https://arxiv.org/abs/1901.02860v3) et [XLNet](https://arxiv.org/abs/1906.08237v2) sont de nature autorégressive. BERT ne l’est pas. C’est un compromis. En perdant l’autorégression, BERT a acquis la capacité d'enchâsser le contexte des deux côtés d’un mot pour obtenir de meilleurs résultats. XLNet ramène l’autorégression tout en trouvant une autre façon d’intégrer le contexte des deux côtés.
<br><br>


## <span style="color: #FFBF00"> **1.4 L’évolution des blocs du *transformer*** </span>
La [publication](https://arxiv.org/abs/1706.03762) initiale sur les *transformers* introduisait deux types de blocs :
<br>
### <span style="color: #51C353"> **1.4.1 Le bloc encodeur** </span>
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-encoder-block-2.png">
  <figcaption>
Un bloc encodeur de l’article d’origine peut recevoir des entrées jusqu’à une certaine longueur maximale (512 *tokens*). Si une séquence d’entrée est plus courte que cette limite, nous avons simplement à rembourrer le reste de la séquence.  </figcaption>
</figure>
</center>
<br>
### <span style="color: #51C353"> **1.4.2 Le bloc décodeur** </span>
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-decoder-block-2.png">
</figure>
</center>
Une différence clé dans la couche d’auto-attention est qu’elle masque les *tokens* futurs. Non pas en changeant le mot en [mask] comme BERT, mais en interférant dans le calcul de l’auto-attention en bloquant les informations des *tokens* qui sont à la droite de la position à calculer.

Par exemple, si nous devons mettre en évidence le chemin de la position #4, nous pouvons voir qu’il est seulement permis de regarder le *token* présent et les précédents :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-decoder-block-self-attention-2.png">
</figure>
</center>

Il est important que la distinction entre l’auto-attention (ce que BERT utilise) et l’auto-attention masquée (ce que le GPT-2 utilise) soit claire. Un bloc d’auto-attention normal permet à une position d’atteindre le sommet des *tokens* à sa droite. L’auto-attention masquée empêche que cela se produise :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-and-masked-self-attention.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.4.3 Que le bloc décodeur** </span>
Suite à l’article original, [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf) a proposé une autre disposition du *transformer* qui est capable de faire de la modélisation linguistique. Ce modèle s’est débarrassé de l’encodeur pour ne garder que le décodeur. Pour cette raison, appelons le modèle « Transformer-Décodeur ». Ce premier modèle de langage était constitué d’une pile de six blocs décodeurs :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-decoder-intro.png">
  <figcaption>
Les blocs décodeurs sont identiques. Nous avons développé le premier pour que vous puissiez voir que sa couche d’auto-attention est la variante masquée. Notez que le modèle peut maintenant adresser jusqu’à 4 096 tokens contre 512 dans l’original.
</figcaption>
</figure>
</center>
Ces blocs sont très similaires aux blocs décodeurs d’origine sauf qu’ils suppriment la deuxième couche d’auto-attention. Le GPT-2 d'OpenAI utilise uniquement ces blocs décodeurs.
<br><br>


## <span style="color: #FFBF00"> **1.5 L’intérieur du GPT-2** </span>
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt-2-layers-2.png">
  <figcaption>
Le GPT-2 peut traiter 1024 jetons. Chaque jeton parcourt tous les blocs décodeurs le long de son propre chemin.
</figcaption>
</figure>
</center>

La façon la plus simple d’exécuter un GPT-2 entraîné est de lui permettre de se promener de lui-même (ce qui est techniquement appelé *generating unconditional samples*). Nous pouvons aussi le pousser à ce qu’il parle d’un certain sujet (*generating interactive conditional samples*). Dans le premier cas, nous pouvons simplement lui donner le *token* de démarrage et lui faire commencer à générer des mots. A noter que le GPT2 utilise `<|endoftext|>` comme *token* de démarrage. Appelons-le `<s>` à la place dans la suite afin de simplifier les graphiques.
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif">  
</figure>
</center>

Le *token* est traité successivement à travers toutes les couches puis un vecteur est produit le long de ce chemin. Ce vecteur peut être évalué par rapport au vocabulaire du modèle (tous les mots que le modèle connaît, 50 000 mots dans le cas du GTP-2). Dans ce cas, nous avons sélectionné le *token* ayant la probabilité la  plus élevée : « *the* ».

Sur votre téléphone, si vous cliquez à chaque fois sur le mot suggéré par votre application clavier, la suggestion peut parfois restée coincée dans des boucles répétitives où la seule issue est de cliquer sur le deuxième ou le troisième mot suggéré. La même chose peut se produire ici.

GPT-2 a un paramètre appelé top-k que nous pouvons utiliser pour que le modèle considère d'autres mots d’échantillonnage autres que le top mot (ce qui est le cas lorsque top-k = 1).

Dans l’étape suivante nous ajoutons le résultat de la première étape à notre séquence d’entrée et le modèle fait sa prochaine prédiction :
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/gpt2/gpt-2-simple-output-3.gif">
</figure>
</center>
Notez que la deuxième étape est la seule qui est active dans ce calcul. Chaque couche du GPT-2 a conservé sa propre interprétation du premier *token* et l’utilisera dans le traitement du deuxième *token* (nous y reviendrons plus en détail dans la section suivante sur l’auto-attention). GPT-2 ne réinterprète pas le premier *token* à la lumière du second.
<br>

### <span style="color: #51C353"> **1.5.1 Encodage de l'entrée** </span>
Regardons plus en détail pour mieux comprendre le modèle. Commençons par l’entrée.     

Comme dans d’autres modèles de NLP dont nous avons déjà parlé, le GPT-2 recherche l’enchâssement du mot d’entrée dans sa matrice d'enchâssement (obtenue après entraînement).
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-token-embeddings-wte-2.png">
  <figcaption>
Chaque ligne est un l'enchâssement d'un mot c’est à dire un vecteur représentant un mot et capturant une partie de sa signification. La taille de cette liste est différente selon la taille du GPT2. Le plus petit modèle utilise une taille d’enchâssement de 768 par *token*.
</figcaption>
</figure>
</center>

Ainsi au début nous recherchons l’enchâssement du *token* de départ `<s>` dans la matrice. Avant de le transmettre au premier bloc du modèle, nous devons enchâsser l'encodage positionnel (un signal qui indique aux blocs l’ordre des mots dans la séquence). Une partie du modèle entraîné contient une matrice ayant un vecteur d'encodage positionnel pour chacune des 1024 positions de l’entrée.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-positional-encoding.png">
</figure>
</center>

Schématiquement :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-input-embedding-positional-encoding-3.png">
  <figcaption>
Envoyer un mot au premier bloc, c’est rechercher son enchâssement et additionner le vecteur d'encodage positionnel pour la position #1.</figcaption>
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.5.2 Voyage dans le bloc** </span>
Le premier bloc peut maintenant traiter le premier *token* en le faisant passer d’abord par le processus d’auto-attention puis par sa couche *feed-forward*. Une fois le traitement effectué, le bloc envoie le vecteur résultant pour qu’il soit traité par le bloc suivant. Le processus est identique dans chaque bloc mais chaque bloc a des poids qui lui sont propres dans l’auto-attention et dans les sous-couches du réseau neuronal.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-transformer-block-vectors-2.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.5.3 Rappel de l’auto-attention** </span>
Le langage dépend beaucoup du contexte. Par exemple, regardez la deuxième loi de la robotique :

> « *A robot must obey the orders given **it** by human beings except where **such orders** would conflict with the **First Law*** »  
 (Un robot doit obéir aux ordres donnés par les êtres humains, sauf si ces ordres entrent en conflit avec la première loi.)

A trois endroits dans la phrase des mots renvoient à d’autres mots. Il n’y a aucun moyen de comprendre ou de traiter ces mots sans enchâsser le contexte auquel ils font référence. Lorsqu’un modèle traite cette phrase, il doit pouvoir savoir que :
* ***it*** se réfère au robot
* ***such** orders* se réfèrent à la partie antérieure de la loi, à savoir « les ordres donnés par des êtres humains » 
* ***First Law*** se réfère à l’ensemble de la première loi

C’est ce que réalise l’auto-attention. Pour ce faire, on attribue des scores à la pertinence de chaque mot du segment et on additionne leur représentation vectorielle.

A titre d’exemple, la couche d’auto-attention dans le bloc du haut fait attention à « *a robot* » lorsqu’elle traite le mot « *it* ». Le vecteur qui est ensuite passer au réseau *feed-forward* est la somme des vecteurs de chacun des trois mots multipliés par leurs scores.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-example-2.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.5.4 Le processus de l’auto-attention** </span>
L’auto-attention est traitée le long du parcours de chaque *token*. Les composantes significatives sont trois vecteurs :
* **Query** : la requête est une représentation du mot courant. Elle est utilisée pour donner un score au mot vis-à-vis des autres mots (en utilisant leurs clés). 
* **Key** : les vecteurs clés sont comme des labels pour tous les mots de la séquence. C’est contre eux que nous nous mesurons dans notre recherche de mots pertinents.
* **Value** : les vecteurs de valeurs sont des représentations de mots réels. Une fois que nous avons évalué la pertinence de chaque mot, ce sont les valeurs que nous additionnons pour représenter le mot courant.

Une analogie grossière est de penser à la recherche dans un classeur. La requête (*query*) est le sujet que vous recherchez. Les clés (*key*) sont comme les étiquettes des chemises à l’intérieur de l’armoire. Lorsque vous faites correspondre la requête et la clé, nous enlevons le contenu du dossier. Le contenu correspond au vecteur de valeur (*value*). Sauf que vous ne recherchez pas seulement une valeur, mais un mélange de valeurs à partir d’un mélange de dossiers.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-example-folders-3.png">
</figure>
</center>

Multiplier le vecteur de requête par chaque vecteur clé produit un score pour chaque dossier (techniquement : le produit scalaire suivi de la fonction softmax).
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-example-folders-scores-3.png">
</figure>
</center>

Nous multiplions chaque valeur par son score et sommons. Cela donne le résultat de notre auto-attention.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-value-vector-sum.png">
</figure>
</center>

Cette opération permet d’obtenir un vecteur pondéré où on peut voir par exemple que l’attention a été a prêté à 50% sur le mot « *robot* », 30% au mot « *a* », et 19% au mot « *it* ».

Dans la partie 2 on s’intéressera de plus près à l’auto-attention. Mais d’abord, continuons notre cheminement vers le haut de la pile et vers la sortie du modèle.
<br>

### <span style="color: #51C353"> **1.5.5 Sortie du modèle** </span>
Lorsque le bloc le plus haut du modèle produit son vecteur de sortie (le résultat de sa propre auto-attention suivie de son propre réseau *feed-forward*), le modèle multiplie ce vecteur par la matrice d’enchâssement.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-output-projection-2.png">
</figure>
</center>

Rappelons que chaque ligne de la matrice d’enchâssement correspond à l'enchâssement d'un mot dans le vocabulaire du modèle. Le résultat de cette multiplication est interprété comme un score pour chaque mot du vocabulaire du modèle.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-output-scores-2.png">
</figure>
</center>

Nous pouvons simplement sélectionner le *token* avec le score le plus élevé (top_k = 1). Mais de meilleurs résultats sont obtenus si le modèle tient également compte d’autres termes. Ainsi, une bonne stratégie consiste à tirer au hasard un mot provenant du vocabulaire. Chaque mot ayant comme probabilité d’être sélectionner, le score qui lui a été attribué (de sorte que les mots avec un score plus élevé ont une plus grande chance d’être sélectionnés). Un terrain d’entente consiste à fixer top_k à 40 et à demander au modèle de prendre en compte les 40 mots ayant obtenu les scores les plus élevés.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-output.png">
</figure>
</center>
Le modèle a alors terminé une itération aboutissant à la production d’un seul mot. Le modèle recommence alors le processus jusqu’à ce que le contexte entier soit généré (1024 *tokens*) ou qu’un *token* de fin de séquence soit produit.
<br><br>


## <span style="color: #FFBF00"> **1.6 Fin de la partie 1** </span>
Voici donc un succinct aperçu du fonctionnement du GPT2. Le détail de ce qui se passe à l’intérieur de la couche d’auto-attention fait l’objet de la deuxième partie.

Notons que nous avons procéder dans cette première partie à quelques simplifications excessives :
* Nous employons les termes « mots » et « *tokens* » de façon interchangeable. 
Mais en réalité, le GPT2 utilise le ***Byte Pair Encoding*** pour créer les *tokens* dans son vocabulaire.
Cela signifie que les *tokens* sont généralement des parties des mots. Je vous invite à voir l'article sur les [*tokenizers*](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) du blog pour plus de détails.
* Nous avons pris des libertés dans la rotation/transposition des vecteurs pour mieux gérer la mise en page dans les images. Au moment de l’implémentation, il faut être plus précis.
* Les *transformers* utilisent des couches de normalisation dans les blocs. Nous nous sommes davantage concentrés sur l’auto-attention dans cet article. 
<br><br><br>



# <span style="color: #FF0000"> **2. Illustration de l’auto-attention** </span>
Plus tôt nous avons montré l’image suivante afin d’illustrer l’auto-attention appliquée pour le mot « *it* » 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-1-2.png">
</figure>
</center>
Dans cette partie, nous examinons en détail la façon de procéder. Notre volonté est d’essayer de donner un sens à ce qui arrive à chaque mot. C’est pourquoi sur les graphiques nous utilisons de nombreux vecteurs uniques. Les implémentations réelles se font en multipliant des matrices géantes ensemble. L’objectif ici est de se concentrer sur l’intuition de ce qui se passe au niveau des mots.
<br><br>


## <span style="color: #FFBF00"> **2.1 Auto-attention (sans masque)** </span>
Commençons par regarder l’auto-attention « originale » telle qu’elle est calculée dans un bloc encodeur. L’auto-attention s’applique en trois étapes principales :
* Création des vecteurs *Query*, *Key* et *Value* pour chaque chemin.
* Pour chaque *token* d’entrée, on utilise son vecteur de requête pour lui attribuer un score par rapport à tous les autres vecteurs clés. 
* Sommation des vecteurs de valeurs après les avoir multipliés par leurs scores associés.
<br>


### <span style="color: #51C353"> **2.1.1 Création des vecteurs *Query*, *Key* et *Value*** </span>
Pour le premier *token*, nous prenons sa requête et la comparons à toutes les clés. Cela produit un score pour chaque clé. La première étape de l’auto-attention consiste à calculer les trois vecteurs pour chaque *token* (ignorons les têtes d’attention pour le moment) :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-1.png">
</figure>
</center>
<br>


### <span style="color: #51C353"> **2.1.2 Score** </span>
Nous nous concentrons sur le premier *token*. Nous multiplions sa requête par tous les autres vecteurs clés pour obtenir un score pour chacun des quatre *tokens*.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-2.png">
</figure>
</center>
<br>


### <span style="color: #51C353"> **2.1.3 Somme** </span>
Nous pouvons maintenant multiplier les scores par les vecteurs de valeurs. Une valeur avec un score élevé constituera une grande partie du vecteur résultant une fois que nous les aurons additionnés.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-3-2.png">
  <figcaption>
Plus le score est bas, plus le vecteur de valeur est transparent. C’est pour indiquer comment le fait de multiplier par un petit nombre dilue les valeurs du vecteur.
  </figcaption>
</figure>
</center>
Si nous faisons la même opération pour chaque *token*, nous obtenons un vecteur représentant et tenant compte du contexte pour chacun d’eux. Ces vecteurs sont ensuite présentés à la sous-couche suivante du bloc (le réseau *feed-forward*).

Résumé :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-summary.png">
</figure>
</center>
<br><br>


## <span style="color: #FFBF00"> **2.2 Auto-attention (avec masque)** </span>
L’auto-attention masquée est identique à l’auto-attention, sauf à l’étape 2.

Supposons que le modèle n’a que deux *tokens* en entrée et que nous observons le deuxième *token*. Dans ce cas, les deux derniers *tokens* sont masqués. Le modèle attribue alors toujours aux futurs *tokens* un score de 0.

<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/masked-self-attention-2.png">
</figure>
</center>
<br><br>

Ce « masquage » est souvent mis en œuvre sous la forme d’une matrice appelée masque d’attention.

 A titre d’exemple, supposons avoir une séquence de quatre mots : « *robot must obey orders* ».                        

Sous forme matricielle, nous calculons les scores en multipliant une matrice de requêtes par une matrice clés :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/queries-keys-attention-mask.png">
</figure>
</center>
<br><br>

Après la multiplication, nous appliquons notre masque d’attention. Il règle les cellules que l’on veut masquer sur -inf ou un nombre négatif très important (par exemple -1 milliard pour le GPT2) :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-attention-mask.png">
</figure>
</center>
<br><br>

Ensuite, l’application de la fonction softmax produit les scores que nous utilisons pour l’auto-attention :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-attention-masked-scores-softmax.png">
</figure>
</center>
<br><br>
La signification de ce tableau de scores est la suivante :
* Lorsque le modèle traite le premier exemple du jeu de données (ligne #1), qui ne contient qu’un seul mot (« *robot* »), 100% de son attention se porte sur ce mot.
* Lorsque le modèle traite le deuxième exemple du jeu de données (ligne #2), qui contient les mots (« *robot must* »), lorsqu’il traite le mot « *must* », 48% de son attention porte sur « *robot* » et 52%  sur « *must* ».
* Et ainsi de suite.
<br><br>


## <span style="color: #FFBF00"> **2.3 L’auto-attention masquée du GPT-2** </span>    
Le GPT-2 conserve les vecteurs clé et valeur des *tokens* qu’il a déjà traité afin de ne peut à avoir à les recalculer à chaque fois qu’un nouveau *token* est traité. En pratique, le processus se déroule comme suit :    
<br>
### <span style="color: #51C353"> **2.3.1  Etape 1 : Création des vecteurs *Query*, *Key* et *Value*** </span>    
Supposons que le modèle traite le mot « *it* ». Si nous parlons du bloc du bas, alors l’entrée pour ce token serait l’enchâssement de « *it* » + l’encodage positionnel pour la position #9 :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-1.png">
</figure>
</center>
Chaque bloc a ses propres poids.  Nous nous servons de la matrice de poids pour créer les vecteurs des requêtes, des clés et des valeurs. Cela consiste en pratique à une simple multiplication.      

Le vecteur résultant de la multiplication est la concaténation des vecteurs recherchés pour le mot « *it* ». 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-3.png">
  <figcaption>
En multipliant le vecteur d’entrée par le vecteur de poids d’attention (et en ajoutant un vecteur de biais non représenté ici), on obtient les vecteurs clé, valeur et requête pour ce *token*.
  </figcaption>
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.2  Etape 1.5 : Les têtes d’attention** </span>    
Dans les exemples précédents, nous avons plongé directement dans l’auto-attention en ignorant la partie « multi-têtes ». Il serait utile de faire la lumière sur ce concept.

L’auto-attention est menée plusieurs fois sur différentes parties des vecteurs Q,K,V.

Séparer les têtes d’attention, c’est simplement reconstruire le vecteur long sous forme de matrice.

Le plus petit GPT2 possède 12 têtes d’attention. Il s’agit donc de la première dimension de la matrice remodelée :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-split-attention-heads-1.png">
</figure>
</center>

Dans les exemples précédents, nous avons examiné ce qui se passe à l’intérieur d’une tête d’attention. Voici comment on pourrait visualiser par exemple trois des douze têtes d’attention :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-split-attention-heads-2.png">
  </figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.3  Etape 2 : Scoring** </span>    
Nous pouvons maintenant procéder au scoring (en sachant que nous ne regardons qu’une seule tête d’attention et que toutes les autres mènent une opération similaire). Dans le cadre du *token* « *it* » : 
<center>
<figure class="image">
   <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-scoring-2.png">
  
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.4  Etape 3 : Somme** </span>    
Comme nous l’avons vu précédemment, nous multiplions maintenant chaque valeur par son score, puis nous les additionnons pour obtenir le résultat de l’attention portée à la tête d’attention n°1 :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-multihead-sum-1.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.5 Etape 3.5 : Fusion des têtes d’attention** </span>
Nous concaténons les têtes d’attention.
<center>
<figure class="image">
    <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-merge-heads-1.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.6  Etape 4 : Projection** </span>
Voici notre deuxième grande matrice de poids qui projette les résultats des têtes d’attention dans le vecteur de sortie de la sous-couche d’auto-attention :
<center>
<figure class="image">
    <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-projection.png">
  <figcaption>
  Nous avons produit le vecteur que nous pouvons envoyer à la couche suivante
  </figcaption>
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.7  Etape 5 : Réseau de neurones entièrement conecté** </span>
Le réseau neuronal entièrement connecté est l’endroit où le bloc traite son *token* d’entrée après que l’auto-attention a inclus le contexte approprié dans sa représentation. Il est composé de deux couches.

La première couche est quatre fois plus grande que le modèle (768x4 = 3072). Pourquoi quatre fois ? Nous reprenons ici la taille utilisée dans l’article original du *transformer* (la dimension du modèle était 512 et la couche #1 dans ce modèle était 2048). Cela semble donner aux *transformers* une capacité de représentation suffisante pour faire face aux tâches qui leur ont été confiées jusqu’à présent. 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-fully-connected1.png">
</figure>
</center>

La deuxième couche projette le résultat de la première couche dans la dimension du modèle (768 pour le petit GPT2). Le résultat de cette multiplication est le résultat du bloc pour ce *token*.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-fully-connected2.png">
</figure>
</center>

Le *token* « *it* » est à présent terminé !
<br>

### <span style="color: #51C353"> **2.3.8 Résumé** </span>
Pour résumer, notre vecteur d’entrée rencontre ces matrices de poids :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-transformer-block-weights-2.png">
</figure>
</center>
Chaque bloc a son propre jeu de ces poids. D’autre part, le modèle n’a qu’une seule matrice d’enchâssement et une seule matrice d'encodage positionnel :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-weights-2.png">
</figure>
</center>

Si vous voulez calculer tous les paramètres du modèle :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-117-parameters.png">
</figure>
</center>
<br><br><br>




# <span style="color: #FF0000"> **3. Autres tâches que la modélisation linguistique** </span>
Le *transformer* avec seulement des décodeurs a fait ses preuves dans beaucoup d’applications autre que la modélisation du langage. Terminons cet article en regardant certaines de ces applications.
<br><br>


## <span style="color: #FFBF00"> **3.1 La traduction automatique** </span>
Un encodeur n’est pas nécessaire pour effectuer une traduction :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/decoder-only-transformer-translation.png">
</figure>
</center>
<br><br>


## <span style="color: #FFBF00"> **3.2 Résumé d’un texte** </span>
C’est sur cette tâche qu’a été entraîné le premier *transformer* basé uniquement sur la partie décodeur. A savoir lire un article de Wikipedia (sans la section d’introduction devant la table des matières) et à le résumer. Les sections d’introduction réelles des articles ont été utilisées comme label dans les données d’entraînement :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/wikipedia-summarization.png">
</figure>
</center>
<br><br>



## <span style="color: #FFBF00"> **3.3 Apprentissage par transfert** </span>
Dans [Sample Efficient Text Summarization Using a Single Pre-Trained Transformer](https://arxiv.org/abs/1905.08836), un Transformer-Décodeur est d’abord pré-entrainé à la modélisation linguistique puis *finetuné* pour faire le résumé. Il s’avère que cela permet d’obtenir de meilleurs résultats qu’un Transformer encodeur-décodeur pré-entrainé.
<br>
Le papier du GPT2 montre également les résultats de tache de résumé après avoir pré-entraîné le modèle à la modélisation du langage.
<br><br>



## <span style="color: #FFBF00"> **3.4 Génération de musique** </span>
Le [Music Transformer](https://magenta.tensorflow.org/music-transformer) utilise un Transformer-Décodeur  pour générer de la musique avec un timing et une dynamique expressifs. La modélisation musicale est  comme la modélisation du langage : il suffit de laisser le modèle apprendre la musique d’une manière auto-supervisée puis de lui faire échantillonner les sorties.

Vous pourriez être curieux de savoir comment la musique est représentée dans ce scénario. Rappelez-vous que la modélisation du langage peut se faire au moyen de représentations vectorielles de caractères, de mots ou de *tokens* qui font partie des mots. Avec un extrait de piano par exemple, nous devons représenter les notes mais aussi la vélocité (une mesure de la force avec laquelle la touche du piano est pressée).
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-transformer-performance-encoding-3.png">
</figure>
</center>

Un extrait de musique n’est qu’une série de ces vecteurs d’un seul coup. L’exemple suivant illustre la séquence d’entrée :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-representation-example.png">
</figure>
</center>

La représentation vectorielle *one-hot* de cette séquence d’entrée ressemble à ceci :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-transformer-input-representation-2.png">
</figure>
</center>

Un exemple de représentation de l’auto-attention du Music Transformer :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-transformer-self-attention-2.png">
  <figcaption>
 On observe un motif triangulaire récurrent. La requête se situe à l’un de ces derniers sommets et s’occupe de toutes les notes aiguës précédentes sur le sommet, jusqu’au début de l'extrait. [...] La figure montre une requête (la source de toutes les lignes d’attention) et les mémoires précédentes en cours de traitement (les notes qui reçoivent plus de probabilités softmax sont surlignées). La coloration des lignes d’attention correspond aux différentes têtes et la largeur des poids de la probabilité softmax.
  </figcaption>
</figure>
</center>
<br><br><br>


# <span style="color: #FF0000"> **Conclusion** </span>
J'espère que cet article vous aura permis de vous familiariser avec le GPT-2.<br>
Voici quelques liens qui pourraient vous être utiles :
- les outils développés par Hugging Face, à savoir le [générateur de textes](https://transformer.huggingface.co/doc/gpt2-large) et le [chatbox](https://convai.huggingface.co/),
- un [tutoriel](https://minimaxir.com/2020/01/twitter-gpt2-bot/) en anglais de Max Woolf pour créer un bot twitter générant du texte,
- le [code du GPT2 annoté](https://amaarora.github.io/2020/02/18/annotatedGPT2.html) en anglais par Aman Arora,
- une [présentation vidéo](https://www.youtube.com/watch?v=5gRNOkQaUyI) où j'explique comment à l'INSERM, nous utilisons le GPT2 afin d'effectuer de la classification.
<br><br><br>


# <span style="color: #FF0000"> **Références** </span> 

- [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/) de Jay Alammar (2019)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860v3) de et al. (2019)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237v2) de Yang et al. (2019)
- [Music Transformer: Generating Music with Long-Term Structure](https://arxiv.org/abs/1809.04281) de Huang et al. (2018) 
- [Sample Efficient Text Summarization Using a Single Pre-Trained Transformer](https://arxiv.org/abs/1905.08836) de Khandelwal et al. (2019) 
- [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198v1.pdf) de Liu et al. (2018) 
- [Improving Language Understandingby Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) de Radford et al. (2018)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) de Radford et al. (2019) 
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) de Vaswani et al. (2017) 


<br><br><br>
# <span style="color: #FF0000"> **Citation** <span>
> @inproceedings{gpt2_blog_post,  
  author    = {Loïck BOURDOIS},  
  title     = {Illustration du GPT2},  
  year      = {2019},  
  url = {https://lbourdois.github.io/blog/nlp/GPT2/}  
}
