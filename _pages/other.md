---
permalink: /other/
title: "Autre"
classes: wide
---

Sur cette page vous pouvez trouver les contenus sur lesquels j'ai travaillÃ© (Ã  titre personnel ou professionnel) mais qui sont rÃ©fÃ©rencÃ©s sur d'autres sites que mon blog personnel. Il s'agit principalement de traductions de cours, et des crÃ©ations de jeux de donnÃ©es et de modÃ¨les.

# Traductions
## Cours de Yann Le Cun et Alfredo Canziani de la NYU
Cette traduction a Ã©tÃ© la plus longue Ã  effectuer s'Ã©talant de 2020 Ã  2022.  
Le contenu est structurÃ© en 19 unitÃ©s rÃ©parties sur 33 vidÃ©os ğŸ¥ de cours (cours magistraux et travaux dirigÃ©s) dâ€™une durÃ©e totale dâ€™environ 45H, 74 pages web ğŸŒ rÃ©sumant les vidÃ©os via les notes prises par les Ã©tudiants pendant le cours, et 16 notebooks Jupyter ğŸ““ (en PyTorch) utilisÃ©s lors des TD. Enfin un jeu de donnÃ©es de plus de 3000 donnÃ©es parallÃ¨les vÃ©rifiÃ©es manuellement a Ã©tÃ© crÃ©Ã© pour entraÃ®ner un modÃ¨le de traduction.  
Vous pouvez retrouver toutes ces ressources sur le site internet dÃ©diÃ© qui a Ã©tÃ© conÃ§u Ã  l'occasion : [https://lbourdois.github.io/cours-dl-nyu/](https://lbourdois.github.io/cours-dl-nyu/).


## Cours de Hugging Face ğŸ¤—
### Cours de NLP
En 2022, j'ai traduit le cours de traitement automatique du langage de Hugging Face.  
Le contenu est structurÃ© en 10 chapitres comprenant un total de 76 vidÃ©os ğŸ¥ d'une durÃ©e totale d'environ 5H, de 78 pages web ğŸŒ et 61 notebooks Jupyter ğŸ““  (en PyTorch et Tensorflow).  
Vous pouvez retrouver toutes ces ressources sur le site de [Hugging Face](https://huggingface.co/learn/nlp-course/fr/chapter1/1).

### Cours d'audio
En 2023, j'ai traduit le cours de traitement automatique du langage de Hugging Face.  
Le contenu est structurÃ© en 8 unitÃ©s rÃ©parties sur 46 pages web ğŸŒ.  
Vous pouvez retrouver toutes ces ressources sur le site de [Hugging Face](https://huggingface.co/learn/audio-course/fr/).

### Cours sur les modÃ¨les de diffusion
En 2023, j'ai traduit le cours sur les modÃ¨les de diffusion de Hugging Face.  
Le contenu est structurÃ© en 4 chapitres portant sur 17 pages web ğŸŒ et 8 notebooks Jupyter ğŸ““ (en PyTorch).  
Vous pouvez retrouver toutes ces ressources sur le GitHub de [Hugging Face](https://github.com/huggingface/diffusion-models-class/tree/main/units/fr) (le contenu n'ayant pas encore Ã©tÃ© propagÃ© sur le site officiel).

<br><br>

# ModÃ¨les et jeux de donnÃ©es

## FAT5
Le FAT5 est une implÃ©mentation du T5 en PyTorch avec un objectif UL2 optimisÃ© pour GPGPU dÃ©veloppÃ© avec [Boris ALBAR](https://b-albar.github.io/portfolio/).  
Elle utilise des noyaux CUDA et Triton personnalisÃ©s ainsi que des optimisations spÃ©cifiques pour augmenter le dÃ©bit et rÃ©duire l'utilisation de la mÃ©moire pour l'entraÃ®nement et l'infÃ©rence d'un facteur 4 par rapport Ã  l'implÃ©mentation originale disponible dans Hugging Face.  
Nous l'avons appliquÃ©e en prÃ©-entraÃ®nant un modÃ¨le en franÃ§ais de 147M paramÃ¨tres en utilisant uniquement une A100. Nous estimons ainsi pouvoir ramener le prix de prÃ©-entraÃ®nement d'un tel modÃ¨le Ã  seulement 2200â‚¬ (estimation faite sur une instance OVH).   
Le code de prÃ©-entrainement est disponible sur [GitHub](https://github.com/catie-aq/flashT5) sous licence Apache-2.0 et les poids du modÃ¨le entraÃ®nÃ© sur le compte [Hugging Face du CATIE](https://huggingface.co/CATIE-AQ). Un article de blog dÃ©taillant notre mÃ©thodologie est disponible [ici](https://huggingface.co/spaces/CATIE-AQ/FAT5-rapport).

## NER
A complÃ©ter.

## Question Answering
A complÃ©ter.
