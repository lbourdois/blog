---
title: "HISTORIQUE DES STATE SPACE MODELS (SSM) EN 2022"
categories:
  - SSM
tags:
  - Introduction aux SSM 
excerpt : SSM - Revue de littÃ©rature des SSM parus lors de l'annÃ©e 2022
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/NLP_radom_blog.png"
    teaser: "https://github.com/lbourdois/blog/assets/58078086/cb2dca34-9a3e-481a-8773-2360a1ceaa1c"
author_profile: false
sidebar:
    nav: sidebar-ssm
classes: wide
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


# <span style="color: #FF0000"> **Introduction** </span>
Dans lâ€™[article prÃ©cÃ©dent](https://lbourdois.github.io/blog/ssm/introduction_ssm/), nous avons dÃ©fini ce quâ€™est un *State Space Model* (SSM) Ã  lâ€™aide dâ€™un systÃ¨me en temps continu. Nous lâ€™avons discrÃ©tisÃ© pour faire apparaÃ®tre sa vue rÃ©currente puis convolutive. Lâ€™intÃ©rÃªt ici est de pouvoir entraÃ®ner le modÃ¨le de maniÃ¨re convolutive puis de rÃ©aliser lâ€™infÃ©rence de maniÃ¨re rÃ©currente sur de trÃ¨s longues sÃ©quences.  
<br>

| ![image](https://github.com/lbourdois/blog/assets/58078086/12bbe1cf-3911-4bad-9a3b-3f427bc6bc82)|
|:--:|
| Figure 1 : *Image provenant de l'article de blog Â« [Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3) Â» d'Albert GU et al. (2022)*|

<br>
Cette vision a Ã©tÃ© introduite par Albert GU dans ses papiers [LSSL](https://arxiv.org/abs/2110.13985) et [S4](https://arxiv.org/abs/2111.00396) parus en 2021. Le S4 Ã©tant lâ€™Ã©quivalent du Â« *Attention is all you need* Â» pour les transformers.  
Dans le prÃ©sent article, nous allons passer en revue la littÃ©rature des SSM parus durant lâ€™annÃ©e 2022. Ceux apparus en 2023 seront listÃ©s dans le prochain article.
Lâ€™objectif est de montrer les diffÃ©rentes Ã©volutions de ces types de modÃ¨les au cours des mois, tout en restant synthÃ©tique (i.e. je ne vais pas rentrer dans tous les dÃ©tails des papiers listÃ©s). Lors de cette annÃ©e 2022, les diffÃ©rentes avancÃ©es se sont focalisÃ©es Ã  appliquer des algorithmes de discrÃ©tisations diffÃ©rents, tout en remplaÃ§ant la matrice [HiPPO](https://arxiv.org/abs/2008.07669) par une plus simple.
<br><br><br>

## <span style="color: #FFBF00"> **ModÃ¨les thÃ©oriques** </span>

Dans cette section, nous allons passer en revue les travaux thÃ©oriques des propositions d'amÃ©lioration de l'architecture du S4 vont Ãªtre passÃ©s en revue. Nous aborderons ensuite dans une section diffÃ©rente, des applications concrÃ¨tes sur diffÃ©rentes tÃ¢ches (audio, vision, etc.).
<br><br>

### <span style="color: #51C353"> **S4 V2** </span> 
  
Le 4 mars 2022, les auteurs du S4 ont actualisÃ© leur papier afin dâ€™y intÃ©grer une section sur lâ€™importance de la matrice HiPPO (cf. la section 4.4 de la version la plus rÃ©cente du papier).  
Pour rÃ©sumer, elle consiste Ã  rapporter les rÃ©sultats observÃ©s Ã  la suite de la rÃ©alisation dâ€™ablations sur le jeu de donnÃ©es sÃ©quentielles CIFAR-10. Au lieu d'utiliser un SSM avec la matrice HiPPO, les auteurs ont essayÃ© d'utiliser diverses paramÃ©trisations comme une matrice dense alÃ©atoire et une matrice diagonale alÃ©atoire. 

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Hippo.png) |
|:--:|
| Figure 2 : *Accuracy sur l'Ã©chantillon de validation de CIFAR-10, tirÃ©e de la figure 3 du papier du S4* |

<br>
L'utilisation d'HiPPO se rÃ©vÃ¨le donc importante, les performances obtenues sont-elles dues Ã  ses qualitÃ©s intrinsÃ¨ques spÃ©cifiques ou bien nâ€™importe quelle matrice normale Ã  faible rang (NPLR pour *Normal Plus Low-Rank*) pourrait suffire ?

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/NPLR.png) |
|:--:|
| Figure 3 : *Accuracy sur l'Ã©chantillon de validation de CIFAR-10 avec diffÃ©rentes initialisations et parameterisations, tirÃ©e de la figure 4 du papier du S4* |

<br>
Initialiser une matrice NPLR avec HiPPO augmente considÃ©rablement les performances.
Ainsi, dâ€™aprÃ¨s ces expÃ©riences, la matrice HiPPO est primordiale afin dâ€™obtenir un modÃ¨le performant.

Les auteurs du S4 ont approfondi leurs travaux quâ€™ils ont exposÃ©s le 24 juin 2022 dans lâ€™article [*How to Train Your HiPPO*](https://arxiv.org/abs/2206.12037). Il sâ€™agit dâ€™un papier extrÃªmement dÃ©taillÃ© de plus de 39 pages.  
Dans cet article, les auteurs se concentrent sur une interprÃ©tation plus intuitive des SSM en tant que modÃ¨le convolutif oÃ¹ le noyau de convolution est une combinaison linÃ©aire de fonctions de base particuliÃ¨res, ce qui conduit Ã  plusieurs gÃ©nÃ©ralisations et Ã  de nouvelles mÃ©thodes.  
Ainsi, ils prouvent que la matrice $$\mathbf{A}$$ du S4 produit des polynÃ´mes de Legendre Ã  Ã©chelle exponentielle (LegS). Cela confÃ¨re au systÃ¨me une meilleure capacitÃ© Ã  modÃ©liser les dÃ©pendances Ã  long terme via des noyaux trÃ¨s lisses.  
Les auteurs dÃ©rivent Ã©galement un nouveau SSM qui produit des approximations de fonctions de Fourier tronquÃ©es (FouT). Cette mÃ©thode gÃ©nÃ©ralise les [transformÃ©es de Fourier Ã  court terme]( https://fr.wikipedia.org/wiki/Transform%C3%A9e_de_Fourier_%C3%A0_court_terme) et les convolutions locales (c'est-Ã -dire un ConvNet standard). Ce SSM peut Ã©galement coder des fonctions de pointe pour rÃ©soudre des tÃ¢ches de mÃ©morisation classiques.  
A noter que câ€™est surtout HiPPO-FouT qui est introduit dans ce papier, HiPPO-LegS ayant Ã©tÃ© introduit dans le papier original dâ€™HiPPO deux ans plus tÃ´t. De mÃªme quâ€™HiPPO-LegT (polynÃ´mes de Legendre tronquÃ©s).  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master//assets/images/SSM_2022/Hippo_LegS_LegT_FouT.png) |
|:--:|
| Figure 4 : *Les diffÃ©rentes variantes d'HiPPO* |

<br>
Les couleurs reprÃ©sentent les 4 premiÃ¨res fonctions de base $$K_n(t)$$ (le noyau de convolution) pour chacune des mÃ©thodes (nous invitons le lecteur Ã  regarder le tableau 1 du papier pour savoir Ã  quoi Ã©quivaut $$K_n(t)$$ pour chacune des mÃ©thodes).

De plus, les auteurs travaillent Ã©galement sur le pas de temps $$âˆ†$$, qui indÃ©pendamment d'une notion de discrÃ©tisation peut Ãªtre interprÃ©tÃ© simplement comme contrÃ´lant la longueur des dÃ©pendances ou la largeur des noyaux du SSM. Les auteurs dÃ©taillent aussi comment choisir une bonne valeur de $$âˆ†$$ pour une tÃ¢che donnÃ©e.

Les travaux menÃ©s permettent dâ€™amÃ©liorer les rÃ©sultats du S4 de plus de 5,5 points sur le benchmark [LRA](https://arxiv.org/abs/2011.04006) de TAY, DEHGHANI et al. (2020) :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4v2_results.png) |
|:--:|
| Figure 5 : *RÃ©sultats du S4 v2 sur le benchmark* |

<br>
Le modÃ¨le rÃ©sultant de ce papier est gÃ©nÃ©ralement appelÃ© Â« S4 V2 Â» ou Â« S4 updated Â» dans la littÃ©rature Ã  opposer au Â« S4 original Â» ou Â« S4 V1 Â».
<br><br>

### <span style="color: #51C353"> Le DSS : *Diagonal State Spaces* </span>

Le 27 mars 2022, Ankit GUPTA introduit dans son papier [*Diagonal State Spaces are as Effective as Structured State Spaces*](https://arxiv.org/abs/2203.14343) les *Diagonal State Spaces* (DSS).  
Il semble que suite Ã  ce papier, Albert GU et lui se soient mis Ã  travailler ensemble dâ€™une part sur une version actualisÃ©e de ce papier (GU apparaissant par la suite comme co-auteur dans les v2 et v3 de lâ€™article) et dâ€™autre part dans le cadre du S4D (voir section suivante).  
La principale chose Ã  retenir est que cette approche est sensiblement plus simple que le S4. En effet, le DSS repose sur des matrices d'Ã©tat diagonales (donc sans la correction de rang faible du S4, i.e. sans la matrice HiPPO) qui, si sont initialisÃ©es de maniÃ¨re appropriÃ©e, fonctionnent mieux que le S4 original. Lâ€™usage dâ€™une matrice diagonale Ã  la place de la matrice HiPPO pour $$\mathbf{A}$$ est depuis devenu une norme.  

ArrÃªtons-nous nÃ©anmoins sur les quelques complexitÃ©s/limites que contiennent ce papier. En les listant nous pourrons comprendre les apports des mÃ©thodes suivantes qui visent Ã  simplifier davantage les choses.

<br>

**1. La discrÃ©tisation**  
Le DSS utilise le mÃªme systÃ¨me dâ€™Ã©quations diffÃ©rentielles que le S4 : 

$$
  \begin{aligned}
    x' &= \mathbf{A}x + \mathbf{B}u \\
    y &= \mathbf{C}x
  \end{aligned}
$$

Cependant il utilise une discrÃ©tisation diffÃ©rente afin dâ€™aboutir aux vues convolutives et rÃ©currentes, Ã  savoir la discrÃ©tisation [*zero-order hold*](https://en.wikipedia.org/wiki/Zero-order_hold) (ZOH) ou bloqueur d'ordre zÃ©ro en franÃ§ais, au lieu de la discrÃ©tisation bilinÃ©aire, qui suppose que le signal Ã©chantillonÃ© est constant entre chaque point d'Ã©chantillonnage.  
Ci-dessous un tableau comparatif des valeurs de $$\mathbf{A}$$, $$\mathbf{B}$$ et $$\mathbf{C}$$ pour chacune des deux discrÃ©tisations dans la vue rÃ©currente, ainsi que lâ€™expression du noyau de convolution dans la vue convolutive :
  
| DiscrÃ©tisation | BilinÃ©aire | ZOH                    |
| -------------- | ---------- | ----------------------- |
| RÃ©currence     | $$\mathbf{\bar{A}} = (\mathbf {I} - \frac{\Delta}{2} \mathbf{A})^{-1}(\mathbf {I} + \frac{\Delta}{2} \mathbf{A})$$ <br> $$\mathbf {\bar{B}} = (\mathbf{I} - \frac{\Delta}{2}  \mathbf{A})^{-1} \Delta \mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$   | $$\mathbf{\bar{A}} = e^{\mathbf{A}\Delta}$$ <br> $$\mathbf{\bar{B}} = (\mathbf{\bar{A}} - I)\mathbf{A}^{-1}\mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$  |
| Convolution    | $$\mathbf{\bar{K}}_k = (\mathbf{\bar{C}}  \mathbf{\bar{B}}, \mathbf{\bar{C}} \mathbf{\bar{A}}  \mathbf{\bar{B}}, â€¦, \mathbf{\bar{C}}  \mathbf{\bar{A}}^{k} \mathbf{\bar{B}})$$       | $$\mathbf{\bar{K}} = (\ \mathbf{C} e^{\mathbf{A}\cdot k\Delta} (e^{\mathbf{A}\Delta} - I)\mathbf{A}^{-1}\mathbf{B}\ )_{0 \leq k < L}$$ |

<br>

Pour la ZOH, aprÃ¨s avoir dÃ©roulÃ© les calculs, on obtient en fin de compte $$y_k = \sum_{j=0}^k \bar{C}\bar{A}^j\bar{B}\cdot u_{k-j} = \sum_{j=0}^k \bar{K}_j\cdot u_{k-j}$$.

Calculer $$y$$ Ã  partir de $$u$$ et $$\bar{K}$$ sâ€™effectue alors par Transformation de Fourier rapide (FFT) en $$O(L~log(L))$$ avec $$L$$ la longueur de la sÃ©quence en calculant simultanÃ©ment la multiplication de deux polynÃ´mes de degrÃ©s $$L-1$$.
<br><br>

**2. DSSsoftmax et DSSexp**  

<u>Version courte</u>  

GUPTA formule une proposition pour obtenir des DSS qui soient aussi expressifs que le S4, aboutissant Ã  la formulation de deux DSS diffÃ©rents : le DSSexp et le DSSsoftmax. Les informations Ã  retenir les concernant peuvent se rÃ©sumer au tableau suivant :
  
| Approche        | DSSexp          | DSSsoftmax      |
| --------------- | --------------- | --------------- |
| Vue convolutive | $$K = \bar{K}_{\Delta, L}(\Lambda,\mathbb{I}_{1 \leq i \leq N},\ \widetilde{w})\\ = \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$ | $$K = \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\\ = w \cdot \Lambda^{-1} \cdot \text{row-softmax}(P)$$  |
| Vue rÃ©currente  | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$ | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$ |
| InterprÃ©tation  | Agit comme la porte d'oubli d'une LSTM  | Si $$\Re(\lambda)<<0$$ : conserve l'information locale,<br> si $$\Re(\lambda)>>0$$ : peut capturer des informations Ã  trÃ¨s longues distances                                                                                        |
  
Nous travaillons donc ici sur $$â„‚$$ et non pas $$â„$$.  

<br>

<u>Version longue</u>  

GUPTA formule la proposition suivante pour obtenir des DSS qui sont aussi expressifs que le S4 :

Soit $$K \in \mathbb{R}^{1\times L}$$ le noyau de longueur $$L$$ d'un espace d'Ã©tat donnÃ© $$(A, B, C)$$ et d'un temps d'Ã©chantillonnage $$\Delta > 0$$, oÃ¹ $$A \in \mathbb{C}^{N \times N}$$ est diagonalisable sur $$\mathbb{C}$$ avec des valeurs propres $$\lambda_1,\ldots,\lambda_N$$ et $$\forall i$$, $$\lambda_i \neq 0$$ et $$e^{L\lambda_i\Delta} \neq 1$$. Soit $$P \in \mathbb{C}^{N \times L} P_{i,k} = \lambda_i k\Delta$$ et $$\Lambda$$ la matrice diagonale avec $$\lambda_1,\ldots,\lambda_N$$. Alors il existe $$\widetilde{w}, w \in \mathbb{C}^{1\times N}$$ tel que :  
- (a) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ (1)_{1 \leq i \leq N},\ \widetilde{w})\ \ =\ \ \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$
- (b) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\ \ =\ \ w \cdot \Lambda^{-1} \cdot  \text{row-softmax}(P)$$

(a) suggÃ¨re que nous pouvons paramÃ©trer les espaces d'Ã©tat via $$\Lambda, \widetilde{w} \in \mathbb{C}^N$$ et calculer le noyau comme indiquÃ©. Malheureusement, dans la pratique, la partie rÃ©elle des Ã©lÃ©ments de $$Î›$$ peut devenir positive pendant l'apprentissage, le rendant instable pour les entrÃ©es longues. Pour rÃ©soudre ce problÃ¨me, les auteurs proposent deux mÃ©thodes : DSSexp et DSSsoftmax.

2.1 Vue convolutive  
Dans DSSexp, les parties rÃ©elles de $$Î›$$ doivent Ãªtre nÃ©gatives. On a alors $$\Lambda = - \text{elementwise-exp}(\Lambda_\mathrm{re}) + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$. $$K$$ se calcule alors comme dans la formule indiquÃ©e dans la partie (a) de la proposition.  
Dans DSSsoftmax, chaque ligne de $$Î›$$ est normalisÃ©e par la somme de ces Ã©lÃ©ments. On a $$\Lambda = \Lambda_\mathrm{re} + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$.  
$$K$$ se calcule alors comme dans la formule indiquÃ©e dans la partie (b) de la proposition.  
A noter que softmax sur $$\mathbb{C}$$ n'est pas forcÃ©ment dÃ©fini lors de sofmax $$(0, i \pi)$$, les auteurs utilisant une version corrigÃ©e du softmax pour prÃ©venir ce problÃ¨me (cf. annexe A.2. du papier).

2.2 Vue rÃ©currente  
Dans DSSexp, en utilisant la formule de la rÃ©currence dans le tableau ci-dessus, on obtient $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$, oÃ¹ dans les deux Ã©galitÃ©s, $$\lambda_i$$ est la iÃ¨me valeur propre de Lambda.  
Etant donnÃ© que $$\bar{A}$$ est diagonale, il est possible de calculer les $$x_k$$ indÃ©pendamment de la maniÃ¨re suivante :  $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + \lambda_i^{-1} (e^{\lambda_i\Delta} - 1)u_k$$.  
Il est alors possible de dÃ©duire que, si $$|\lambda_i|\Delta \approx 0$$, nous avons $$x_{i,k} \approx x_{i,k-1}$$ permettant de copier l'histoire sur de nombreux pas de temps. En revanche, si $$\mathrm{Re}(\lambda_i)\Delta \ll 0$$, alors $$_{i,k} \approx -\lambda_i^{-1}u_k$$ et l'information des pas de temps prÃ©cÃ©dents est oubliÃ©e, similaire Ã  une porte Â« forget Â» dans les LSTMs.

Dans DSSsoftmax, en utilisant la formule de la rÃ©currence du tableau ci-dessus, on obtient : $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$.  
D'oÃ¹ $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + {u_k(e^{\lambda_i\Delta} - 1) \over \lambda_i (e^{\lambda_i\Delta L} - 1)}$$.  
A noter que $$e^{\lambda_i\Delta}$$ peut Ãªtre instable. Il faut alors calculer deux cas diffÃ©rents en fonction du signe de $$\mathrm{Re}(\lambda)$$ en introduisant un Ã©tat intermÃ©diaire $$\widetilde{x}_{k}$$.  
-	Si $$\mathrm{Re}(\lambda) \leq 0$$ : $$\widetilde{x}_{k} = e^{\lambda\Delta}\cdot \widetilde{x}_{k-1} + u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {(e^{\lambda\Delta} - 1) \over \lambda (e^{\lambda\Delta L} - 1) }$$

Et notamment si $$\mathrm{Re}(\lambda) \ll 0$$ alors $$\widetilde{x}_k \approx u_k$$ et $$x_k \approx u_k / \lambda$$, entraÃ®nant une focalisation sur une information locale (les pas prÃ©cÃ©dents sont ignorÃ©s)

-	Si $$\mathrm{Re}(\lambda) > 0$$ : 
$$\widetilde{x}_{k} = \widetilde{x}_{k-1} + e^{-k\lambda\Delta} \cdot u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {e^{\lambda\Delta (k-(L-1))} \over \lambda}\cdot {e^{-\lambda\Delta}-1 \over e^{-\lambda\Delta L} - 1 }$$

De mÃªme si $$\mathrm{Re}(\lambda) \gg 0$$ alors $$\widetilde{x}_0 \approx u_0$$ et $$\widetilde{x}_k \approx \widetilde{x}_{k-1} \approx u_0$$, $$x_{k < L-1} \approx 0$$ et $$x_{L-1} \approx u_0 / \lambda$$, le modÃ¨le peut ainsi capturer des informations Ã  trÃ¨s longues distances.
En pratique, les auteurs du S4D indiquant que $$\mathrm{Re}(\lambda) \gg 0$$ ne fonctionne pas si $$L$$ est trÃ¨s grande (explosion quand $$t \rightarrow \infty$$ dans $$K(t) = C \exp(A^\intercal).B)$$.  

<br>

**3. Initialisation**  
Les parties rÃ©elles et imaginaires de $$w$$ sont initialisÃ©es Ã  partir de $$\mathcal{N}(0,1)$$, les Ã©lÃ©ments de $$\Delta_{\log}$$ Ã  partir de $$\exp(\mathcal{U}~(log(0.001), log(0.1)))$$, et $$\Delta$$ via les valeurs propres de la matrice HiPPO. Les auteurs se demandent sâ€™il ne serait pas possible de trouver une initialisation plus simple pour $$\Delta$$. Ils notent nÃ©anmoins quâ€™une alÃ©atoire conduit Ã  de moins bons rÃ©sultats.  

Concernant les rÃ©sultats, DSS a Ã©tÃ© testÃ© sur LRA et [*Speech Commands*](https://arxiv.org/abs/1804.03209v1) de WARDEN (2018) :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results.png) |
|:--:|
| Figure 6 : *RÃ©sultats du DSS sur LRA* |

<br>
Le DSS (en version softmax ou en version exp) obtient de meilleurs rÃ©sultats moyens que ceux du S4 original pour ce benchmark. Le DSSsoftmax semble performer lÃ©gÃ¨rement mieux que le DSSexp. Un intÃ©rÃªt Ã©galement de ce papier est quâ€™il est le premier Ã  reproduire les rÃ©sultats du S4 et donc confirmer que les SSM passent ce benchmark.


| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results2.png) |
|:--:|
| Figure 7 : *RÃ©sultats du DSS sur Speech Commands* |

<br>
Sur Speech Commands, le S4 garde lâ€™avantage sur les DSS.
<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/ag1988/dss).  
Ce papier a fait l'objet d'un *spotlight talk* Ã  [NeurIPS 2022](https://nips.cc/virtual/2022/poster/54460).
<br><br>
  
### <span style="color: #51C353"> **Le S4D : le S4 diagonal** </span>
Le 23 juin 2022, GU, GUPTA et al. introduisent le S4D dans leur article [*On the Parameterization and Initialization of Diagonal State Space Models*](https://arxiv.org/abs/2206.11893).  
L'initialisation de la matrice d'Ã©tat $$\mathbf{A}$$ du DSS repose sur une approximation particuliÃ¨re de la matrice HiPPO du S4. Si la matrice du S4 possÃ¨de une interprÃ©tation mathÃ©matique pour traiter les dÃ©pendances Ã  longue portÃ©e, l'efficacitÃ© de l'approximation diagonale reste thÃ©oriquement inexpliquÃ©e.  
Avec le S4D, les auteurs introduisent un SSM diagonal combinant le meilleur du calcul et de la paramÃ©trisation de S4 et de l'initialisation de DSS. Au final, cela donne une mÃ©thode trÃ¨s simple, thÃ©oriquement fondÃ©e et empiriquement efficace.  

Une comparaison des trois mÃ©thodes est donnÃ©e dans le tableau 1 du papier :  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4D_table1.png) |
|:--:|
| Figure 8 : *Comparaison du S4, du DSS et du S4D* |

<br>
Le S4D peut utiliser la discrÃ©tisation bilinÃ©aire du S4 ou bien la discrÃ©tisation ZOH du DSS.  

Dans le S4D, le noyau de convolution discrÃ©tisÃ© de lâ€™Ã©quation $$y = u \ast \mathbf{\overline{K}}$$ se calcule de la faÃ§on suivante : 
$$\mathbf{\overline{K}}_\ell = \sum_{n = 0}^{N-1} \mathbf{C}_n \mathbf{\overline{A}}_n^\ell \mathbf{\overline{B}}_n \implies \mathbf{\overline{K}} = (\mathbf{\overline{B}}^\top \circ \mathbf{C}) \cdot \mathcal{V}_L(\mathbf{\overline{A}})$$  
oÃ¹ :  
â€¢ $$\circ$$ reprÃ©sente le [produit matriciel dâ€™Hadamard]( https://fr.wikipedia.org/wiki/Produit_matriciel_de_Hadamard),  
â€¢ $$\cdot$$ un produit matriciel classique,  
â€¢ $$\mathcal{V}_L$$ est la [matrice de Vandermonde]( https://fr.wikipedia.org/wiki/Matrice_de_Vandermonde) câ€™est-Ã -dire : $$\mathcal{V} = 
\begin{bmatrix}
1&\alpha _{1}&{\alpha _{1}}^{2}&\dots &{\alpha _{1}}^{n-1}\\
1&\alpha _{2}&{\alpha _{2}}^{2}&\dots &{\alpha _{2}}^{n-1}\\
1&\alpha _{3}&{\alpha _{3}}^{2}&\dots &{\alpha _{3}}^{n-1}\\
vdots &\vdots &\vdots & \vdots \\
1&\alpha _{m}&{\alpha _{m}}^{2}&\dots &{\alpha _{m}}^{n-1}
\end{bmatrix}$$.

Autrement dit, pour tout $$i$$ et $$j$$, le coefficient en ligne $$i$$ et colonne $$j$$ est $$\displaystyle V_{i,j}={\alpha _{i}}^{j-1}$$.  

Au final, dans le S4D, 

$$
  \mathbf{\overline{K}} =
  \begin{bmatrix}
    \mathbf{\overline{B}}_0 \mathbf{C}_0 & \dots & \mathbf{\overline{B}}_{N-1} \mathbf{C}_{N-1}
  \end{bmatrix}
  \begin{bmatrix}
    1      & \mathbf{\overline{A}}_0     & \mathbf{\overline{A}}_0^2     & \dots  & \mathbf{\overline{A}}_0^{L-1}     \\
    1      & \mathbf{\overline{A}}_1     & \mathbf{\overline{A}}_1^2     & \dots  & \mathbf{\overline{A}}_1^{L-1}     \\
    \vdots & \vdots                 & \vdots                    & \ddots & \vdots                        \\
    1      & \mathbf{\overline{A}}_{N-1} & \mathbf{\overline{A}}_{N-1}^2 & \dots  & \mathbf{\overline{A}}_{N-1}^{L-1} \\
  \end{bmatrix}
\qquad \text{oÃ¹ } \mathcal{V}_L(\mathbf{\overline{A}})_{n, \ell} = \mathbf{\overline{A}}_n^\ell
$$.  
Le tout est calculable en $$O(N+L)$$ opÃ©rations et espace.  

La paramÃ©trisation des diffÃ©rentes matrices est la suivante :
-	$$\mathbf{A} = -\exp(\Re(\mathbf{A})) + i \cdot \Im(\mathbf{A})$$.  
Les auteurs indiquent quâ€™il est possible de remplacer lâ€™exponentielle par nâ€™importe quelle fonction positive.
-	$$\mathbf{B} = 1$$ puis est entraÃ®nÃ©
-	$$\mathbf{C}$$ alÃ©atoire avec un Ã©cart-type de 1 puis entraÃ®nÃ©.
  
Notons que le S4 prend en compte des rÃ©els alors que S4D des complexes en paramÃ©trant avec une taille dâ€™Ã©tat de $$N/2$$ et en ajoutant implicitement les paires conjuguÃ©es aux paramÃ¨tres. On a alors lâ€™Ã©quivalent de $$N$$ paramÃ¨tres rÃ©els assurant que la sortie est rÃ©elle.  

Concernant lâ€™initialisation, les auteurs en introduisent deux :    
â€¢ **S4D-Inv** qui est une approximation de S4-LegS : $$\quad \mathbf{A}_n = -\frac{1}{2} + i \frac{N}{\pi} \left( \frac{N}{2n+1}-1 \right)$$  
â€¢ **S4D-Lin** qui est une approximation de S4-FouT : $$\quad \mathbf{A}_n = -\frac{1}{2}\mathbf{1} + i \pi n$$

Nous invitons le lecteur Ã  consulter la partie 4 du papier pour plus de dÃ©tails concernant ces Ã©quations.  
Dâ€™un point de vue interprÃ©tabilitÃ©, la partie rÃ©elle de $$\mathbf{A}_n$$ contrÃ´le le taux de dÃ©croissance des poids. La partie imaginaire de $$\mathbf{A}_n$$ contrÃ´le quant Ã  elle les frÃ©quences dâ€™oscillations de la fonction de base $$K_n(t) = e^{t\mathbf{A}}\mathbf{B}$$.  

Enfin les auteurs avancent quelques rÃ©sultats :  
1)	Calculer le modÃ¨le avec une softmax au lieu de Vandermonde ne fait pas de grande diffÃ©rence
2)	EntraÃ®ner B donne toujours de meilleurs rÃ©sultats.    
3)	Il n'existe pas de diffÃ©rences notables entres les deux discrÃ©tisations possibles.    
4)	Restreindre la partie rÃ©elle de A conduit Ã  de meilleurs rÃ©sultats (pas de faÃ§on significative nÃ©anmoins)    
5)	Toutes les modifications testÃ©es pour lâ€™initialisation ont dÃ©gradÃ© les rÃ©sultats. A savoir appliquer un coefficient sur la partie imaginaire ou utiliser une partie imaginaire alÃ©atoire / utiliser une partie rÃ©elle alÃ©atoire / utiliser une partie imaginaire et une partie rÃ©elle alÃ©atoire.  

Cette mÃ©thode Ã©tant trÃ¨s facile Ã  implÃ©menter par rapport aux autres (Vandermade se limitant Ã  deux lignes de code), le S4D a remplacÃ© le S4 dans les usages (on peut dâ€™ailleurs observer des abus de langage oÃ¹ dans la table 6 du papier du Mamba par exemple, les auteurs utilisent le terme S4 pour dÃ©signer le S4D).
<br>

<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/state-spaces/s4).  
Le 1er dÃ©cembre 2022, GUPTA et al. prÃ©sentent une suite au DSS avec [*Simplifying and Understanding State Space Models with Diagonal Linear RNNs*](https://arxiv.org/abs/2212.00768) qui introduit le DLR. Ils se dÃ©barrassent de l'Ã©tape de discrÃ©tisation et proposent un modÃ¨le basÃ© sur des RNN linÃ©aires diagonaux (DLR) pouvant opÃ©rer sur environ 1 million de positions (contrairement aux RNN classiques). Le code de ce modÃ¨le est disponible sur [GitHub](https://github.com/ag1988/dlr).
<br><br>

### <span style="color: #51C353"> **Le GSS : *Gated State Space*** </span>

Cinq jours aprÃ¨s le S4D, le 27 juin 2022, MEHTA, GUPTA et al. introduisent le GSS dans leur papier [*Long Range Language Modeling via Gated State Spaces*](https://arxiv.org/abs/2206.13947.pdf).  
Dans ce travail, ils se concentrent sur la modÃ©lisation de sÃ©quences autorÃ©gressives (lÃ  oÃ¹ les travaux prÃ©cÃ©dents sur les SSM se concentraient particuliÃ¨rement sur les tÃ¢ches de classification de sÃ©quences) Ã  partir de livres en anglais, de code source Github et d'articles de mathÃ©matiques ArXiv. Ils montrent que leur couche appelÃ©e *Gated State Space* (GSS) s'entraÃ®ne significativement plus vite que le DSS (2 Ã  3 fois plus vite). Ils attestent Ã©galement que l'exploitation de l'auto-attention pour modÃ©liser les dÃ©pendances locales amÃ©liore encore les performances du GSS. 

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GSS_table1.png) |
|:--:|
| Figure 9 : *Comparaison du DSS vs GSS. Les modÃ¨les sont entraÃ®nÃ©s sur des sÃ©quences de longueurs 4K puis Ã©valuÃ©s sur des sÃ©quences pouvant aller jusquâ€™Ã  65K tokens.*|

<br>
Partant du constat que les SSM (S4/DSS) sâ€™entraÃ®nent plus lentement que prÃ©vu sur TPU, les auteurs ont modifiÃ© l'architecture afin de rÃ©duire la dimensionnalitÃ© d'opÃ©rations spÃ©cifiques qui se sont rÃ©vÃ©lÃ©es Ãªtre des goulots d'Ã©tranglement. Ces modifications s'inspirent d'une observation empirique bien Ã©tayÃ©e concernant l'efficacitÃ© des unitÃ©s de *gating* ([*Language Modeling with Gated Convolutional Networks*](https://arxiv.org/abs/1612.08083) de DAUPHIN et al. (2016),  [*GLU Variants Improve Transformer*](https://arxiv.org/abs/2002.05202) de SHAZEER (2020), etc.). Plus prÃ©cisÃ©ment, les auteurs s'inspirent du papier [*Transformer Quality in Linear Time*](https://arxiv.org/abs/2202.10447) de HUA et al. (2022). Ces derniers ont montrÃ© qu'avec leur modÃ¨le FLASH, le remplacement de la couche *feed-forward* dans le Transformer par des unitÃ©s de *gating* permet d'utiliser une attention unitÃªte plus faible avec une perte de qualitÃ© minimale. Ils ont appelÃ© cette composante la *Gated Attention Unit* (GAU).


 | ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GAU.png) |
|:--:|
| Figure 10 : *La Gated Attention Unit.<br>Ce nâ€™est pas exactement la mÃªme figure que celle du papier : jâ€™ai effectuÃ© une translation horizontale afin dâ€™avoir lâ€™entrÃ©e en bas et non en haut pour faciliter le parallÃ¨le avec la figure du Mega visible plus bas.* |

<br>
Les auteurs du GSS ont donc Ã©tendu l'utilisation des gating units aux SSM et observent alors une rÃ©duction de la dimensionnalitÃ© lors de l'exÃ©cution d'opÃ©rations FFT. 

 | ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GSS.png) |
|:--:|
| Figure 11 : *Adaptation de la GAU aux SSM.* |

<br>
A noter que contrairement Ã  HUA et al., les auteurs nâ€™observent pas beaucoup d'avantages Ã  utiliser les activations RELUÂ² ou Swish au lieu de la GELU d'oÃ¹ sa conservation.  
De plus, le DSS utilise un pas de temps $$âˆ†$$ fixÃ© Ã  1 (les auteurs observant que cela permet de rÃ©duire le temps de calcul nÃ©cessaire Ã  la crÃ©ation des noyaux et de simplifier leur calcul).  
Un point particuliÃ¨rement intÃ©ressant est que contrairement aux observations rÃ©alisÃ©es dans le S4 et le DSS, la performance du modÃ¨le sur les tÃ¢ches de modÃ©lisation du langage sâ€™est retrouvÃ©e beaucoup moins sensible Ã  l'initialisation permettant alors dâ€™entraÃ®ner le modÃ¨le avec succÃ¨s en initialisant les variables de l'espace d'Ã©tat de maniÃ¨re alÃ©atoire. Cela constitue un rÃ©sultat trÃ¨s important puisquâ€™il montre que ni la matrice HiPPO (S4), ni lâ€™initialisation HiPPO (DSS) ne sont nÃ©cessaires.  

Concernant lâ€™hybride GSS-Transformer, il consiste simplement Ã  intercaler avec parcimonie des blocs Transformer traditionnels avec des couches GSS. Le modÃ¨le hybride obtient une perplexitÃ© plus faible que le modÃ¨le purement SSM :

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GSS_table2.png" width="700"/> |
|:--:|
| Figure 12 : *Performances du modÃ¨le hybride GSS-Transformer.* |

<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/state-spaces/s4).
<br><br>

### <span style="color: #51C353"> **Mega** </span>

Le 21 septembre 2022, MA, ZHOU et al., ont publiÃ© le [*Mega: Moving Average Equipped Gated Attention*](https://arxiv.org/abs/2209.10655).  
Le Mega est un transformer avec un mÃ©canisme d'attention Ã  une seule tÃªte, utilisant le systÃ¨me de portes du GAU, et est Ã©quipÃ© d'une moyenne mobile exponentielle (EMA) amortie pour incorporer le biais inductif positionnel.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega.png) |
|:--:|
| Figure 13 : *Vue dâ€™ensemble du Mega, figure conÃ§ue Ã  partir de ma comprÃ©hension du papier.<br>Les auteurs remplacent la RELUÂ² du GAU par une fonction de Laplace qui est plus stable (cf. la figure 4 dans le papier).* |

<br>
Les auteurs proposent aussi une variante, Mega-chunk, qui divise efficacement l'ensemble de la sÃ©quence en plusieurs morceaux de longueur fixe. Ils reprennent ici le principe dÃ©jÃ  prÃ©sent et expliquÃ© dans le modÃ¨le FLASH (cf. la figure 4 du papier de ce modÃ¨le). Cela offre une complexitÃ© linÃ©aire en termes de temps et d'espace avec une perte de qualitÃ© minimale.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega-chunk.png) |
|:--:|
| Figure 14 : *Le Mega chunk* |

<br>
Cela offre une complexitÃ© linÃ©aire appliquant simplement l'attention localement Ã  chaque morceau de longueur fixe.  
Plus prÃ©cisÃ©ment, on divise les sÃ©quences de requÃªtes, de clÃ©s et de valeurs en morceaux de longueur $$c$$. Par exemple, $$\mathbf{Q} = {\mathbf{Q}_1, ... , \mathbf{Q}_k}$$, oÃ¹ $$k = -\frac{n}{c}$$ est le nombre de morceaux. L'opÃ©ration d'attention est appliquÃ©e individuellement Ã  chaque bloc, ce qui donne une complexitÃ© linÃ©aire $$\mathcal{O}(kc^2) = \mathcal{O}(nc)$$ par rapport Ã  $$n$$.  
Cette mÃ©thode souffre nÃ©anmoins d'une limitation critique, Ã  savoir la perte d'informations contextuelles provenant d'autres blocs. Mais la sous-couche EMA attÃ©nue ce problÃ¨me en capturant les informations contextuelles locales Ã  proximitÃ© de chaque token dont les rÃ©sultats sont utilisÃ©s comme entrÃ©es dans la sous-couche d'attention. Ainsi, le contexte effectif exploitÃ© par l'attention au niveau du bloc peut aller au-delÃ  de la limite du bloc.  

Le Mega est extrÃªmement compÃ©titif puisquâ€™il devient alors le meilleur modÃ¨le sur le LRA :  
 
| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_Mega.png) |
|:--:|
| Figure 15 : *Les rÃ©sultats du Mega sur le benchmark LRA* |

<br>
Que fait donc un transformer dans un article de blog sur les SSM ? IntÃ©ressons-nous Ã  lâ€™EMA amortie pour comprendre le lien entre le Mega et le S4D.  

â€¢ <u>Rappel sur lâ€™EMA Â« classique Â»</u> :  
Lâ€™Ã©quation dâ€™une [EMA Â« classique Â»](https://fr.wikipedia.org/wiki/Moyenne_mobile#Moyenne_mobile_exponentielle) est $$ğ²_t = ğœ¶ âŠ™ ğ±_t + (1âˆ’ğœ¶) âŠ™ ğ²_{tâˆ’1}$$ avec $$ğœ¶$$ in $$[0,1]^d$$ le coefficient de lâ€™EMA reprÃ©sentant le degrÃ© de diminution de la pondÃ©ration et âŠ™ le produit matriciel de Hadamard.  
Un ğœ¶ plus Ã©levÃ© dÃ©cote plus rapidement les observations les plus anciennes.  
On impose donc ici un biais inductif : le poids de la dÃ©pendance entre deux tokens diminue de maniÃ¨re exponentielle au fil du temps avec un facteur ğœ¶ agnostique Ã  lâ€™entrÃ©e. Cette propriÃ©tÃ© favorise les dÃ©pendances locales et limite les dÃ©pendances Ã  long terme.  
Le calcul de lâ€™EMA peut Ãªtre reprÃ©sentÃ© comme n convolutions individuelles pouvant Ãªtre calculÃ©es efficacement par FFT.  

â€¢ <u>EMA utilisÃ©e dans le Mega</u> :  
Le Mega utilise une EMA Â« amortie Â» multidimensionnelle. Câ€™est-Ã -dire que dans lâ€™Ã©quation de lâ€™EMA Â« amortie Â», $$ğ²_t = ğœ¶ âŠ™ ğ±_t + (1âˆ’ğœ¶ âŠ™ ğœ¹) âŠ™ ğ²_{tâˆ’1}$$ oÃ¹ un paramÃ¨tre $$ğœ¹$$ in $$[0,1]^d$$ est introduit qui reprÃ©sente le facteur dâ€™amortissement, $$x$$ est Ã©tendue Ã  $$h$$ dimensions via une matrice dâ€™expansion $$ğœ·$$ in $$R^{d \times h}$$.  
Lâ€™Ã©quation devient alors $$ğ²_{t,j} = ğœ¼_j^{\intercal} ğ¡_t^{(j)}$$ avec $$ğ¡_t^{(j)} = ğœ¶_j âŠ™ ğ®_t^{(j)}+ (1âˆ’ğœ¶_j âŠ™ ğœ¹_j) âŠ™ ğ¡_{tâˆ’1}^{(j)}$$ et $$ğœ¼ \in R^{d \times h}$$ est la matrice de projection qui renvoie lâ€™Ã©tat cachÃ© en $$h$$ dimension Ã  la sortie unidimensionnelle $$ğ²_{t,j} \in \mathbb{R}$$.  

Preuve que lâ€™ EMA Â« amortie Â» multidimensionnelle peut Ãªtre calculÃ©e comme une convolution et donc par FTT (en fixant $$d = 1$$ pour $$ğœ¶$$ et $$ğœ¹$$) :  
On a $$ğ²_t = ğœ¼^{\intercal} ğ¡_t$$ avec $$ğ¡_t = ğœ¶_j âŠ™ ğ®_t + (1âˆ’ğœ¶ âŠ™ ğœ¹) âŠ™ ğ¡_{tâˆ’1}$$. Notons $$Ï• = 1âˆ’ğœ¶ âŠ™ ğœ¹$$.  
Alors :  $$ğ¡_t = ğœ¶ âŠ™ ğ®_t + (1âˆ’ğœ¶ âŠ™ ğœ¹) âŠ™ ğ¡_{tâˆ’1} = ğœ¶ âŠ™ ğœ· ğ±_t + Ï• âŠ™ ğ¡_{tâˆ’1}$$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; et $$ğ²_t = ğœ¼^{\intercal} ğ¡_t = ğœ¼^{\intercal} (ğœ¶ âŠ™ ğœ· ğ±_t + Ï• âŠ™ ğ¡_{tâˆ’1})$$    
Ensuite, en dÃ©roulant les deux Ã©quations ci-dessus, on obtient explicitement :  
Etape 0 : $$ğ¡_1 = ğœ¶ âŠ™ ğœ·ğ±_1 + Ï• âŠ™ ğ¡_0$$  
Etape 1 : $$ğ¡_2 = ğœ¶ âŠ™ ğœ·ğ±_2 + Ï• âŠ™ ğ¡_1$$
$$= ğœ¶ âŠ™ ğœ· ğ±_2 + Ï• âŠ™ (Ï• âŠ™ ğ¡_0 + ğœ¶ âŠ™ ğœ· ğ±_1) = ğœ¶ âŠ™ ğœ· ğ±_2 + Ï•^2 âŠ™ ğ¡_0 + Ï• âŠ™ ğœ¶ âŠ™ ğœ· ğ±_1$$  
â€¦  

Et de mÃªme :  
Etape 0 : $$ğ²_1 = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_1 + Ï• âŠ™ğ¡0) = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_1 + ğœ¼^{\intercal} Ï• âŠ™ ğ¡_0$$    
Etape 1 : $$ğ²_2 = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_2 + ğœ¼^{\intercal} Ï• âŠ™ ğ¡_1$$ 
$$= ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_2 + ğœ¼^{\intercal} Ï• âŠ™ (ğœ¶ âŠ™ ğœ· ğ±_1 + Ï• âŠ™ ğ¡_0) = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_2 + ğœ¼^{\intercal} Ï• âŠ™ ğœ¶ âŠ™ ğœ· ğ±_1 + ğœ¼^{\intercal} Ï•^2 âŠ™ ğ¡_0$$  
â€¦  
Etape $$t$$ :  $$ğ²_t = ğœ¼^{\intercal} ğœ¶ âŠ™ ğœ· ğ±_t + â€¦ + ğœ¼^{\intercal} Ï•_{tâˆ’1} âŠ™ ğœ¶ âŠ™ ğœ· ğ±_{tâˆ’1} + ğœ¼^{\intercal} Ï•^t âŠ™ ğ¡_0$$.  

Et donc $$ğ² = \mathbf{K} * ğ± + ğœ¼^{\intercal} Ï•^t âŠ™ ğ¡_0$$ avec $$ğ’¦ = (ğœ¼^{\intercal} (ğœ¶ âŠ™ ğœ·), ğœ¼^{\intercal} (Ï• âŠ™ ğœ¶ âŠ™ ğœ·), â€¦, ğœ¼^{\intercal}(Ï•^t âŠ™ ğœ¶ âŠ™ ğœ·) \in \mathbb{R}^n$$.  
$$\mathbf{K}$$ est calculÃ© dans le Mega via le produit de Vandermonde, ce qui nous rappelle la mÃ©thode utilisÃ©e dans le S4D.    
<br>

<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/facebookresearch/mega).  
Le modÃ¨le est Ã©galement disponible sur [Transformers](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/mega#overview).  
Pour plus de dÃ©tails sur les liens entre le Mega et le S4, le lecteur est invitÃ© Ã  consulter les messages Ã©changÃ©s entre Albert GU et les auteurs du Mega trouvables en commentaires de la soumission du Mega sur [Open Review](https://openreview.net/forum?id=qNLe3iq2El). En rÃ©sumÃ©, en Ã©tablissant un lien entre lâ€™Ã©tape de discrÃ©tisation des SSM et lâ€™EMA amortie, il est possible de voir le Mega vue comme un hybride SSM/Attention simplifiant le S4 pour qu'il soit Ã  valeur rÃ©elle plutÃ´t que complexe. 
<br><br>

### <span style="color: #51C353"> **Liquid-S4 : *Liquid Structural State-Space Models*** </span>

Le 26 septembre 2022, HASANI, LECHNER et al. mettent en ligne [*Liquid Structural State-Space Models*](https://arxiv.org/abs/2209.12951.pdf) introduisant le Liquid-S4.
Dans ce papier, les auteurs utilisent la formulation des SSM structurels (S4) pour obtenir des instances de rÃ©seaux liquides linÃ©aires possÃ©dant les capacitÃ©s d'approximation du S4 et des LTC (*liquid time-constant*).  
Les rÃ©seaux neuronaux LTC sont des rÃ©seaux neuronaux causaux Ã  temps continu dotÃ©s d'un module de transition d'Ã©tat dÃ©pendant des entrÃ©es, leur permettant d'apprendre Ã  s'adapter aux entrÃ©es lors de l'infÃ©rence. Il est possible de voir cela comme une sorte de mÃ©canisme de sÃ©lection.  
Pour en savoir plus sur les rÃ©seaux liquides, vous pouvez consulter un papier prÃ©cÃ©dent par les mÃªmes auteurs : [*Liquid Time-constant Networks*](https://ojs.aaai.org/index.php/AAAI/article/view/16936) (2021).  
Dans le cadre du Liquid-S4, il faut simplement savoir que l'Ã©tat d'un LTC Ã  chaque pas de temps est donnÃ© par :
\begin{equation}
    \frac{d\textbf{x}(t)}{dt} = - \underbrace{\Big[\mathbf{A} + \mathbf{B} \odot f(\textbf{x}(t),\textbf{u}(t), t, \theta)\Big]}_\text{Liquid time-constant} \odot \textbf{x}(t) + \mathbf{B} \odot f(\textbf{x}(t), \textbf{u}(t), t, \theta). 
\end{equation}
Avec :  
- $$\textbf{x}^{(N \times 1)}(t)$$ est le vecteur de l'Ã©tat cachÃ© de taille $N$,
- $$\textbf{u}^{(m \times 1)}(t)$$ est un signal d'entrÃ©e avec $m$ caractÃ©ristiques,
- $$\mathbf{A}^{(N \times 1)}$$ est un mÃ©canisme de transition d'Ã©tat constant dans le temps,
- $$\mathbf{B}^{(N \times 1)}$$ est un vecteur de biais
- $$\odot$$ reprÃ©sente le produit d'Hadamard
- $$f(.)$$ est une non-linÃ©aritÃ© bornÃ©e paramÃ©trÃ©e par $\theta$.  

En pratique, un SSM utilisant un rÃ©seau liquide se formule via le systÃ¨me d'Ã©quations diffÃ©rentielles suivant :  

$$
  \begin{aligned}
    x' &= (\mathbf{A} + \mathbf{B} u) x + \mathbf{B}u\\
    y &= \mathbf{C}x
  \end{aligned}
$$

Ce systÃ¨me dynamique peut Ãªtre rÃ©solu efficacement via la mÃªme paramÃ©trisation que le S4, en donnant lieu Ã  un noyau convolutif supplÃ©mentaire qui prend en compte les similitudes des signaux dÃ©calÃ©s. Le modÃ¨le obtenu est le Liquid-S4. Explicitons ceci avec un peu de maths.  

La vue rÃ©currente du Liquid-S4 est obtenue en discrÃ©tisant le systÃ¨me avec la rÃ¨gle des trapÃ¨zes (forme bilinÃ©aire). On obtient alors :
\begin{align}
    x_k = \big( \overline{\textbf{A}} + \overline{\textbf{B}}~u_k\big)~x_{k-1} + \overline{\textbf{B}}~u_k,~~~~~~y_k = \overline{\textbf{C}}~x_k
\end{align}

Comme pour le S4, la vue convolutive est obtenue en dÃ©roulant la vue rÃ©currente dans le temps (en supposant $$x_{-1} = 0$$) :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Liquid_S4_kernel.png) |
|:--:|
| Figure 16 : *La partie en violet ne s'affichant pas sur mon blog, je dois passer par une image :/* |

<br>

Vous pouvez voir deux couleurs dans les formules. Elles correspondent Ã  deux types de configurations de poids :  
1. En noir, les poids des entrÃ©es temporelles individuelles indÃ©pendantes, i.e le noyau convolutif du S4.  
2. En violet, les poids associÃ©s Ã  tous les ordres d'auto-corrÃ©lation du signal d'entrÃ©e. Il s'agit d'un noyau de corrÃ©lation d'entrÃ©e supplÃ©mentaire, appelÃ© noyau liquide par les auteurs.
   
Finalement le noyau de convolution s'exprime de la faÃ§on suivante :
$$
    \overline{\textbf{K}}_{\text{liquid}} \in \mathbb{R}^{\tilde{L}} := \mathcal{K}_L(\overline{\textbf{C}},\overline{\textbf{A}},\overline{\textbf{B}}) := \big(\overline{\textbf{C}}\overline{\textbf{A}}^{(\tilde{L}-i-p)}\overline{\textbf{B}}^p\big)_{i \in [\tilde{L}],~ p \in [\mathcal{P}]} = \big( \overline{\textbf{C}}\overline{\textbf{A}}^{\tilde{L}-2}\overline{\textbf{B}}^2, \dots, \overline{\textbf{C}}\overline{\textbf{B}}^p \big)
$$

Les auteurs montrent ensuite que ceci est calculable efficacement via un processus semblable Ã  ce qui a Ã©tÃ© appliquÃ© dans le S4 (HiPPO, Woodbury, TransformÃ©e de Fourier inverse, etc.). Nous invitons le lecteur Ã  consulter l'algorithme 1 dans le papier pour plus de dÃ©tails.  

TestÃ©e sur le LRA, cette approche apparaÃ®t comme la meilleure. Seul Mega, publiÃ© sept jours plus tÃ´t et donc non prÃ©sent dans le papier, fait mieux :

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_Liquid_S4.png" width="700"/> |
|:--:|
| Figure 17 : *Les rÃ©sultats du Liquid-S4 sur le benchmark LRA* |

<br>

HASANI, LECHNER et al. appliquent leur modÃ¨le Ã©galement sur les jeux de donnÃ©es Speech Commands, sCIFAR et [BIDMC Vital Signs](https://ieeexplore.ieee.org/document/7748483) de PIMENTEL et al., et y Ã©tablissent le nouvel Ã©tat de l'art.  
<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/raminmh/liquid-s4).  
Les Ã©changes sur [Open Review](https://openreview.net/forum?id=g4OTKRKfS7R).  
L'implÃ©mentation officielle des LTC sur [GitHub](https://github.com/raminmh/liquid_time_constant_networks).
<br><br>

### <span style="color: #51C353"> **Le S5 : *Simplified State Space Layers for Sequence Modeling*** </span>

Chronologiquement, le [*Simplified State Space Layers for Sequence Modeling*](https://arxiv.org/abs/2208.04933) de SMITH, WARRINGTON et LINDERMAN introduisant le modÃ¨le S5 a Ã©tÃ© dÃ©voilÃ© le 9 aoÃ»t 2022, donc avant le Mega et le Liquid-S5. Cependant j'aborde ce papier aprÃ¨s ces derniers car le 6 octobre 2022, les auteurs du S5 ont procÃ©dÃ© Ã  une actualisation de leur publication amÃ©liorant leur modÃ¨le de plus de 5 points sur le LRA par rapport Ã  la V1. De plus, ils proposent une comparaison portant sur l'ensemble des SSM sortis en 2022. Cela me paraissait plus pertinent d'aborder le S5 Ã  partir de sa V2.  

Dans le S5, les auteurs proposent de remplacer la formulation du S4 utilisant une banque de SSM indÃ©pendants entrÃ©e unique/sortie unique (SISO pour *single-input, single-output*) par un SSM Ã  entrÃ©e multiple/sortie multiple (MIMO pour *multi-input, multi-output*) qui a une dimension latente rÃ©duite. 

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S5vsS4.png" width="700"/> |
|:--:|
| Figure 18 : *Le comportement interne du S4 vs celui du S5* |

<br>

La dimension latente rÃ©duite du systÃ¨me MIMO permet l'utilisation de l'algorithme *parallel scan* qui simplifie les calculs nÃ©cessaires pour appliquer la couche S5 en tant que transformation sÃ©quence Ã  sÃ©quence. Le modÃ¨le rÃ©sultant perd ainsi la vue convolutive du SSM pour se focaliser uniquement sur la vue rÃ©currente (obtenue par discrÃ©tisation ZOH). Le parti pris des auteurs est donc d'opÃ©rer sur le domaine temporel plutÃ´t que celui frÃ©quentiel. Ils utilisent une approximation diagonale de la matrice HiPPO leur permettant d'avoir une initialisation et une paramÃ©trisation efficaces adaptÃ©es Ã  leur systÃ¨me MIMO.

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S5.png" width="700"/> |
|:--:|
| Figure 19 : *Comparaison complÃ¨te du fonctionnement du S4 vs celui du S5* |

<br>

L'utilisation du *parallel scan* Ã©tant un composant repris dans d'autres SSM par la suite (le Mamba notamment), dÃ©taillons un peu son fonctionnent dans le cadre du S5 afin de se familiariser avec cet algorithme dÃ¨s cet article. Pour cela le plus simple est de reprendre l'exemple donnÃ© dans l'appendix H du papier oÃ¹ les auteurs l'appliquent sur une sÃ©quence de longeur $$L = 4$$.  

Pour calculer un *parallel scan* deux choses sont nÃ©cessaires :  
- Les Ã©lÃ©ments initiaux sur lesquels l'analyse va opÃ©rer.  
Nous dÃ©finissons les Ã©lÃ©ments initiaux d'une sÃ©quence de longueur $$L$$ comme, $$c_{1:L}$$, de sorte que chaque Ã©lÃ©ment $$c_k$$ soit le tuple $$c_k = (c_{k,a}, c_{k,b}) := (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_k)$$. Dans le cas de $$L = 4$$, nous avons donc $$(\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_1), (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_2),(\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_3)$$ et $$(\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_4)$$.  
- Un opÃ©rateur associatif binaire $$\bullet$$ utilisÃ© pour combiner les Ã©lÃ©ments. MathÃ©matiquement, un opÃ©rateur associatif binaire Ã©tant $$ L \bullet M \bullet C = (L \bullet M)\bullet N = L \bullet (M \bullet N) $$.  

Si nous devions procÃ©der de maniÃ¨re sÃ©quentielle avec le *scan*, en posant $$s_0:=(\mathbf{I},0)$$, nous devrions effectuer 4 calculs pour obtenir les 4 sorties $$s_i$$ :  
$$s_1 = s_0 \bullet c_1 =  (\mathbf{I},\enspace 0) \bullet (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_1) = (\overline{\mathbf{A}}\mathbf{I},\enspace \overline{\mathbf{A}}0+\overline{\mathbf{B}}u_1) = (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_1)$$  
$$s_2 = s_1 \bullet c_2 =(\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_1) \bullet (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_2)  =  (\overline{\mathbf{A}}^2,\enspace \overline{\mathbf{A}}\overline{\mathbf{B}}u_1 + \overline{\mathbf{B}}u_2 )$$  
$$s_3 = s_2 \bullet c_3 = (\overline{\mathbf{A}}^2,\enspace \overline{\mathbf{A}}\overline{\mathbf{B}}u_1 + \overline{\mathbf{B}}u_2 ) \bullet (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_3)  =  (\overline{\mathbf{A}}^3,\enspace \overline{\mathbf{A}}^2\overline{\mathbf{B}}u_1 + \overline{\mathbf{A}}\overline{\mathbf{B}}u_2  + \overline{\mathbf{B}}u_3)$$  
$$s_4 = s_3 \bullet c_4 = (\overline{\mathbf{A}}^3,\enspace \overline{\mathbf{A}}^2\overline{\mathbf{B}}u_1 + \overline{\mathbf{A}}\overline{\mathbf{B}}u_2  + \overline{\mathbf{B}}u_3) \bullet (\overline{\mathbf{A}},\enspace \overline{\mathbf{B}}u_4)\\ = (\overline{\mathbf{A}}^4, \enspace \overline{\mathbf{A}}^3\overline{\mathbf{B}}u_1 + \overline{\mathbf{A}}^2\overline{\mathbf{B}}u_2 + \overline{\mathbf{A}}\overline{\mathbf{B}}u_3 + \overline{\mathbf{B}}u_4).$$

Pour obtenir les Ã©tats $$x_i$$, nous devrions alors prendre le deuxiÃ¨me Ã©lÃ©ment de chaque tuple $$s_i$$.  

ProcÃ©der de maniÃ¨re sÃ©quentielle n'est pas la plus efficace puisqu'il est possible de parallÃ©liser le calcul d'une rÃ©currence avec le *parallel scan*.
Ci-dessous une illustration de son fonctionnement dans le cadre de notre sÃ©quence de taille $$L$$ = 4 :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/parallel_scan_S5.gif) |
|:--:|
| Figure 20 : *Fonctionnement du parallel scan dans le cadre du S5* |

A nouveau, pour obtenir les Ã©tats $$x_i$$, nous devrions alors prendre le deuxiÃ¨me Ã©lÃ©ment de chaque tuple $$s_i$$.  
Vous remarquerez ici qu'il est possible de calculer $$s_2$$ et $$i_4$$ en parallÃ¨le, puis $$s_1$$, $$s_3$$ et $$s_4$$ en parallÃ¨le. On passe alors de 4 calculs sÃ©quentiels Ã  seulement 2. La complexitÃ© du *parallel scan* Ã©tant en $$O(log(L))$$.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/parallel_scan.gif) |
|:--:|
| Figure 21 : *Fonctionnement du parallel scan d'une maniÃ¨re gÃ©nÃ©rale* |

<br>

Concernant les performances du S5, celui-ci se classe deuxiÃ¨me sur le LRA :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_S5.png) |
|:--:|
| Figure 22 : *Les rÃ©sultats du S5 sur le benchmark LRA* |

<br>
Notons qu'en plus du LRA, les auteurs du S5 comparent leur modÃ¨le sur le *Speech Commands*, le *pendulum regression dataset* ainsi que *sMNIST*, *psMNIST* et *sCIFAR*. L'ensemble des rÃ©sultats est disponible dans l'Appendix du papier qui contient Ã©galement une Ã©tude d'ablation.
<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/lindermanlab/S5).  
Les Ã©changes sur [Open Review](https://openreview.net/forum?id=Ai8Hw3AXqks).
<br><br>

### <span style="color: #51C353"> **SGConv** </span>

Le 17 octobre 2022, [*What Makes Convolutional Models Great on Long Sequence Modeling?*](https://arxiv.org/abs/2210.09298) LI, CAI et al. indiquent trouver le S4 trop complexe car il nÃ©cessite un paramÃ©trage et des schÃ©mas d'initialisation sophistiquÃ©s (= HiPPO). Et par consÃ©quent quâ€™il est moins intuitif et difficile Ã  utiliser pour les personnes ayant des connaissances prÃ©alables limitÃ©es. Ainsi leur objectif est de dÃ©mystifier le S4 en se focalisant sur la vue convolutive de ce dernier.
Ils identifient deux principes critiques dont bÃ©nÃ©ficie S4 et qui sont suffisants pour constituer un modÃ¨le convolutif global performant :  
1) La paramÃ©trisation du noyau convolutif doit Ãªtre efficace dans le sens oÃ¹ le nombre de paramÃ¨tres doit augmenter de faÃ§on sous-linÃ©aire avec la longueur de la sÃ©quence.  
2) Le noyau doit avoir une structure dÃ©croissante selon laquelle les poids pour la convolution avec les voisins les plus proches sont plus importants que ceux des voisins les plus Ã©loignÃ©s.

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/SGConv_kernel.png) |
|:--:|
| Figure 23 : *Respecter les deux principes critiques Ã©noncÃ©s par les auteurs revient Ã  avoir des noyaux de convolution ressemblant Ã  ceux visibles sur la figure.* |

<br>

Sur la base de ces deux principes, ils proposent un modÃ¨le convolutif efficace appelÃ© Convolution globale structurÃ©e (SGConv). 

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/SGConv.png" width="700"/> |
|:--:|
| Figure 24 : *Le SGConv construit les noyaux de convolution comme la concatÃ©nation de sinusoÃ¯des successivement plus longues mais de norme plus faible. L'avantage de cette forme est qu'elle permet une convolution trÃ¨s rapide dans le domaine des frÃ©quences.* |

<br>

Les auteurs du SGConv indiquent quâ€™ils obtiennent de meilleurs rÃ©sultats que le S4 sur plusieurs tÃ¢ches (textes, audios, images). Nous ne les dÃ©taillerons pas toutes. IntÃ©ressons-nous seulement au LRA :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_SGConv.png) |
|:--:|
| Figure 25 : *RÃ©sultats du SGConv sur le LRA.* |

<br>

En effet, en regardant ce tableau, on peut effectivement constater que le SGConv fait mieux que les deux versions du S4. NÃ©anmoins, il est curieux que les auteurs nâ€™intÃ¨grent pas le Mega, le Liquid-S4 ou encore le S5 dans leur comparaison qui pourtant obtiennent de meilleurs rÃ©sultats en utilisant un noyau de convolution qui est une somme de fonctions exponentielles dÃ©croissantes.  
De plus, alors que tous les modÃ¨les s'Ã©tant Ã©valuer sur le LRA traitent les donnÃ©es comme des sÃ©quences 1D, le SGConv intÃ¨gre implicitement un biais inductif 2D pour les tÃ¢ches d'image, y compris PathX ce qui est questionnable.  

Au final, le SGConv semble donc avoir des performances similaires aux variantes de SSM les plus rÃ©centes mais en perdant la vue rÃ©currente du S4.    
Ce papier apparaÃ®t nÃ©anmoins comme le premier se focalisant uniquement sur la vue convolutive d'un SSM.
<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/ctlllll/SGConv).  
Les Ã©changes sur [Open Review](https://openreview.net/forum?id=TGJSPbRpJX-).
<br><br>


### <span style="color: #51C353"> **Autres modÃ¨les** </span>
Deux autres papiers Â« thÃ©oriques Â» ont Ã©tÃ© publiÃ© en 2022. Le [*Pretraining Without Attention*](https://arxiv.org/abs/2212.10544) de WANG et al. qui prÃ©sente le BiGS et le [*Hungry Hungry Hippos: Towards Language Modeling with State Space Models*](https://arxiv.org/abs/2212.14052) de FU, DAO et al. introduisant le H3.  
Du fait de leur publication tardive (respectivement les 20 et 28 dÃ©cembre 2022) et de leur communication effectuÃ©e en 2023 aprÃ¨s les V2 de chacun de ces papiers, je traiterais ces modÃ¨les dans le prochain article de la sÃ©rie sur les SSM.  
<br><br>

## <span style="color: #FFBF00"> **Applications des SSM** </span>

### <span style="color: #51C353"> **SaShiMi** </span>

Dans [*It's Raw! Audio Generation with State-Space Models*](https://arxiv.org/abs/2202.09729) paru le 20 fÃ©vrier 2022, GOEL, GU et al. appliquent le S4 Ã  de la gÃ©nÃ©ration dâ€™audio de maniÃ¨re causale.  
Contrairement aux mÃ©thodes reposant sur le conditionnement Ã  partir de textes, de spectrogrammes, etc., il s'agit d'une mÃ©thode opÃ©rant directement sur le signal dâ€™entrÃ© permettant de se comparer notamment au [WaveNet](https://arxiv.org/abs/1609.03499) de OORD et al. (2016).  
SaShiMi peut s'entraÃ®ner directement sur des sÃ©quences de plus de 100K (8s audio) sur un seul GPU V100, comparÃ© aux limitations de longueur de contexte auxquelles font face des modÃ¨les comme WaveNet. Il utilise efficacement ce long contexte pour amÃ©liorer l'estimation de la densitÃ©.  
Les auteurs ont comparÃ© leur modÃ¨le sur divers benchmarks portant notamment sur de la gÃ©nÃ©ration de musique de piano ou encore de la parole (Ã©nonciation de chiffres).  
Il est possible de consulter les audios gÃ©nÃ©rÃ©s [ici](https://hazyresearch.stanford.edu/sashimi-examples/).  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Sashimi.png) |
|:--:|
| Figure 26 : *Vue d'ensemble de l'architecture de Sashimi* |

<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/state-spaces/s4).
<br><br>

### <span style="color: #51C353"> **ViS4mer** </span>

ISLAM et BERTASIUS introduisent le 4 avril 2022 le ViS4mer dans leur [*Long Movie Clip Classification with State-Space Video Models*](https://arxiv.org/abs/2204.01692).  
Il s'agit d'un hybride entre un S4 et un Transformer afin de rÃ©aliser de la classification de (longues) vidÃ©os. Plus prÃ©cisÃ©ment, le modÃ¨le utilise un encodeur Transformer standard pour l'extraction des caractÃ©ristiques spatiotemporelles Ã  courte distance, et un dÃ©codeur S4 temporel multi-Ã©chelle pour le raisonnement temporel Ã  longue distance. Le modÃ¨le alors obtenu apparaÃ®t comme Ã©tant 2,6 fois plus rapide et 8 fois plus efficace en termes de mÃ©moire qu'un Transformer.  
Il sâ€™agit Ã  ma connaissance du premier papier Ã  avoir mis en avant lâ€™intÃ©rÃªt dâ€™hybrider des SSM et des Transformers.  

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/ViS4mer_decodeur.png" width="700"/> |
|:--:|
| Figure 27 : *Vue d'ensemble du dÃ©codeur du ViS4mer*|

<br>

ViS4mer obtient des rÃ©sultats de pointe dans 6 des 9 tÃ¢ches de classification de vidÃ©os de longue durÃ©e sur le benchmark [*Long Video Understanding*](https://arxiv.org/abs/2106.11310) (LVU) de WU et KRÃ„HENBÃœHL (2021) qui consiste Ã  classer des vidÃ©os ayant une durÃ©e de 1 Ã  3 min. Le modÃ¨le semble Ã©galement avoir de bonnes capacitÃ©s de gÃ©nÃ©ralisation en obtenant des rÃ©sultats compÃ©titifs sur les jeux de donnÃ©es Breakfast et COIN procedural activity alors qu'il a vu 275 fois moins de donnÃ©es.<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/md-mohaiminul/ViS4mer).
<br><br>

### <span style="color: #51C353"> **CCNN** </span>

Le 7 juin 2022, ROMERO, KNIGGE et al. introduisent le CCNN dans leur papier [*Towards a General Purpose CNN for Long Range Dependencies in ND*](https://arxiv.org/abs/2206.03398).  
Ils partent de l'idÃ©e que les rÃ©seaux de neurones convolutifs sont puissants mais doivent Ãªtre adaptÃ©s spÃ©cifiquement Ã  chaque tÃ¢che :  
â€¢ Longueur de l'entrÃ©e : 32x32, 1024x1024 â†’ Comment modÃ©liser les dÃ©pendances Ã  longue distance ?  
â€¢ RÃ©solution de l'entrÃ©e : 8kHz, 16kHz â†’ DÃ©pendances Ã  longue distance, agnosticitÃ© de la rÃ©solution ?  
â€¢ DimensionnalitÃ© de l'entrÃ©e : 1D, 2D, 3D â†’ Comment dÃ©finir les noyaux convolutifs ?  
â€¢ TÃ¢che : Classification, Segmentation, ... â†’ Comment dÃ©finir les stratÃ©gies d'Ã©chantillonnage haut-bas ?  
Est-il alors possible de concevoir une architecture unique, avec laquelle les tÃ¢ches peuvent Ãªtre rÃ©solues indÃ©pendamment de la dimensionnalitÃ©, de la rÃ©solution et de la longueur de l'entrÃ©e, sans modification de l'architecture ?
Oui et ceci grÃ¢ce au CCNN qui utilise des noyaux de convolution continus.  

ROMERO, KNIGGE et al. s'inspirent notamment du S4 pour crÃ©er une variante de blocs rÃ©siduels efficaces qu'ils appellent *bloc S4*. Toutefois, contrairement au S4 qui ne fonctionne qu'avec des signaux 1D, le CCNN modÃ©lise facilement des signaux ND.

| <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/CCNN.png" height="600"/> |
|:--:|
| Figure 28 : *Vue d'ensemble du CCNN*|

<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/david-knigge/ccnn).  
Les diapositives d'une prÃ©sentation du papier est disponible [ici](https://app.slidebean.com/sbp/gk5j826nq7/CCNN#1)
<br><br>

### <span style="color: #51C353"> **$$\mathbf{SSSD^{S4}}$$**</span>

Le 19 aoÃ»t 2022, LOPEZ ALCARAZ et STRODTHOFF proposent dans leur papier [*Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models*](https://arxiv.org/abs/2208.09399) un hybride entre un S4 et un modÃ¨le de diffusion pour la prÃ©diction de donnÃ©es manquantes dans des sÃ©ries temporelles. Leur modÃ¨le est dÃ©nommÃ© **$$\mathbf{SSSD^{S4}}$$** (ou plus simplement SSSD).

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/SSSDS4.png) |
|:--:|
| Figure 29 : *Vue d'ensemble du $$\mathbf{SSSD^{S4}}$$*|

<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/AI4HealthUOL/SSSD).
<br><br>

### <span style="color: #51C353"> **S4ND** </span>

Le 12 octobre 2022, NGUYEN, GOEL, GU et al. prÃ©sentent le [*S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces*](https://arxiv.org/abs/2210.06583).

Ce modÃ¨le Ã©tend le S4 (qui est 1D) aux signaux continus multidimensionnels tels que les images et les vidÃ©os (lÃ  oÃ¹ les ConvNets et ViT apprennent sur des pixels discrets). Pour cela ils transforment l'ODE standard du S4 en une EDP multidimensionnelle :  

$$
\begin{aligned}
  x'(t) &= \mathbf{A}x(t) + \mathbf{B}u(t)  \\
  y(t)  &= \mathbf{C}x(t)
\end{aligned}
$$

devient :  

$$
 \begin{aligned}
      \frac{\partial}{\partial t^{(1)}} x(t^{(1)}, t^{(2)}) &= (\mathbf{A}^{(1)} x^{(1)}(t^{(1)}, t^{(2)}), x^{(2)}(t^{(1)}, t^{(2)})) + \mathbf{B}^{(1)} u(t^{(1)}, t^{(2)})
      \\
      \frac{\partial}{\partial t^{(2)}} x(t^{(1)}, t^{(2)}) &= (x^{(1)}(t^{(1)}, t^{(2)}), \mathbf{A}^{(2)} x^{(2)}(t^{(1)}, t^{(2)})) + \mathbf{B}^{(2)} u(t^{(1)}, t^{(2)})
      \\
      y(t^{(1)}, t^{(2)}) &= \langle \mathbf{C}, x(t^{(1)}, t^{(2)}) \rangle
\end{aligned}
$$

avec $$\mathbf{A}^{(\tau)} \in \mathbb{C}^{N^{(\tau)} \times N^{(\tau)}} \), \( \mathbf{B}^{(\tau)} \in \mathbb{C}^{N^{(\tau)} \times 1} \),   \( \mathbf{C} \in \mathbb{C}^{N^{(1)} \times N^{(2)}}$$, avec comme condition initiale de l'EDP linÃ©aire $$x(0, 0) = 0$$.
  
En fonction du jeu de donnÃ©es testÃ©, les auteurs obtiennent des rÃ©sultats similaires ou plus performants que ceux d'un ViT ou un ConvNext.  
L'intÃ©rÃªt principal du S4ND Ã©tant qu'il peut fonctionner avec diffÃ©rentes rÃ©solutions via diffÃ©rents taux d'Ã©chantillonnage. Les auteurs mettent en avant cette caractÃ©ristique Ã  travers deux expÃ©riences :  
1) En zÃ©ro-shot, S4ND surpasse un Conv2D de plus de 40 points lorsqu'il est entraÃ®nÃ© sur des images $$8\times8$$ et testÃ© sur des images $$32\times32$$.  
2) Avec un redimensionnement progressif, S4ND peut accÃ©lÃ©rer l'entraÃ®nement de 22% avec une baisse de la prÃ©cision finale de âˆ¼1% par rapport Ã  l'entraÃ®nement Ã  la seule haute rÃ©solution.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4ND.png) |
|:--:|
| Figure 30 : *Exemple du S4ND pour des images 2D*|

<br>
<u>Pour aller plus loin</u>  
L'implÃ©mentation officielle est disponible sur [GitHub](https://github.com/AI4HealthUOL/SSSD).  
Une information complÃ¨tement inutile mais amusante : les auteurs ont appelÃ© leur fichier TeX [Darude S4NDstorm](https://www.youtube.com/watch?v=y6120QOlsfU).
<br><br><br>


# <span style="color: #FF0000"> **Conclusion** </span>
Nous avons donc fait une revue des diffÃ©rents modÃ¨les de SSM parus en 2022. 
Il s'agit d'une annÃ©e oÃ¹ les travaux se sont principalement portÃ©s sur une amÃ©lioration/simplification du S4 via diverses approches (diagonalisation, *gating*, LTC, etc.). Lors de cette annÃ©e 2022, nous avons Ã©galement pu voir les premiÃ¨res applications des SSM.  
Avec le SGConv et le S5, nous pouvons aussi apercevoir les prÃ©mices d'un phÃ©nomÃ¨ne qui, comme nous le verrons dans lâ€™article suivant, s'accentuera en 2023. A savoir lâ€™Ã©mergence de travaux se focalisant uniquement sur la vue convolutive des SSM (par exemple le Hyena et ses dÃ©rivÃ©s) ou se focalisant uniquement sur la vue rÃ©currente des SSM (par exemple le Mamba).
<br><br><br>

# <span style="color: #FF0000"> **RÃ©fÃ©rences** </span>
- [Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers](https://arxiv.org/abs/2110.13985) d'Albert GU, Isys JOHNSON, Karan GOEL, Khaled SAAB, Tri DAO, Atri RUDRA, Christopher RÃ‰ (2021)
- [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396) d'Albert GU, Karan GOEL, et Christopher RÃ‰ (2021)
- [HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://arxiv.org/abs/2008.07669) d'Albert GU, Tri DAO, Stefano ERMON, Atri RUDRA, Christopher RÃ‰ (2020)
- [*How to Train Your HiPPO*](https://arxiv.org/abs/2206.12037) d'Albert GU, Isys JOHNSON, Aman TIMALSINA, Atri RUDRA, and Christopher RÃ‰ (2022)
- [*Long Range Arena: A Benchmark for Efficient Transformers*](https://arxiv.org/abs/2011.04006) de Yi TAY, Mostafa DEHGHANI, Samira ABNAR, Yikang SHEN, Dara BAHRI, Philip PHAM, Jinfeng RAO, Liu YANG, Sebastian RUDER et Donald METZLER (2020)
- [*Diagonal State Spaces are as Effective as Structured State Spaces*](https://arxiv.org/abs/2203.14343) de Ankit GUPTA, Albert GU et Jonathan BERANT (2022)
- [*Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition*](https://arxiv.org/abs/1804.03209v1) de Pete WARDEN (2018)
- [*Simplifying and Understanding State Space Models with Diagonal Linear RNNs*](https://arxiv.org/abs/2212.00768) de Ankit GUPTA, Harsh MEHTA et Jonathan BERANT (2022)
- [*On the Parameterization and Initialization of Diagonal State Space Models*](https://arxiv.org/abs/2206.11893) d'Albert GU, Ankit GUPTA, Karan GOEL, Christopher RÃ‰
- [*Long Range Language Modeling via Gated State Spaces*](https://arxiv.org/abs/2206.13947.pdf) d'Harsh MEHTA, Ankit GUPTA, Ashok CUTKOSKY et Behnam NEYSHABUR (2022)
- [*Language Modeling with Gated Convolutional Networks*](https://arxiv.org/abs/1612.08083) de Yann N. DAUPHIN, Angela FAN, Michael AULI et David GRANGIER (2016)
- [*GLU Variants Improve Transformer*](https://arxiv.org/abs/2002.05202) de Noam SHAZEER (2020)
- [*Transformer Quality in Linear Time*](https://arxiv.org/abs/2202.10447) de Weizhe HUA, Zihang DAI, Hanxiao LIU et Quoc V. LE (2022)
- [*Mega: Moving Average Equipped Gated Attention*](https://arxiv.org/abs/2209.10655) de Xuezhe MA, Chunting ZHOU, Xiang KONG, Junxian HE, Liangke GUI, Graham NEUBIG, Jonathan MAY et Luke ZETTLEMOYER (2022)
- [*Liquid Structural State-Space Models*](https://arxiv.org/abs/2209.12951.pdf) de Ramin HASANI, Mathias LECHNER, Tsun-Hsuan WANG, Makram CHACHINE, Alexander AMINI et Daniela RUS (2022)
- [*Liquid Time-constant Networks*](https://ojs.aaai.org/index.php/AAAI/article/view/16936) de Ramin HASANI, Mathias LECHNER, Alexander AMINI, Daniela RUS et Radu GROSU (2021)
- [*Simplified State Space Layers for Sequence Modeling*](https://arxiv.org/abs/2208.04933) de Jimmy T.H. SMITH, Andrew WARRINGTON et Scott W. LINDERMAN (2022)
- [*Toward a Robust Estimation of Respiratory Rate From Pulse Oximeters*](https://ieeexplore.ieee.org/document/7748483) de Marco A. F. PIMENTEL, Alistair E. W. JOHNSON, Peter H. CHARLTON, Drew BIRRENKOTT, Peter J. WATKINSON, Lionel TARASSENKO et David A. CLIFTON (2017)
- [*What Makes Convolutional Models Great on Long Sequence Modeling?*](https://arxiv.org/abs/2210.09298) de Yuhong LI, Tianle CAI, Yi ZHANG, Deming CHEN et Debadeepta DEY (2022)
-  [*Pretraining Without Attention*](https://arxiv.org/abs/2212.10544) de Junxiong WANG, Jing Nathan YAN, Albert GU et Alexander M. RUSH (2022)
-  [*Hungry Hungry Hippos: Towards Language Modeling with State Space Models*](https://arxiv.org/abs/2212.14052) de Daniel Y. FU, Tri DAO, Khaled K. SAAB, Armin W. THOMAS, Atri RUDRA et Christopher RÃ‰ (2022)
- [*It's Raw! Audio Generation with State-Space Models*](https://arxiv.org/abs/2202.09729) de Karan GOEL, Albert GU, Chris DONAHUE, Christopher RÃ‰ (2022)
- [*WaveNet: A Generative Model for Raw Audio*](https://arxiv.org/abs/1609.03499) de Aaron van den OORD, Sander DIELEMAN, Heiga ZEN, Karen SIMONYAN, Oriol VINYALS, Alex GRAVES, Nal KALCHBRENNER, Andrew SENIOR et Koray KAVUKCCUOGLU
- [*Long Movie Clip Classification with State-Space Video Models*](https://arxiv.org/abs/2204.01692) de Md Mohaiminul ISLAM, Gedas BERTASIUS (2022)
- [*Long Video Understanding*](https://arxiv.org/abs/2106.11310) de Chao-Yuan WU et Philipp KRÃ„HENBÃœHL (2021) 
- [*Towards a General Purpose CNN for Long Range Dependencies in ND*](https://arxiv.org/abs/2206.03398) de David W. ROMERO, David M. KNIGGE, Albert GU, Erik J. BEKKERS, Efstratios GAVVES, Jakub M. TOMCZAK, Mark HOOGENDOORN (2022)
- [*Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models*](https://arxiv.org/abs/2208.09399) de Juan Miguel LOPEZ ALCARAZ, Nils STRODTHOFF (2022)
- [*S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces*](https://arxiv.org/abs/2210.06583) de Eric NGUYEN, Karan GOEL, Albert GU, Gordon W. DOWNS, Preey SHAH, Tri DAO, Stephen A. BACCUS, Christopher RÃ‰ (2022)
<br><br><br>

# <span style="color: #FF0000"> **Citation** <span>
> @inproceedings{ssm_in_2022_blog_post,  
  author    = {LoÃ¯ck BOURDOIS},  
  title     = {Ã‰volution des State Space Models (SSM) en 2022},  
  year      = {2023},  
  url = {https://lbourdois.github.io/blog/ssm/ssm_en_2022}  
}
