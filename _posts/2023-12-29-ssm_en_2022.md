---
title: "ÉVOLUTION DES STATE SPACE MODELS (SSM) EN 2022"
categories:
  - SSM
tags:
  - Introduction aux SSM 
excerpt : SSM - Revue de littérature des SSM parus lors de l'année 2022
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/NLP_radom_blog.png"
    teaser: "https://github.com/lbourdois/blog/assets/58078086/cb2dca34-9a3e-481a-8773-2360a1ceaa1c"
author_profile: false
sidebar:
    nav: sidebar-ssm
classes: wide
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# ⚠ **WIP** ⚠ 


# <span style="color: #FF0000"> **Avant-propos** </span>
Dans l’[article précédent](https://lbourdois.github.io/blog/ssm/introduction_ssm/), nous avons défini ce qu’est un State Space Model (SSM) à l’aide d’un système en temps continu. Nous l’avons discrétisé pour faire apparaître sa vue récurrente ainsi que sa vue convolutive. L’intérêt est alors de pouvoir entraîner notre modèle de manière convolutive puis de réaliser l’inférence de manière récurrente sur de très longues séquences.  
<br>

| ![image](https://github.com/lbourdois/blog/assets/58078086/12bbe1cf-3911-4bad-9a3b-3f427bc6bc82)|
|:--:|
| Figure 1 : *Image provenant de l'article de blog « [Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models](https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3) » d'Albert GU et al. (2022)*|

<br>
Cette vision a été introduite par Albert GU dans ses papiers LSSL et S4 parus en 2021. Le S4 étant l’équivalent du « *Attention is all you need* » pour les transformers.
Dans cet article, nous allons faire une revue de littérature des SSM parus durant l’année 2022. Ceux apparus en 2023 seront listés dans le prochain article.
L’objectif est de montrer les différentes évolutions de ces types de modèles au cours des mois, tout en restant à une vue globale des choses (i.e. je ne vais pas rentrer dans tous les détails des papiers listés). Lors de cette année 2022, les différentes avancées se sont focalisées à appliquer des algorithmes de discrétisations différents, ainsi qu’à remplacer la matrice HiPPO par une plus simple.
<br><br><br>

## <span style="color: #FFBF00"> **Modèles théoriques** </span>

Dans cette section, nous allons passer en revue des travaux théorique, au sens où ils sont des propositions d’amélioration de l’architecture du S4. Nous aborderons ensuite dans une section différente, des applications concrètes sur différentes tâches (audio, vision, etc.).
<br><br>

### <span style="color: #51C353"> **S4 V2** </span>

Commençons par une partie que le lecteur peut considérer comme facultative car dès la section suivante nous verrons que ce qui est décrit dans celle-ci se retrouve caduc du fait du remplacement de la matrice HiPPO par une plus simple. Nous rédigeons cette section afin de rendre compte de l’ensemble des travaux effectués sur la matrice HiPPO avant son abandon.   
  
Le 4 mars 2022, les auteurs du S4 ont actualisé leur papier afin d’y intégrer une section sur l’importance de la matrice HiPPO. Il s’agit de la section 4.4 dans la version la plus récente du papier.  
Pour résumer, elle consiste à rapporter les résultats observés à la suite de la réalisation d’ablations sur le jeu de données séquentielles CIFAR-10. Au lieu d'utiliser un SSM avec la matrice HiPPO, les auteurs ont essayé d'utiliser diverses paramétrisations comme une matrice dense aléatoire et une matrice diagonale aléatoire. 

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Hippo.png) |
|:--:|
| Figure 2 : *Accuracy sur l'échantillon de validation de CIFAR-10, tirée de la figure 3 du papier du S4* |

<br>
HiPPO est donc importante, mais est-ce que ces performances sont du spécifiquement à cette matrice ou bien est-ce que n’importe quelle matrice normale à faible rang (NPLR pour normal plus low-rank) suffirait ?

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/NPLR.png) |
|:--:|
| Figure 3 : *Accuracy sur l'échantillon de validation de CIFAR-10 avec différentes initialisations et parameterisations, tirée de la figure 4 du papier du S4* |

<br>
Initialiser une matrice NPLR avec HiPPO augmente considérablement les performances.
Ainsi, d’après ces expériences, la matrice HiPPO est primordiale afin d’obtenir un modèle performant.

Les auteurs du S4 ont approfondi leurs travaux et le 24 juin 2022, ont publié l’article [*How to Train Your HiPPO*](https://arxiv.org/abs/2206.12037). Il s’agit d’un papier extrêmement détaillé de plus de 39 pages.  
Pour résumer, dans ce travail, les auteurs se concentrent sur une interprétation plus intuitive des SSM en tant que modèle convolutif où le noyau de convolution est une combinaison linéaire de fonctions de base particulières, ce qui conduit à plusieurs généralisations et à de nouvelles méthodes.  
Ainsi, ils prouvent que la matrice A du S4 produit des polynômes de Legendre à échelle exponentielle (LegS). Cela confère au système une meilleure capacité à modéliser les dépendances à long terme via des noyaux très lisses.  
Les auteurs dérivent également un nouveau SSM qui produit des approximations de fonctions de Fourier tronquées (FouT). Cette méthode généralise les [transformées de Fourier à court terme]( https://fr.wikipedia.org/wiki/Transform%C3%A9e_de_Fourier_%C3%A0_court_terme) et les convolutions locales (c'est-à-dire un ConvNet standard), et peut également coder des fonctions de pointe pour résoudre des tâches de mémorisation classiques.  
A noter que c’est surtout HiPPO-FouT qui est introduit dans ce papier. HiPPO-LegS ayant été introduit dans le papier original d’HiPPO deux ans plus tôt. De même qu’HiPPO-LegT (polynômes de Legendre tronqués).  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master//assets/images/SSM_2022/Hippo_LegS_LegT_FouT.png) |
|:--:|
| Figure 4 : *Les différentes variantes d'Hippo* |

<br>
Les couleurs représentent les 4 premières fonctions de base $$K_n(t)$$ (le noyau de convolution) pour chacune des méthodes (nous invitons le lecteur à regarder le tableau 1 du papier pour savoir à quoi équivaut $$K_n(t)$$ dans chacune des méthodes).

De plus, les auteurs travaillent également sur le pas de temps $$∆$$, qui indépendamment d'une notion de discrétisation, peut être interprété simplement comme contrôlant la longueur des dépendances ou la largeur des noyaux du SSM. Les auteurs détaillent aussi comment choisir une bonne valeur de $$∆$$ pour une tâche donnée.

Les travaux menés permettent d’améliorer les résultats du S4 sur le benchmark LRA de plus de 6 points :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4v2_results.png) |
|:--:|
| Figure 5 : *Résultats du S4 v2 sur le benchmark* |

<br>
Le modèle résultant de ce papier est généralement appelé « S4 V2 » ou « S4 updated » dans la littérature à opposer au « S4 original » ou « S4 V1 ».

<br><br>

### <span style="color: #51C353"> Le DSS : *Diagonal State Spaces* </span>

Le 27 mars 2022, Ankit GUPTA introduit dans son papier [*Diagonal State Spaces are as Effective as Structured State Spaces*](https://arxiv.org/abs/2203.14343) les *Diagonal State Spaces* (DSS).  
Il semble que suite à ce papier, Albert GU et lui se sont mis à travailler ensemble d’une part sur une version actualisée de ce papier (GU apparaissant par la suite comme co-auteur dans les v2 et v3 de l’article) et d’autre part dans le cadre du S4D (voir section suivante).  
La principale chose à retenir est que cette approche est sensiblement plus simple que le S4. En effet, le DSS repose sur des matrices d'état diagonales (donc sans la correction de rang faible du S4, i.e. sans la matrice HiPPO) qui, si sont initialisées de manière appropriée, fonctionne mieux que le S4 original. L’usage d’une matrice diagonale à la place de la matrice HiPPO pour A est depuis devenu une norme.  

Arrêtons-nous néanmoins sur les quelques complexités/limites que contiennent ce papier. En effet, c’est en les listant que nous pourrons comprendre les apports des méthodes suivantes qui visent à simplifier davantage les choses.

**1. La discrétisation**  
Le DSS utilise le même système d’équations différentielles que le S4 : 

$$
  \begin{aligned}
    x' &= \mathbf{A}x + \mathbf{B}u \\
    y &= \mathbf{C}x
  \end{aligned}
$$

Cependant il utilise une discrétisation différente afin d’aboutir aux vues convolutives et récurrentes, à savoir la discrétisation [*zero-order hold*](https://en.wikipedia.org/wiki/Zero-order_hold) (ZOH) ou bloqueur d'ordre zéro en français, au lieu de la discrétisation bilinéaire, qui suppose que le signal que nous échantillonnons est constant entre chaque point d'échantillonnage.  
Ci-dessous un tableau comparatif des valeurs de $$A$$, $$B$$ et $$C$$ pour chacunes des deux discrétisations dans la vue récurrente, ainsi que l’expression du noyau de convolution dans la vue convolutive :

| Discrétisation | Bilinéaire | ZOH                    |
| -------------- | ---------- | ----------------------- |
| Récurrence     | $$\mathbf{\bar{A}} = (\mathbf {I} - \frac{\Delta}{2} \mathbf{A})^{-1}(\mathbf {I} + \frac{\Delta}{2} \mathbf{A})$$ <br> $$\mathbf {\bar{B}} = (\mathbf{I} - \frac{\Delta}{2}  \mathbf{A})^{-1} \Delta \mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$   | $$\mathbf{\bar{A}} = e^{\mathbf{A}\Delta}$$ <br> $$\mathbf{\bar{B}} = (\mathbf{\bar{A}} - I)\mathbf{A}^{-1}\mathbf{B}$$ <br>  $$\mathbf{\bar{C}} = \mathbf{C}$$  |
| Convolution    | $$\mathbf{\bar{K}}_k = (\mathbf{\bar{C}}  \mathbf{\bar{B}}, \mathbf{\bar{C}} \mathbf{\bar{A}}  \mathbf{\bar{B}}, …, \mathbf{\bar{C}}  \mathbf{\bar{A}}^{k} \mathbf{\bar{B}})$$       | $$\mathbf{\bar{K}} = (\ \mathbf{C} e^{\mathbf{A}\cdot k\Delta} (e^{\mathbf{A}\Delta} - I)\mathbf{A}^{-1}\mathbf{B}\ )_{0 \leq k < L}$$ |

<br>

Pour la ZOH, après avoir déroulé les calculs, on obtient en fin de compte $$y_k = \sum_{j=0}^k \bar{C}\bar{A}^j\bar{B}\cdot u_{k-j} = \sum_{j=0}^k \bar{K}_j\cdot u_{k-j}$$.

Calculer $$y$$ à partir de $$u$$ et $$\bar{K}$$ s’effectue alors par Transformation de Fourier rapide en $$O(L~log(L))$$ avec $$L$$ la longueur de la séquence en calculant simultanément la multiplication de deux polynômes de degrés $$L-1$$.
<br>

**2. DSSsoftmax et DSSexp**  

<u>Version courte</u>  

GUPTA formule une proposition pour obtenir des DSS qui soient aussi expressifs que le S4, ce qui en pratique abouti à la formulation de deux DSS différents : le DSSexp et le DSSsoftmax. Les informations à retenir les concernant peuvent se résumer au tableau suivant :

| Approche        | DSSexp          | DSSsoftmax      |
| --------------- | --------------- | --------------- |
| Vue convolutive | $$K = \bar{K}_{\Delta, L}(\Lambda,\mathbb{I}_{1 \leq i \leq N},\ \widetilde{w})\\ = \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$ | $$K = \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\\ = w \cdot \Lambda^{-1} \cdot \text{row-softmax}(P)$$  |
| Vue récurrente  | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$ | $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ <br> $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$ |
| Interprétation  | Agit comme la porte d'oubli d'une LSTM  | Si $$\Re(\lambda)<<0$$ : conserve l'information locale,<br> si $$\Re(\lambda)>>0$$ : peut capturer des informations à très longues distances                                                                                        |

Nous travaillons donc ici sur $$ℂ$$ et non pas $$ℝ$$.  

<br>

<u>Version longue</u>  

GUPTA formule la proposition suivante pour obtenir des DSS qui sont aussi expressifs que le S4 :

Let $$K \in \mathbb{R}^{1\times L}$$ be the kernel of length $$L$$ of a given state space $$(A, B, C)$$ and sample time $$\Delta > 0$$, where $$A \in \mathbb{C}^{N \times N}$$ is diagonalizable over $$\mathbb{C}$$ with eigenvalues $$\lambda_1,\ldots,\lambda_N$$ and $$\forall i$$, $$\lambda_i \neq 0$$ and $$e^{L\lambda_i\Delta} \neq 1$$. Let $$P \in \mathbb{C}^{N \times L}$$ be $$P_{i,k} = \lambda_i k\Delta$$ and $$\Lambda$$ be the diagonal matrix with $$\lambda_1,\ldots,\lambda_N$$. Then there exist $$\widetilde{w}, w \in \mathbb{C}^{1\times N}$$ such that

- (a) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ (1)_{1 \leq i \leq N},\ \widetilde{w})\ \ =\ \ \widetilde{w} \cdot \Lambda^{-1} (e^{\Lambda\Delta} - I) \cdot \text{elementwise-exp}(P)$$
- (b) : $$K\ \ =\ \ \bar{K}_{\Delta, L}(\Lambda,\ ((e^{L\lambda_i\Delta} - 1)^{-1})_{1\leq i \leq N},\ w)\ \ =\ \ w \cdot \Lambda^{-1} \cdot  \text{row-softmax}(P)$$

(a) suggère que nous pouvons paramétrer les espaces d'état via $$\Lambda, \widetilde{w} \in \mathbb{C}^N$$ et calculer le noyau comme indiqué. Malheureusement, dans la pratique, la partie réelle des éléments de $$Λ$$ peut devenir positive pendant l'apprentissage, ce qui rend l'apprentissage instable pour les entrées longues. Pour résoudre ça, les auteurs proposent deux méthodes : DSSexp et DSSsoftmax.

2.1 Convolution  
Dans DSSexp, les parties réelles de $$Λ$$ doivent être négatives. On a alors $$\Lambda = - \text{elementwise-exp}(\Lambda_\mathrm{re}) + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$. $$K$$ se calcule alors comme dans la formule indiquée dans la partie (a) de la proposition.  
Dans DSSsoftmax, on normalise chaque ligne de $$Λ$$ par la somme de ces éléments. On a alors $$\Lambda = \Lambda_\mathrm{re} + i\cdot \Lambda_\mathrm{im}$$ et $$\Delta = \mathrm{exp}(\Delta_{\log}) \in \mathbb{R}_{> 0}$$.  
$$K$$ se calcule alors comme dans la formule indiquée dans la partie (b) de la proposition.  
A noter que softmax sur $$\mathbb{C}$$ pouvant par exemple ne pas être défini lors de sofmax $$(0, i \pi)$$, les auteurs utilisent une version corrigée du softmax (détaillée dans l’annexe A.2. du papier) pour prévenir ce problème.

2.2 Récurrence  
Dans DSSexp, en utilisant la formule de la récurrence dans le tableau ci-dessus, on obtient $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left(\lambda_i^{-1} (e^{\lambda_i\Delta} - 1) \right)_{1\leq i \leq N}$$, où dans les deux égalités, $$\lambda_i$$ est la ième valeur propre de Lambda.  
Etant donné que $$\bar{A}$$ est diagonale, il est possible de calculer les $$x_k$$ indépendamment de la manière suivante :  $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + \lambda_i^{-1} (e^{\lambda_i\Delta} - 1)u_k$$.  
Et il est alors possible de déduire que, si $$|\lambda_i|\Delta \approx 0$$, nous avons $$x_{i,k} \approx x_{i,k-1}$$ et nous copions donc l'histoire sur de nombreux pas de temps. En revanche, si $$\mathrm{Re}(\lambda_i)\Delta \ll 0$$, alors $$_{i,k} \approx -\lambda_i^{-1}u_k$$ et donc l'information des pas de temps précédents est oubliée, similaire à une porte « forget » dans les LSTMs.

Dans DSSsoftmax, en utilisant la formule de la récurrence dans le tableau ci-dessus, on obtient : $$\bar{A} = \mathrm{diag}(e^{\lambda_1\Delta}, \ldots, e^{\lambda_N\Delta})$$ et $$\bar{B} = \left( {e^{\lambda_i\Delta} - 1 \over \lambda_i (e^{\lambda_i\Delta L} - 1)} \right)_{1\leq i \leq N}$$.  
Ce qui fait que $$x_{i,k} = e^{\lambda_i\Delta} x_{i,k-1} + {u_k(e^{\lambda_i\Delta} - 1) \over \lambda_i (e^{\lambda_i\Delta L} - 1)}$$.  
A noter que $$e^{\lambda_i\Delta}$$ peut être instable, et on calcule alors deux cas différents en fonction du signe de $$\mathrm{Re}(\lambda)$$ en introduisant un état intermédiaire $$\widetilde{x}_{k}$$.  
-	Si $$\mathrm{Re}(\lambda) \leq 0$$ : $$\widetilde{x}_{k} = e^{\lambda\Delta}\cdot \widetilde{x}_{k-1} + u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {(e^{\lambda\Delta} - 1) \over \lambda (e^{\lambda\Delta L} - 1) }$$

Et notamment si $$\mathrm{Re}(\lambda) \ll 0$$ alors $$\widetilde{x}_k \approx u_k$$ et $$x_k \approx u_k / \lambda$$, ce qui fait qu’on se focalise sur une information locale (les pas précédents sont ignorés)

-	Si $$\mathrm{Re}(\lambda) > 0$$ : 
$$\widetilde{x}_{k} = \widetilde{x}_{k-1} + e^{-k\lambda\Delta} \cdot u_k  \ \ \ \ ,\ \ \ \ x_k = \widetilde{x}_k \cdot {e^{\lambda\Delta (k-(L-1))} \over \lambda}\cdot {e^{-\lambda\Delta}-1 \over e^{-\lambda\Delta L} - 1 }$$

Et notamment si $$\mathrm{Re}(\lambda) \gg 0$$ alors $$\widetilde{x}_0 \approx u_0$$ et $$\widetilde{x}_k \approx \widetilde{x}_{k-1} \approx u_0$$, $$x_{k < L-1} \approx 0$$ et $$x_{L-1} \approx u_0 / \lambda$$, ce qui fait que le modèle peut capturer des informations à très longues distances.
En pratique, les auteurs du S4D indiquant que $$\mathrm{Re}(\lambda) \gg 0$$ ne fonctionne pas si $$L$$ est très grande (= ça explose quand $$t \rightarrow \infty$$ dans $$K(t) = C \exp(A^\intercal).B)$$.  

**3. Initialisation**  
Les parties réelles et imaginaires de w sont initialisées à partir de $$\mathcal{N}(0,1)$$, les éléments de $$\Delta_{\log}$$ à partir de $$\exp(\mathcal{U}~(log(0.001), log(0.1)))$$, et $$\Delta$$ via les valeurs propres de la matrice HiPPO. Les auteurs se demandent s’il ne serait pas possible de trouver une initialisation plus simple pour $$\Delta$$. Ils notent néanmoins qu’une aléatoire conduit à de moins bons résultats.  

Concernant les résultats, DSS a été testé sur LRA et Speech Commands :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results.png) |
|:--:|
| Figure 6 : *Résultats du DSS sur LRA* |

<br>
Le DSS (en version softmax ou en version exp) obtient de meilleurs résultats moyens que ce du S4 original sur ce benchmark. Le DSSsoftmax semble performer légèrement mieux que le DSSexp. Un intérêt également de ce papier est qu’il est le premier à reproduire les résultats du S4 et donc confirmer que les SSM passent ce benchmark.


| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/DSS_results2.png) |
|:--:|
| Figure 7 : *Résultats du DSS sur Speech Commands* |

<br>
Sur Speech Commands, le S4 garde l’avantage sur les DSS.

<br><br>

### <span style="color: #51C353"> **Le S4D : le S4 diagonal** </span>
Le 23 juin 2022, GU, GUPTA et al. introduisent le S4D dans leur article [*On the Parameterization and Initialization of Diagonal State Space Models*](https://arxiv.org/abs/2206.11893).  
L'initialisation de la matrice d'état $$A$$ du DSS repose sur une approximation particulière de la matrice HiPPO du S4. Si la matrice du S4 a une interprétation mathématique pour traiter les dépendances à longue portée, l'efficacité de l'approximation diagonale reste théoriquement inexpliquée.  
Avec le S4D, les auteurs introduisent un SSM diagonal combinant meilleur du calcul et de la paramétrisation de S4 et de l'initialisation de DSS, ce qui donne une méthode extrêmement simple, théoriquement fondée et empiriquement efficace.  

Une comparaison des trois méthodes est donnée par le tableau 1 du papier :  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4D_table1.png) |
|:--:|
| Figure 8 : *Comparaison du S4, du DSS et du S4D* |

<br>
Le S4D peut utiliser la discrétisation bilinéaire du S4 ou bien la discrétisation ZOH du DSS.  

Dans le S4D, le noyau de convolution $$K$$ de l’équation 
$$y = u \ast \mathbf{\overline{K}}$$ où $$\mathbf{\overline{K}} = (\mathbf{C}\mathbf{\overline{B}}, \mathbf{C}\mathbf{\overline{A}}\mathbf{\overline{B}}, \dots, \mathbf{C}\mathbf{\overline{A}}^{L-1}\mathbf{\overline{B}})$$, se calcule de la façon suivante : 
$$\mathbf{\overline{K}}_\ell = \sum_{n = 0}^{N-1} \mathbf{C}_n \mathbf{\overline{A}}_n^\ell \mathbf{\overline{B}}_n \implies \mathbf{\overline{K}} = (\mathbf{\overline{B}}^\top \circ \mathbf{C}) \cdot \mathcal{V}_L(\mathbf{\overline{A}})$$  où :  
• $$\circ$$ représente le [produit matriciel d’Hadamard]( https://fr.wikipedia.org/wiki/Produit_matriciel_de_Hadamard),  
• $$\cdot$$ un produit matriciel classique,  
• $$\mathcal{V}_L$$ est la [matrice de Vandermonde]( https://fr.wikipedia.org/wiki/Matrice_de_Vandermonde) c’est-à-dire : $$\mathcal{V} = 
\begin{bmatrix}
1&\alpha _{1}&{\alpha _{1}}^{2}&\dots &{\alpha _{1}}^{n-1}\\
1&\alpha _{2}&{\alpha _{2}}^{2}&\dots &{\alpha _{2}}^{n-1}\\
1&\alpha _{3}&{\alpha _{3}}^{2}&\dots &{\alpha _{3}}^{n-1}\\
vdots &\vdots &\vdots & \vdots \\
1&\alpha _{m}&{\alpha _{m}}^{2}&\dots &{\alpha _{m}}^{n-1}
\end{bmatrix}$$.

Autrement dit, pour tous $$i$$ et $$j$$, le coefficient en ligne $$i$$ et colonne $$j$$ est $$\displaystyle V_{i,j}={\alpha _{i}}^{j-1}$$.  

Au final, dans le S4D, 

$$
  \mathbf{\overline{K}} =
  \begin{bmatrix}
    \mathbf{\overline{B}}_0 \mathbf{C}_0 & \dots & \mathbf{\overline{B}}_{N-1} \mathbf{C}_{N-1}
  \end{bmatrix}
  \begin{bmatrix}
    1      & \mathbf{\overline{A}}_0     & \mathbf{\overline{A}}_0^2     & \dots  & \mathbf{\overline{A}}_0^{L-1}     \\
    1      & \mathbf{\overline{A}}_1     & \mathbf{\overline{A}}_1^2     & \dots  & \mathbf{\overline{A}}_1^{L-1}     \\
    \vdots & \vdots                 & \vdots                    & \ddots & \vdots                        \\
    1      & \mathbf{\overline{A}}_{N-1} & \mathbf{\overline{A}}_{N-1}^2 & \dots  & \mathbf{\overline{A}}_{N-1}^{L-1} \\
  \end{bmatrix}
\qquad \text{où } \mathcal{V}_L(\mathbf{\overline{A}})_{n, \ell} = \mathbf{\overline{A}}_n^\ell
$$.  
Le tout est calculable en $$\widetilde{O}(N+L)$$ opérations et espace.  

La paramétrisation des différentes matrices est la suivante :
-	$$\mathbf{A} = -\exp(\Re(\mathbf{A})) + i \cdot \Im(\mathbf{A})$$.  
Les auteurs indiquent qu’il est possible de remplacer l’exponentielle par n’importe quelles fonctions positives.
-	$$\mathbf{B} = 1$$ puis est entraîné
-	$$\mathbf{C}$$ aléatoire avec un écart-type de 1 puis entraîné.
  
Notons que le S4 prend en compte des réels alors que S4D des complexes en paramétrant avec une taille d’état de $$N/2$$ et en ajoutant implicitement les paires conjuguées aux paramètres. On a alors l’équivalent de $$N$$ paramètres réels assurant que la sortie est réelle.  

Concernant l’initialisation, les auteurs en introduisent deux :    
• **S4D-Inv** qui est une approximation de S4-LegS : $$\quad \mathbf{A}_n = -\frac{1}{2} + i \frac{N}{\pi} \left( \frac{N}{2n+1}-1 \right)$$  
• **S4D-Lin** qui est une approximation de S4-FouT : $$\quad \mathbf{A}_n = -\frac{1}{2}\mathbf{1} + i \pi n$$

Nous invitons le lecteur à consulter la partie 4 du papier pour plus de détails sur d’où sortent ces équations.  
D’un point de vue interprétabilité, la partie réelle de $$\mathbf{A}_n$$ contrôle le taux de décroissance des poids. La partie imaginaire de $$\mathbf{A}_n$$ contrôle quant à elle les fréquences d’oscillations de la fonction de base $$K_n(t) = e^{t\mathbf{A}}\mathbf{B}$$.  

Enfin les auteurs avancent quelques résultats :  
1)	Calculer le modèle avec une softmax au lieu de Vandermonde ne fait pas de grande différence  
2)	Entraîner B donne toujours de meilleurs résultats  
3)	Pas de différences notables entres les deux discrétisations possibles  
4)	Restreindre la partie réelle de A donne de meilleurs résultats (très légers cependant)  
5)	Toutes les modifications testées pour l’initialisation ont dégradé les résultats. A savoir appliquer un coefficient sur la partie imaginaire ou utiliser une partie imaginaire aléatoire, ou utiliser une partie réelle aléatoire, ou utiliser une partie imaginaire et une partie réelle aléatoire.  

Cette méthode étant très facile à implémenter par rapport aux autres (Vandermade se limitant à deux lignes de code), le S4D a remplacé le S4 dans les usages (on peut d’ailleurs observer des abus de langage où dans la table 6 du papier du Mamba par exemple, les auteurs utilisent le terme S4 pour désigner le S4D).
<br><br>


### <span style="color: #51C353"> **Le GSS : *Gated State Space*** </span>

Cinq jours après le S4D, le 27 juin 2022, MEHTA, GUPTA et al. introduisent le GSS dans leur papier [*Long Range Language Modeling via Gated State Spaces*](https://arxiv.org/pdf/2206.13947.pdf).  
Dans ce travail, ils se concentrent sur la modélisation de séquences autorégressives (là où les travaux précédents sur les SSM se concentraient particulièrement sur les tâches de classification de séquences) à partir de livres en anglais, de code source Github et d'articles de mathématiques ArXiv. Ils montrent que leur couche appelée *Gated State Space* (GSS) s'entraîne significativement plus vite que le DSS (2 à 3 fois plus vite). Ils attestent également que l'exploitation de l'auto-attention pour modéliser les dépendances locales améliore encore les performances du GSS. 

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GSS_table1.png) |
|:--:|
| Figure 9 : *Comparaison du DSS vs GSS. Les modèles sont entraînés sur des séquences de longueurs 4K puis évalués sur des séquences pouvant aller jusqu’à 65K tokens.* |

<br>
Partant du constat que les SSM (S4/DSS) s’entraînaient plus lentement que prévu sur TPU, les auteurs ont modifié l'architecture afin de réduire la dimensionnalité d'opérations spécifiques qui se sont révélées être des goulots d'étranglement. Ces modifications s'inspirent d'une observation empirique bien étayée concernant l'efficacité des unités de gating ([*Language Modeling with Gated Convolutional Networks*](https://arxiv.org/abs/1612.08083) de DAUPHIN et al. (2016),  [*GLU Variants Improve Transformer*](https://arxiv.org/abs/2002.05202) de SHAZEER (2020, etc.). Plus précisément, les auteurs s'impirent du papier [*Transformer Quality in Linear Time*](https://arxiv.org/abs/2202.10447) de HUA et al. qui a montré avec leur modèle FLASH que le remplacement de la couche *feed-forward* dans le Transformer par des unités de gating permet d'utiliser une attention unitête plus faible avec une perte de qualité minimale. Ils ont appelé cette composante la *Gated Attention Unit* (GAU).

 | ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GAU.png) |
|:--:|
| Figure 10 : *La Gated Attention Unit.<br>Ce n’est pas exactement la même figure que celle du papier. En effet, j’ai effectué une translation horizontale afin d’avoir l’entrée en bas et non en haut pour faciliter le parallèle avec la figure du Mega visible plus bas.* |

<br>
Les auteurs du GSS ont donc étendu l'utilisation des gating units aux SSM et observent alors une réduction de la dimensionnalité lors de l'exécution d'opérations FFT. 

 | ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GSS.png) |
|:--:|
| Figure 11 : *Adaptation de la GAU aux SSM.* |

<br>
A noter que, contrairement à HUA et al., les auteurs n’observent pas beaucoup d'avantages à utiliser les activations RELU² ou Swish au lieu de la GELU c’est pourquoi ils la conservent.  
De plus, le DSS utilisé utilise un pas de temps ∆ fixé à 1 (les auteurs observant que cela permet de réduire le temps de calcul nécessaire à la création des noyaux et de simplifier leur calcul).  
Un point particulièrement intéressant est que contrairement aux observations faites dans le S4 et le DSS, la performance du modèle sur les tâches de modélisation du langage s’est retrouvée beaucoup moins sensible à l'initialisation permettant alors d’entraîner le modèle avec succès en initialisant les variables de l'espace d'état de manière aléatoire. Cela constitut un résultat très important puisqu’il montre que ni la matrice Hippo (S4), ni l’initialisation Hippo (DSS) n’est nécessaire.  

Concernant l’hybride GSS-Transformer, il consiste simplement à intercaler avec parcimonie des blocs Transformer traditionnels avec des couches GSS. Le modèle hybride obtient une perplexité plus faible que le modèle purement SSM :

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/GSS_table2.png) |
|:--:|
| Figure 12 : *Performances du modèle hybride GSS-Transformer.* |

<br><br>

### <span style="color: #51C353"> **Liquid-S4** </span>

<br><br>

### <span style="color: #51C353"> **S5** </span>

<br><br>

### <span style="color: #51C353"> **Mega** </span>

Le 21 septembre 2022, MA, ZHOU et al., ont publié le [*Mega: Moving Average Equipped Gated Attention*](https://arxiv.org/abs/2209.10655).  
Le Mega est un transformer avec un mécanisme d'attention à une seule tête, utilisant le système de portes du GAU, et est équipé d'une moyenne mobile exponentielle (EMA) amortie pour incorporer le biais inductif positionnel.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega.png) |
|:--:|
| Figure X : *Vue d’ensemble du Mega, figure conçue à partir de ma compréhension du papier* |

<br>
Les auteurs proposent aussi une variante, Mega-chunk, qui divise efficacement l'ensemble de la séquence en plusieurs morceaux de longueur fixe. Cela offre une complexité linéaire en termes de temps et d'espace tout en n'entraînant qu'une perte de qualité minimale.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Mega-chunk.png) |
|:--:|
| Figure X : *Le Mega chunk* |

<br>
Cela offre une complexité linéaire appliquant simplement l'attention localement à chaque morceau de longueur fixe.  
Plus précisément, on divise les séquences de requêtes, de clés et de valeurs dans en morceaux de longueur $$c$$. Par exemple, $$\mathbf{Q} = {\mathbf{Q}_1, ... , \mathbf{Q}_k}$$, où $$k = -\frac{n}{c}$$ est le nombre de morceaux. L'opération d'attention est appliquée individuellement à chaque bloc, ce qui donne une complexité linéaire $$\mathcal{O}(kc^2) = \mathcal{O}(nc)$$ par rapport à $$n$$.  
Cette méthode souffre d'une limitation critique, à savoir la perte d'informations contextuelles provenant d'autres blocs. Mais la sous-couche EMA atténue ce problème en capturant les informations contextuelles locales à proximité de chaque token, dont les résultats sont utilisés comme entrées dans la sous-couche d'attention. Ainsi, le contexte effectif exploité par l'attention au niveau du bloc peut aller au-delà de la limite du bloc.  

Le Mega est extrêmement compétitif puisqu’il devient alors le meilleur modèle sur le LRA :  
 
| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/LRA_Mega.png) |
|:--:|
| Figure X : *Les résultats du Mega sur le benchmark LRA* |

<br>
Que fait donc un transformer dans un article de blog sur les SSM ? Intéressons-nous à l’EMA amortie pour comprendre le lien entre le Mega et le S4D.  

• <u>Rappel sur l’EMA « classique »</u> :  
L’équation d’une [EMA « classique »](https://fr.wikipedia.org/wiki/Moyenne_mobile#Moyenne_mobile_exponentielle)  est $$𝐲_t = 𝜶 ⊙ 𝐱_t + (1−𝜶) ⊙ 𝐲_{t−1}$$ avec $$𝜶$$ in $$[0,1]^d$$ le coefficient de l’EMA représentant le degré de diminution de la pondération et ⊙ le produit matriciel de Hadamard.  
Un 𝜶 plus élevé décote plus rapidement les observations le plus anciennes.  
On impose donc ici un biais inductif : le poids de la dépendance entre deux tokens diminue de manière exponentielle au fil du temps avec un facteur 𝜶 agnostique à l’entrée. Cette propriété favorise les dépendances locales et limite les dépendances à long terme.  
Le calcul de l’EMA peut être représenté comme n convolutions individuelles pouvant être calculées efficacement par FFT.  

• <u>EMA utilisée dans le Mega</u> :  
Le Mega utilise une EMA « amortie » multidimensionnelle. C’est-à-dire que dans l’équation de l’EMA « amortie », $$𝐲_t = 𝜶 ⊙ 𝐱_t + (1−𝜶 ⊙ 𝜹) ⊙ 𝐲_{t−1}$$ où un paramètre $$𝜹$$ in $$[0,1]^d$$ est introduit qui représente le facteur d’amortissement, $$x$$ est étendue à $$h$$ dimensions via une matrice d’expansion $$𝜷$$ in $$R^{d \times h}$$.  
L’équation devient alors $$𝐲_{t,j} = 𝜼_j^{\intercal} 𝐡_t^{(j)}$$ avec $$𝐡_t^{(j)} = 𝜶_j ⊙ 𝐮_t^{(j)}+ (1−𝜶_j ⊙ 𝜹_j) ⊙ 𝐡_{t−1}^{(j)}$$ et $$𝜼 \in R^{d \times h}$$ est la matrice de projection qui renvoie l’état caché en $$h$$ dimension à la sortie unidimensionnelle $$𝐲_{t,j} \in \mathbb{R}$$.  

Preuve que l’ EMA « amortie » multidimensionnelle peut être calculée comme une convolution et donc par FTT (en fixant $$d = 1$$ pour $$𝜶$$ et $$𝜹$$) :  
On a $$𝐲_t = 𝜼^{\intercal} 𝐡_t$$ avec $$𝐡_t = 𝜶_j ⊙ 𝐮_t + (1−𝜶 ⊙ 𝜹) ⊙ 𝐡_{t−1}$$. Notons $$ϕ = 1−𝜶 ⊙ 𝜹$$.  
Alors :  $$𝐡_t = 𝜶 ⊙ 𝐮_t + (1−𝜶 ⊙ 𝜹) ⊙ 𝐡_{t−1} = 𝜶 ⊙ 𝜷 𝐱_t + ϕ ⊙ 𝐡_{t−1}$$  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; et $$𝐲_t = 𝜼^{\intercal} 𝐡_t = 𝜼^{\intercal} (𝜶 ⊙ 𝜷 𝐱_t + ϕ ⊙ 𝐡_{t−1})$$    
Ensuite, en déroulant les deux équations ci-dessus, on obtient explicitement :  
Etape 0 : $$𝐡_1 = 𝜶 ⊙ 𝜷𝐱_1 + ϕ ⊙ 𝐡_0$$  
Etape 1 : $$𝐡_2 = 𝜶 ⊙ 𝜷𝐱_2 + ϕ ⊙ 𝐡_1$$
$$= 𝜶 ⊙ 𝜷 𝐱_2 + ϕ ⊙ (ϕ ⊙ 𝐡_0 + 𝜶 ⊙ 𝜷 𝐱_1) = 𝜶 ⊙ 𝜷 𝐱_2 + ϕ^2 ⊙ 𝐡_0 + ϕ ⊙ 𝜶 ⊙ 𝜷 𝐱_1$$  
…  

Et de même :  
Etape 0 : $$𝐲_1 = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_1 + ϕ ⊙𝐡0) = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_1 + 𝜼^{\intercal} ϕ ⊙ 𝐡_0$$    
Etape 1 : $$𝐲_2 = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_2 + 𝜼^{\intercal} ϕ ⊙ 𝐡_1$$ 
$$= 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_2 + 𝜼^{\intercal} ϕ ⊙ (𝜶 ⊙ 𝜷 𝐱_1 + ϕ ⊙ 𝐡_0) = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_2 + 𝜼^{\intercal} ϕ ⊙ 𝜶 ⊙ 𝜷 𝐱_1 + 𝜼^{\intercal} ϕ^2 ⊙ 𝐡_0$$  
…  
Etape $$t$$ :  $$𝐲_t = 𝜼^{\intercal} 𝜶 ⊙ 𝜷 𝐱_t + … + 𝜼^{\intercal} ϕ_{t−1} ⊙ 𝜶 ⊙ 𝜷 𝐱_{t−1} + 𝜼^{\intercal} ϕ^t ⊙ 𝐡_0$$.  

Et donc $$𝐲 = 𝒦 * 𝐱 + 𝜼^{\intercal} ϕ^t ⊙ 𝐡_0$$ avec $$𝒦 = (𝜼^{\intercal} (𝜶 ⊙ 𝜷), 𝜼^{\intercal} (ϕ ⊙ 𝜶 ⊙ 𝜷), …, 𝜼^{\intercal}(ϕ^t ⊙ 𝜶 ⊙ 𝜷) \in \mathbb{R}^n$$.  
$$𝒦$$ est calculé dans le Mega via le produit de Vandermonde, ce qui nous rappelle la méthode utilisée dans le S4D.    
Pour plus de détails sur les liens entre le Mega et le S4, le lecteur est invité à consulter les messages échangés entre Albert Gu et les auteurs du Mega trouvables en commentaires de la soumission du Mega sur [Open Review](https://openreview.net/forum?id=qNLe3iq2El). Mais en résumé, en établissant un lien entre l’étape de discrétisation des SSM et l’EMA amortie, il est possible de voir le Mega vue comme un hybride SSM/Attention simplifiant le S4 pour qu'il soit à valeur réelle plutôt que complexe. 

<br><br>

### <span style="color: #51C353"> **SGConv** </span>

<br><br>

## <span style="color: #FFBF00"> **Applications des SSM** </span>

### <span style="color: #51C353"> **SaShiMi** </span>

Dans [*It's Raw! Audio Generation with State-Space Models*](https://arxiv.org/abs/2202.09729) paru le 20 février 2022, GOEL, GU et al. appliquent le S4 à de la génération d’audio de manière causale.  
Contrairement aux méthodes reposant sur le conditionnement à partir de textes, de spectrogrammes, etc., il s'agit d'une méthode opérant directement sur le signal d’entré permettant de se comparer notamment au modèle WaveNet.  
SaShiMi peut s'entraîner directement sur des séquences de plus de 100K (8s audio) sur un seul GPU V100, comparé aux limitations de longueur de contexte auxquelles font face des modèles comme WaveNet. Il utilise efficacement ce long contexte pour améliorer l'estimation de la densité.  
Les auteurs ont comparé leur modèle sur divers benchmarks portant notamment sur de la génération de musique de piano ou encore de la parole (énonciation de chiffres). 
Il est possible de consulter les audios générés [ici](https://hazyresearch.stanford.edu/sashimi-examples/).  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/Sashimi.png) |
|:--:|
| Figure X : *Vue d'ensemble de l'architecture de Sashimi* |
<br><br>

### <span style="color: #51C353"> **ViS4mer** </span>

ISLAM et BERTASIUS introduisent le 4 avril 2022 le ViS4mer dans leur [*Long Movie Clip Classification with State-Space Video Models*](https://arxiv.org/abs/2204.01692).  
Il s'agit d'un hybride entre un S4 et un Transformer afin de réaliser de la classification de (longues) vidéos. Plus précisément, le modèle utilise un encodeur Transformer standard pour l'extraction des caractéristiques spatiotemporelles à courte distance, et un décodeur S4 temporel multi-échelle pour le raisonnement temporel à longue distance. Le modèle alors obtenu apparait comme étant 2,6 fois plus rapide et 8 fois plus efficace en termes de mémoire qu'un Transformer.  
Il s’agit à ma connaissance du premier papier à avoir mis en avant l’intérêt d’hybrider des SSM et des Transformers.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/ViS4mer_decodeur.png) |
|:--:|
| Figure X : *Vue d'ensemble du décodeur du ViS4mer*|
<br>

ViS4mer obtient des résultats de pointe dans 6 des 9 tâches de classification de vidéos de longue durée sur le benchmark *Long Video Understanding* (LVU) de XX et al. (XXXX) qui consiste à classer des vidéos ayant une durée de 1 à 3 min. Le modèle semble également avoir de bonnes capacités de généralisation en obtenant des résultats compétitifs sur les jeux de données Breakfast et COIN procedural activity alors qu'il a vu 275 fois moins de données.

<br><br>

### <span style="color: #51C353"> **CCNN** </span>

Le 7 juin 2022, ROMERO, KNIGGE et al. introduisent le CCNN dans leur papier [*Towards a General Purpose CNN for Long Range Dependencies in ND*](https://arxiv.org/abs/2206.03398).  
Ils partent de l'idée que les réseaux de neurones convolutifs sont puissants mais doivent être adaptés spécifiquement à chaque tâche : 
• Longueur de l'entrée : 32x32, 1024x1024 → Comment modéliser les dépendances à longue distance ?  
• Résolution de l'entrée : 8kHz, 16kHz → Dépendances à longue distance, agnosticité de la résolution ?  
• Dimensionnalité de l'entrée : 1D, 2D, 3D → Comment définir les noyaux convolutifs ?  
• Tâche : Classification, Segmentation, ... → Comment définir les stratégies d'échantillonnage haut-bas ?  
Est-il alors possible de concevoir une architecture unique, avec laquelle les tâches peuvent être résolues indépendamment de la dimensionnalité, de la résolution et de la longueur de l'entrée, sans modification de l'architecture ?
Oui et grâce au CCNN qui utilise des noyaux de convolution continus.  

Ils s'inspirent notamment du S4 pour créer une variante de blocs résiduels efficaces qu'ils appellent *bloc S4*. Toutefois, contrairement au S4 qui ne fonctionne qu'avec des signaux 1D, le CCNN modélise facilement des signaux ND.

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/ViS4mer_decodeur.png) |
|:--:|
| Figure X : *Vue d'ensemble du CCNN*|

<br><br>

### <span style="color: #51C353"> **$$\mathbf{SSSD^{S4}}$$** </span>

Le 19 août 2022, LOPEZ ALCARAZ et STRODTHOFF proposent dans leur papier [*Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models*](https://arxiv.org/abs/2208.09399) un hybride entre un S4 et un modèle de diffusion pour la prédiction de données manquantes dans des séries temporelles. Leur modèle est dénommé **$$\mathbf{SSSD^{S4}}$$** (ou plus simplement SSSD).

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/SSSDS4.png) |
|:--:|
| Figure X : *Vue d'ensemble du $$\mathbf{SSSD^{S4}}$$*|
<br><br>

### <span style="color: #51C353"> **S4ND** </span>

Le 12 octobre 2022, NGUYEN, GOEL, GU et al. présentent le [*S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces*](https://arxiv.org/abs/2210.06583).

Ce modèle étend le S4 (qui est 1D) aux signaux continus multidimensionnels tels que les images et les vidéos (là où les ConvNets et ViT apprennent sur des pixels discrets). Pour cela ils transforment l'ODE standard du S4 en une EDP multidimensionnelle :  

$$
\begin{aligned}
  x'(t) &= \bm{A}x(t) + \bm{B}u(t)  \\
  y(t)  &= \bm{C}x(t)
\end{aligned}
$$

devient :  

$$
 \begin{aligned}
      \frac{\partial}{\partial t^{(1)}} x(t^{(1)}, t^{(2)}) &= (\bm{A}^{(1)} x^{(1)}(t^{(1)}, t^{(2)}), x^{(2)}(t^{(1)}, t^{(2)})) + \bm{B}^{(1)} u(t^{(1)}, t^{(2)})
      \\
      \frac{\partial}{\partial t^{(2)}} x(t^{(1)}, t^{(2)}) &= (x^{(1)}(t^{(1)}, t^{(2)}), \bm{A}^{(2)} x^{(2)}(t^{(1)}, t^{(2)})) + \bm{B}^{(2)} u(t^{(1)}, t^{(2)})
      \\
      y(t^{(1)}, t^{(2)}) &= \langle \bm{C}, x(t^{(1)}, t^{(2)}) \rangle
\end{aligned}
$$

avec $$\( \bm{A}^{(\tau)} \in \mathbbm{C}^{N^{(\tau)} \times N^{(\tau)}} \), \( \bm{B}^{(\tau)} \in \mathbbm{C}^{N^{(\tau)} \times 1} \),   \( \bm{C} \in \mathbbm{C}^{N^{(1)} \times N^{(2)}} \)$$, avec comme condition initiale de l'EDP linéaire $$x(0, 0) = 0$$.
  
En fonction du jeu de données testé, les auteurs obtiennent des résultats similaires ou plus performants que ceux d'un ViT ou un ConvNext.  
L'intérêt principal du S4ND étant qu'il peut fonctionner avec différentes résolutions en s'adaptant à différents taux d'échantillonage. Les auteurs mettent en avant cette caractéristiques via deux expériences :  
1) En zéro-shot, S4ND surpasse un Conv2D de plus de 40 points lorsqu'il est entraîné sur des images $$8\times8$$ et testé sur des images $$32\times32$$.
2) Avec un redimensionnement progressif, S4ND peut accélérer l'entraînement de 22% avec une baisse de la précision finale de ∼1% par rapport à l'entraînement à la seule haute résolution.  

| ![image](https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/SSM_2022/S4ND.png) |
|:--:|
| Figure X : *Exemple du S4ND pour des images 2D*|

<br><br>

# <span style="color: #FF0000"> **Conclusion** </span>

<br><br><br>

# <span style="color: #FF0000"> **Pour aller plus loin** </span>
Slides CCNN : https://app.slidebean.com/sbp/gk5j826nq7/CCNN#1
+ vidéos
+ codes
<br><br><br>

# <span style="color: #FF0000"> **Références** </span>
