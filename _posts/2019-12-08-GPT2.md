---
title: "ILLUSTRATION DU GPT2"
categories:
  - NLP
tags:
  - french GPT2 NLP
  - GPT2 expliqué en français NLP
  - GPT2 NLP français
  - Illustration du GPT2 en français NLP
  - GPT2 français
  - GPT2 french
excerpt : "NLP - Illustration du GPT2 de Radford et al."
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/NLP_radom_blog.png"
    teaser : "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-sizes.png"
toc: true
toc_sticky: true
author_profile: false
sidebar:
    nav: sidebar-sample
    
---


# <span style="color: #FF0000"> **Avant-propos** </span>
 
Cet article est une traduction de l’article de Jay Alammar : [The illustrated GPT-2 (Visualizing transformer language model)](https://jalammar.github.io/illustrated-gpt2/).<br>
Merci à lui de m'avoir autorisé à effectuer cette traduction.
<br><br><br>


# <span style="color: #FF0000"> **Introduction** </span>
Cette année (2019), le [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (*Generative Pretrained Transformer 2*) de Radford et al. a fait preuve d’une impressionnante capacité à écrire des essais cohérents et passionnés qui dépassent ce que nous avions prévu que les modèles linguistiques actuels soient capables de produire. Le GPT-2 n’est pas une architecture particulièrement nouvelle (le GTP-2 étant la version 2 du [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) dévoilé en juin 2018 par Radford et al.), elle est très similaire à celle du decoder du Transformer sorti Le GPT-2 est cependant basé sur un Transformer entraîné sur un ensemble de données massif. Dans cet article, nous allons nous intéresser à l’architecture qui a permis au modèle de produire ces résultats.
<br>
Le but ici est également de compléter l’article sur les Transformers, avec plus de visuels expliquant leur fonctionnement interne, et comment ils ont évolué depuis l’article original. 
<br><br><br>


# <span style="color: #FF0000"> **1. GPT-2 et modélisation du langage** </span>
## <span style="color: #FFBF00"> **1.1 Qu’est-ce qu’un modèle linguistique ?** </span>
Dans la partie 4 de l’article sur le [word embedding](https://lbourdois.github.io/blog/nlp/word_embedding/), nous avons examiné ce qu’est un modèle linguistique.
Essentiellement un modèle d’apprentissage automatique qui est capable de regarder une partie d’une phrase et de prédire le mot suivant. Les modèles les plus connus sont les claviers de smartphones qui suggèrent le mot suivant en fonction de ce que vous avez déjà tapé.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/swiftkey-keyboard.png">
</figure>
</center>

En ce sens, nous pouvons dire que le GPT-2 est fondamentalement une fonction de prédiction de mots d’une application clavier, mais qui est beaucoup plus sophistiqué que ce que votre téléphone a. Le GPT-2 a été entraîné sur un énorme ensemble de données de 40 Go appelé WebText que les chercheurs d’OpenAI ont trouvé sur Internet. Pour comparer en termes de taille de stockage, l’application clavier que j’utilise, SwiftKey, occupe 78MBs d’espace. La plus petite variante du GPT-2 occupe 500MBs de stockage pour stocker tous ses paramètres. La plus grande variante de GPT-2 est 13 fois plus grande, ce qui lui permet de prendre plus de 6,5 Go d’espace de stockage.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-sizes.png">
</figure>
</center>

Une excellente façon d’expérimenter le  GPT-2 est d’utiliser [Write with Transformer](https://transformer.huggingface.co/doc/gpt2-large). Il utilise le GPT-2 pour afficher dix prédictions possibles pour le mot suivant (avec leur score de probabilité). 
Vous pouvez sélectionner un mot puis voir la liste suivante des prédictions pour continuer à écrire le passage.
<br><br>


## <span style="color: #FFBF00"> **1.2 Transformers pour la modélisation du langage ?** </span>
Comme nous l’avons vu dans l’article sur le [Transformer](https://lbourdois.github.io/blog/nlp/Transformer/), le modèle de Transformer original est composé d’un encoder et d’un decoder (chacun est une pile de ce que nous pouvons appeler des transformer blocks). Cette architecture est appropriée parce que le modèle s’attaque à la traduction automatique, un problème où les architectures encoder-decoder ont connu du succès dans le passé.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-encoder-decoder.png">
</figure>
</center>

Une grande partie des travaux de recherche apparus depuis a vu l’architecture se débarrasser de l’encoder ou du decoder et n’utiliser qu’une seule pile de transformer blocks. Empilant les piles aussi haut que possible, leur fournissant des quantités massives de texte pour l’entraînement.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt-2-transformer-xl-bert-3.png">
</figure>
</center>

A quelle hauteur peut-on empiler ces blocs ? Il s’avère que c’est l’un des principaux facteurs de distinction entre les différentes tailles de modèles du GPT2 :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-sizes-hyperparameters-3.png">
</figure>
</center>
<br><br>


## <span style="color: #FFBF00"> **1.3 Une différence par rapport à BERT** </span>
Le GPT-2 est construit à l’aide de blocs decoders. BERT, pour sa part, utilise des blocs d’encoders. Nous examinerons la différence dans une section suivante. Mais l’une des principales différences entre les deux est que le GPT-2, comme les modèles de langage traditionnels, produit un seul token à la fois. Invitons par exemple un GPT-2 bien entraîné à réciter la première loi de la robotique :  « A robot may not injure a human being or, through inaction, allow a human being to come to harm ».
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/xlnet/gpt-2-output.gif">
</figure>
</center>

La façon dont fonctionnent réellement ces modèles est qu’après chaque token produit, le token est ajouté à la séquence des entrées. Cette nouvelle séquence devient l’entrée du modèle pour la prochaine étape. Cette idée est appelée « auto-régression » et a permis aux RNN  d’être efficaces.
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif">
</figure>
</center>
Le GPT2 et certains modèles plus récents comme TransformerXL et XLNet sont de nature autorégressive. BERT ne l’est pas. C’est un compromis. En perdant l’auto-régression, BERT a acquis la capacité d’incorporer le contexte des deux côtés d’un mot pour obtenir de meilleurs résultats. XLNet ramène l’autorégression tout en trouvant une autre façon d’intégrer le contexte des deux côtés.
<br><br>


## <span style="color: #FFBF00"> **1.4 L’évolution des Transformer block** </span>
La [publication](https://arxiv.org/abs/1706.03762) initiale sur les Transformers introduisait deux types de blocs :
<br>
### <span style="color: #51C353"> **1.4.1 Le bloc encoder** </span>
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-encoder-block-2.png">
  <figcaption>
Un bloc encoder de l’article d’origine peut recevoir des entrées jusqu’à une certaine longueur maximale (512 tokens). Si une séquence d’entrée est plus courte que cette limite, nous avons simplement à « rembourrer » le reste de la séquence.  </figcaption>
</figure>
</center>
<br>
### <span style="color: #51C353"> **1.4.2 Le bloc decoder** </span>
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-decoder-block-2.png">
</figure>
</center>
Une différence clé dans la couche d’auto-attention est qu’elle masque les futurs tokens – non pas en changeant le mot en [mask] comme BERT, mais en interférant dans le calcul de l’auto-attention en bloquant les informations des tokens qui sont à la droite de la position à calculer.

Par exemple, si nous devons mettre en évidence le chemin de la position #4, nous pouvons voir qu’il est seulement permis de s’occuper du token présent et des précédents :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-decoder-block-self-attention-2.png">
</figure>
</center>

Il est important que la distinction entre l’auto-attention (ce que BERT utilise) et l’auto-attention masquée (ce que le GPT-2 utilise) soit claire. Un bloc d’auto-attention normal permet à une position d’atteindre le sommet des tokens à sa droite. L’auto-attention masquée empêche que cela se produise :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-and-masked-self-attention.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.4.3 Le only bloc decoder** </span>
Suite à l’article original, [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf) a proposé une autre disposition du bloc Transformer capable de faire de la modélisation linguistique. Ce modèle s’est débarrassé de l’encoder pour ne garder que le decoder. Pour cette raison, appelons le modèle « Transformer-Decoder ». Ce premier modèle de langage basé sur les Transformers était constitué d’une pile de six blocs decoders :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-decoder-intro.png">
  <figcaption>
Les blocs decoders sont identiques. J’ai étendu le premier pour que vous puissiez voir que sa couche d’auto-attention est la variante masquée. Notez que le modèle peut maintenant adresser jusqu’à 4.000 tokens contre 512 dans l’original.
</figcaption>
</figure>
</center>
Ces blocs sont très similaires aux blocs decoders d’origine, sauf qu’ils suppriment la deuxième couche d’auto-attention. Le modèle OpenAI GPT-2 utilise uniquement ces blocs decoders.
<br><br>


## <span style="color: #FFBF00"> **1.5 L’intérieur du GPT-2** </span>
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt-2-layers-2.png">
  <figcaption>
Le GPT-2 peut traiter 1024 jetons. Chaque jeton parcourt tous les blocs décodeurs le long de son propre chemin.
</figcaption>
</figure>
</center>

La façon la plus simple d’exécuter un GPT-2 entraîné est de lui permettre de se promener de lui-même (ce qui est techniquement appelé generating unconditional samples).   Nous pouvons aussi le pousser à ce qu’il parle d’un certain sujet (generating interactive conditional samples). Dans le premier cas, nous pouvons simplement lui donner le token de démarrage et lui faire commencer à générer des mots (le modèle utilise *<|endoftext|>* comme token de démarrage. Appelons-le < s > à la place pour simplifier les graphiques).
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif">  
</figure>
</center>

Le token est traité successivement à travers toutes les couches, puis un vecteur est produit le long de ce chemin. Ce vecteur peut être évalué par rapport au vocabulaire du modèle (tous les mots que le modèle connaît, 50 000 mots dans le cas du GTP-2). Dans ce cas, nous avons sélectionné le token ayant la probabilité la  plus élevée :  «the».

Sur votre téléphone, si vous cliquez à chaque fois sur le mot suggéré par votre application clavier, la suggestion peut parfois restée coincée dans des boucles répétitives où la seule issue est de cliquer sur le deuxième ou le troisième mot suggéré. La même chose peut se produire ici.

GPT-2 a un paramètre appelé top-k que nous pouvons utiliser pour que le modèle considère des mots d’échantillonnage autres que le top mot (ce qui est le cas lorsque top-k = 1).

Dans l’étape suivante, nous ajoutons le résultat de la première étape à notre séquence d’entrée, et le modèle fait sa prochaine prédiction :
<center>
<figure class="image">
  <img src="https://jalammar.github.io/images/gpt2/gpt-2-simple-output-3.gif">
</figure>
</center>
Notez que la deuxième étape est la seule qui est active dans ce calcul. Chaque couche du GPT-2 a conservé sa propre interprétation du premier token et l’utilisera dans le traitement du second token (nous y reviendrons plus en détail dans la section suivante sur l’auto-attention). GPT-2 ne réinterprète pas le premier token à la lumière du second.
<br>

### <span style="color: #51C353"> **1.5.1 Input encoding** </span>
Regardons plus en détail pour mieux comprendre le modèle. Commençons par l’entrée.     

Comme dans d’autres modèles de NLP dont nous avons déjà parlé, le GPT-2 recherche l’embedding du mot d’entrée dans son embedding matrix (obtenue après entraînement).
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-token-embeddings-wte-2.png">
  <figcaption>
Chaque ligne est un word embedding c’est à dire une liste de nombres représentant un mot et capturant une partie de sa signification. La taille de cette liste est différente selon la taille du GPT2. Le plus petit modèle utilise une taille d’embedding de 768 par mot/token.
</figcaption>
</figure>
</center>

Ainsi au début nous recherchons l’embedding du token de départ < s > dans la matrice. Avant de transmettre cela au premier bloc du modèle, nous devons incorporer le codage positionnel (un signal qui indique aux blocs l’ordre des mots dans la séquence). Une partie du modèle entraîné contient une matrice ayant un vecteur de codage positionnel pour chacune des 1024 positions de l’entrée.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-positional-encoding.png">
</figure>
</center>

Schématiquement :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-input-embedding-positional-encoding-3.png">
  <figcaption>
Envoyer un mot au premier bloc du Transformer, c’est rechercher son embedding et additionner le vecteur de codage positionnel pour la position #1.</figcaption>
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.5.2 Voyage dans le bloc** </span>
Le premier bloc peut maintenant traiter le premier token en le faisant passer d’abord par le processus d’auto-attention, puis par sa couche feed forward. Une fois le traitement effectué, le bloc envoie le vecteur résultant pour qu’il soit traité par le bloc suivant. Le processus est identique dans chaque bloc mais chaque bloc a des poids qui lui sont propres dans l’auto-attention et dans les sous-couches du réseau neuronal.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-transformer-block-vectors-2.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.5.3 Rappel de l’auto-attention** </span>
Le langage dépend beaucoup du contexte. Par exemple, regardez la deuxième loi de la robotique :

> A robot must obey the orders given **it** by human beings except where **such orders** would conflict with the **First Law**   
 (Un robot doit obéir aux ordres donnés par les êtres humains, sauf si ces ordres entrent en conflit avec la première loi.)

A trois endroits dans la phrase des mots renvoient à d’autres mots. Il n’y a aucun moyen de comprendre ou de traiter ces mots sans incorporer le contexte auquel ils font référence. Lorsqu’un modèle traite cette phrase, il doit pouvoir savoir que :
* **it** se réfère au robot
* **such** orders se réfèrent à la partie antérieure de la loi, à savoir « les ordres donnés par des êtres humains » 
* **First Law** se réfère à l’ensemble de la première loi

C’est ce que réalise l’auto-attention. Pour ce faire, on attribue des notes à la pertinence de chaque mot du segment et additionne leur représentation vectorielle.

A titre d’exemple, la couche d’auto-attention dans le bloc du haut fait attention à « a robot » lorsqu’elle traite le mot « it ». Le vecteur qui est ensuite passer au feed forward neural network est la somme des vecteurs de chacun des trois mots multipliés par leurs scores.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-example-2.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **1.5.4 Le processus de l’auto-attention** </span>
L’auto-attention est traitée le long du parcours de chaque token. Les composantes significatives sont trois vecteurs :
* **Query** : la requête est une représentation du mot courant. Elle est utilisée pour scorer le mot vis-à-vis des autres mots (en utilisant leurs clés). 
* **Key** : les vecteurs clés sont comme des labels pour tous les mots de la séquence. C’est contre eux que nous nous mesurons dans notre recherche de mots pertinents.
* **Value** : les vecteurs de valeurs sont des représentations de mots réels. Une fois que nous avons évalué la pertinence de chaque mot, ce sont les valeurs que nous additionnons pour représenter le mot courant.

Une analogie grossière est de penser à la recherche dans un classeur. La requête (query) est le sujet que vous recherchez. Les clés (key) sont comme les étiquettes des chemises à l’intérieur de l’armoire. Lorsque vous faites correspondre la requête et la clé, nous enlevons le contenu du dossier. Le contenu correspond au vecteur de valeur (value). Sauf que vous ne recherchez pas seulement une valeur, mais un mélange de valeurs à partir d’un mélange de dossiers.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-example-folders-3.png">
</figure>
</center>

Multiplier le vecteur de requête par chaque vecteur clé produit un score pour chaque dossier (techniquement : le produit scalaire suivi de softmax).
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-example-folders-scores-3.png">
</figure>
</center>

Nous multiplions chaque valeur par son score et sommons. Cela donne le résultat de notre auto-attention.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-value-vector-sum.png">
</figure>
</center>

Cette opération permet d’obtenir un vecteur pondéré où on peut voir par exemple que l’attention a été a prêté à 50% sur le mot *robot*, 30% au mot *a*, et 19% au mot *it*.

Dans la partie 2 on s’intéressera de plus près à l’auto-attention. Mais d’abord, continuons notre cheminement vers le haut de la pile et vers la sortie du modèle.
<br>

### <span style="color: #51C353"> **1.5.5 Sortie du modèle** </span>
Lorsque le bloc le plus haut du modèle produit son vecteur de sortie (le résultat de sa propre auto-attention suivie de son propre réseau feed-forward), le modèle multiplie ce vecteur par la matrice d’embedding.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-output-projection-2.png">
</figure>
</center>

Rappelons que chaque ligne de la matrice d’embedding correspond à un word embedding dans le vocabulaire du modèle. Le résultat de cette multiplication est interprété comme un score pour chaque mot du vocabulaire du modèle.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-output-scores-2.png">
</figure>
</center>

Nous pouvons simplement sélectionner le token avec le score le plus élevé (top_k = 1). Mais de meilleurs résultats sont obtenus si le modèle tient également compte d’autres termes. Ainsi, une bonne stratégie consiste à tirer au hasard un mot provenant du vocabulaire. Chaque mot ayant comme probabilité d’être sélectionner, le score qui lui a été attribué (de sorte que les mots avec un score plus élevé ont une plus grande chance d’être sélectionnés). Un terrain d’entente consiste à fixer top_k à 40, et à demander au modèle de prendre en compte les 40 mots ayant obtenu les scores les plus élevés.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-output.png">
</figure>
</center>
Le modèle a alors terminé une itération aboutissant à l’édition d’un seul mot. Le modèle continue l’itération jusqu’à ce que le contexte entier soit généré (1024 tokens) ou jusqu’à ce qu’un token de fin de séquence soit produit.
<br><br>


## <span style="color: #FFBF00"> **1.6 Fin de la partie 1** </span>
Voici donc un succinct aperçu du fonctionnement du GPT2. Le détail de ce qui se passe à l’intérieur de la couche d’auto-attention fait l’objet de la deuxième partie.

Notons que nous avons procéder dans cette première partie à quelques simplifications excessives :
* Nous employons les termes « mots » et « tokens » de façon interchangeable. 
Mais en réalité, le GPT2 utilise le **Byte Pair Encoding** pour créer les tokens dans son vocabulaire.
Cela signifie que les tokens sont généralement des parties des mots. Je vous invite à voir l'article sur les [Tokenizers](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) du blog pour plus de détails.
* Nous avons pris des libertés dans la rotation/transposition des vecteurs pour mieux gérer la mise en page dans les images. Au moment de l’implémentation, il faut être plus précis.
* Les Transformers utilisent des couches de normalisation dans les blocs. Nous nous sommes davantage concentrés sur l’auto-attention dans cet article. 
<br><br><br>



# <span style="color: #FF0000"> **2. Illustration de l’auto-attention** </span>
Plus tôt nous avons montré l’image suivante afin d’illustrer l’auto-attention appliquée pour le mot « it » 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-1-2.png">
</figure>
</center>
Dans cette partie, nous examinons en détail la façon de procéder. Notre volonté est d’essayer de donner un sens à ce qui arrive à chaque mot. C’est pourquoi sur les graphiques nous utilisons de nombreux vecteurs uniques. Les implémentations réelles se font en multipliant des matrices géantes ensemble. L’objectif ici est de se concentrer sur l’intuition de ce qui se passe au niveau des mots.
<br><br>


## <span style="color: #FFBF00"> **2.1 Auto-attention (sans masking)** </span>
Commençons par regarder l’auto-attention « originale » telle qu’elle est calculée dans un bloc encoder. L’auto-attention s’applique en trois étapes principales :
* Création des vecteurs Query, Key et Value pour chaque chemin.
* Pour chaque token d’entrée, on utilise son vecteur de requête pour lui attribuer un score par rapport à tous les autres vecteurs clés. 
* Sommation des vecteurs de valeurs après les avoir multipliés par leurs scores associés.
<br>


### <span style="color: #51C353"> **2.1.1 Création des vecteurs Query, Key et Value** </span>
Pour le premier token, nous prenons sa requête et la comparons à toutes les clés. Cela produit un score pour chaque clé. La première étape de l’auto-attention consiste à calculer les trois vecteurs pour chaque token (ignorons les têtes d’attention pour le moment) :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-1.png">
</figure>
</center>
<br>


### <span style="color: #51C353"> **2.1.2 Score** </span>
Nous nous concentrons sur le premier token. Nous multiplions sa requête par tous les autres vecteurs clés pour obtenir un score pour chacun des quatre tokens.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-2.png">
</figure>
</center>
<br>


### <span style="color: #51C353"> **2.1.2 Somme** </span>
Nous pouvons maintenant multiplier les scores par les vecteurs de valeurs. Une valeur avec un score élevé constituera une grande partie du vecteur résultant une fois que nous les aurons additionnés.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-3-2.png">
  <figcaption>
Plus le score est bas, plus le vecteur de valeur est transparent. C’est pour indiquer comment le fait de multiplier par un petit nombre dilue les valeurs du vecteur.
  </figcaption>
</figure>
</center>
Si nous faisons la même opération pour chaque token, nous obtenons un vecteur représentant et tenant compte du contexte pour chacun d’eux. Ces vecteurs sont ensuite présentés à la sous-couche suivante du bloc (le réseau de neurones feed-forward).

Résumé :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/self-attention-summary.png">
</figure>
</center>
<br><br>


## <span style="color: #FFBF00"> **2.2 Auto-attention (avec masking)** </span>
L’auto-attention masquée est identique à l’auto-attention, sauf à l’étape 2.

Supposons que le modèle n’a que deux tokens en entrée et que nous observons le deuxième token. Dans ce cas, les deux derniers tokens sont masqués. Le modèle attribue alors toujours aux futurs tokens un score de 0.

<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/masked-self-attention-2.png">
</figure>
</center>
<br><br>

Ce « masquage » est souvent mis en œuvre sous la forme d’une matrice appelée masque d’attention.

 A titre d’exemple, supposons avoir une séquence de quatre mots : « robot must obey orders ».                        

Sous forme matricielle, nous calculons les scores en multipliant une matrice de requêtes par une matrice clés :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/queries-keys-attention-mask.png">
</figure>
</center>
<br><br>

Après la multiplication, nous appliquons notre masque d’attention. Il règle les cellules que l’on veut masquer sur -inf ou un nombre négatif très important (par exemple -1 milliard pour le GPT2) :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-attention-mask.png">
</figure>
</center>
<br><br>

Ensuite, l’application de softmax produit les scores réels que nous utilisons pour l’auto-attention :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/transformer-attention-masked-scores-softmax.png">
</figure>
</center>
<br><br>
La signification de ce tableau de scores est la suivante :
* Lorsque le modèle traite le premier exemple de l’ensemble de données (ligne #1), qui ne contient qu’un seul mot (« robot »), 100% de son attention se porte sur ce mot.
* Lorsque le modèle traite le deuxième exemple de l’ensemble de données (ligne #2), qui contient les mots (« robot must »), lorsqu’il traite le mot « must », 48% de son attention porte sur « robot » et 52%  sur « must ».
* Et ainsi de suite.
<br><br>


## <span style="color: #FFBF00"> **2.3 L’auto-attention masquée du GPT-2** </span>    
Le GPT-2 conserve les vecteurs clé et valeur des tokens qu’il a déjà traité afin de ne peut à avoir à les recalculer à chaque fois qu’un nouveau token est traité. En pratique, le processus se déroule comme suit :    
<br>
### <span style="color: #51C353"> **2.3.1  Etape 1 : Création des vecteurs Query, Key et Value** </span>    
Supposons que le modèle traite le mot « it ». Si nous parlons du bloc du bas, alors l’entrée pour ce token serait l’embedding de « it » + l’encodage positionnel pour le slot #9 :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-1.png">
</figure>
</center>
Chaque bloc d’un Transformer a ses propres poids.  Nous nous servons de la matrice de poids pour créer les vecteurs des requêtes, des clés et des valeurs. Cela consiste en pratique à une simple multiplication.      

Le vecteur résultant de la multiplication est la concaténation des vecteurs recherchés pour le mot « it ». 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-3.png">
  <figcaption>
En multipliant le vecteur d’entrée par le vecteur de poids d’attention (et en ajoutant un vecteur de biais non représenté ici), on obtient les vecteurs clé, valeur et requête pour ce token.
  </figcaption>
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.2  Etape 1.5 : Les têtes d’attention** </span>    
Dans les exemples précédents, nous avons plongé directement dans l’auto-attention en ignorant la partie « multi-tête ». Il serait utile de faire la lumière sur ce concept.

L’auto-attention est menée plusieurs fois sur différentes parties des vecteurs Q,K,V.

« Splitter » les têtes d’attention, c’est simplement reconstruire le vecteur long sous forme de matrice.

Le plus petit GPT2 possède 12 têtes d’attention. Il s’agit donc de la première dimension de la matrice remodelée :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-split-attention-heads-1.png">
</figure>
</center>

Dans les exemples précédents, nous avons examiné ce qui se passe à l’intérieur d’une tête d’attention. Voici comment on pourrait visualiser par exemple trois des douze têtes d’attention :
<center>
<figure class="image">
<img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-split-attention-heads-2.png">
  </figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.3  Etape 2 : Scoring** </span>    
Nous pouvons maintenant procéder au scoring (en sachant que nous ne regardons qu’une seule tête d’attention et que toutes les autres mènent une opération similaire). Dans le cadre du token « it » : 
<center>
<figure class="image">
   <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-scoring-2.png">
  
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.4  Etape 3 : Somme** </span>    
Comme nous l’avons vu précédemment, nous multiplions maintenant chaque valeur par son score, puis nous les additionnons pour obtenir le résultat de l’attention portée à la tête d’attention n°1 :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-multihead-sum-1.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.5 Etape 3.5 : Fusion des têtes d’attention** </span>
Nous concaténons les têtes d’attention.
<center>
<figure class="image">
    <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-self-attention-merge-heads-1.png">
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.6  Etape 4 : Projection** </span>
Voici notre deuxième grande matrice de poids qui projette les résultats des têtes d’attention dans le vecteur de sortie de la sous-couche d’auto-attention :
<center>
<figure class="image">
    <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-projection.png">
  <figcaption>
  Nous avons produit le vecteur que nous pouvons envoyer à la couche suivante
  </figcaption>
</figure>
</center>
<br>

### <span style="color: #51C353"> **2.3.7  Etape 5 : Fully Connected Neural Network** </span>
Le réseau neuronal entièrement connecté est l’endroit où le bloc traite son token d’entrée après que l’auto-attention a inclus le contexte approprié dans sa représentation. Il est composé de deux couches.

La première couche est quatre fois plus grande que le modèle (768x4 = 3072). Pourquoi quatre fois ? Nous reprenons ici la taille utilisée dans l’article original de Transformer (la dimension du modèle était 512 et la couche #1 dans ce modèle était 2048). Cela semble donner aux modèles de Transformer une capacité de représentation suffisante pour faire face aux tâches qui leur ont été confiées jusqu’à présent. 
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-fully-connected1.png">
</figure>
</center>

La deuxième couche projette le résultat de la première couche dans la dimension du modèle (768 pour le petit GPT2). Le résultat de cette multiplication est le résultat du bloc Transformer pour ce token.
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-fully-connected2.png">
</figure>
</center>

Le token « it » est à présent terminé !
<br>

### <span style="color: #51C353"> **2.3.8 Résumé** </span>
Pour résumer, notre vecteur d’entrée rencontre ces matrices de poids :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-transformer-block-weights-2.png">
</figure>
</center>
Chaque bloc a son propre jeu de ces poids. D’autre part, le modèle n’a qu’une seule matrice d’embedding de token et une seule matrice de codage positionnel :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-weights-2.png">
</figure>
</center>

Si vous voulez voir tous les paramètres du modèle :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/gpt2-117-parameters.png">
</figure>
</center>
<br><br><br>




# <span style="color: #FF0000"> **3. Au-delà de la modélisation linguistique** </span>
Le Transformer avec seulement des décodeurs a fait ses preuves dans beaucoup d’applications autre que la modélisation du langage. Terminons ce post en regardant certaines de ces applications.
<br><br>


## <span style="color: #FFBF00"> **3.1 La traduction automatique** </span>
Un encoder n’est pas nécessaire pour effectuer une traduction :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/decoder-only-transformer-translation.png">
</figure>
</center>
<br><br>


## <span style="color: #FFBF00"> **3.2 Résumé d’un texte** </span>
C’est sur cette tâche qu’a été entraîné le premier Transformer à decoder seul. A savoir lire un article de Wikipedia (sans la section d’ouverture devant la table des matières), et à le résumer. Les sections d’ouverture réelles des articles ont été utilisées comme label dans les données d’entraînement :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/wikipedia-summarization.png">
</figure>
</center>
<br><br>



## <span style="color: #FFBF00"> **3.3 Apprentissage par transfert** </span>
Dans [Sample Efficient Text Summarization Using a Single Pre-Trained Transformer](https://arxiv.org/abs/1905.08836), un Transformer à decoder seul est d’abord pré-entrainé à la modélisation du langage, puis affiné pour faire le résumé. Il s’avère qu’il permet d’obtenir de meilleurs résultats qu’un Transformer encoder-decoder pré-entrainé.
<br>
Le papier du GPT2 montre également les résultats de la synthèse après avoir préformé le modèle à la modélisation du langage.
<br><br>



## <span style="color: #FFBF00"> **3.4 Génération de musique** </span>
Le [Music Transformer](https://magenta.tensorflow.org/music-transformer) utilise un Transformer à decoder seul pour générer de la musique avec un timing et une dynamique expressifs. La modélisation musicale est tout comme la modélisation du langage : il suffit de laisser le modèle apprendre la musique d’une manière non supervisée, puis de lui faire échantillonner les sorties.

Vous pourriez être curieux de savoir comment la musique est représentée dans ce scénario. Rappelez-vous que la modélisation du langage peut se faire au moyen de représentations vectorielles de caractères, de mots ou de token qui font partie des mots. Avec une performance musicale comme du piano, nous devons représenter les notes mais aussi la vélocité (une mesure de la force avec laquelle la touche du piano est pressée).
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-transformer-performance-encoding-3.png">
</figure>
</center>

Une performance n’est qu’une série de ces vecteurs d’un seul coup. L’exemple suivant illustre la séquence d’entrée :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-representation-example.png">
</figure>
</center>

La représentation vectorielle one-hot de cette séquence d’entrée ressemblerait à ceci :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-transformer-input-representation-2.png">
</figure>
</center>

Un exemple de représentation de l’auto-attention du Music Transformer :
<center>
<figure class="image">
  <img src="https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/GPT2/music-transformer-self-attention-2.png">
  <figcaption>
 On observe un motif triangulaire récurrent. La requête se situe à l’un de ces derniers sommets et s’occupe de toutes les notes aiguës précédentes sur le sommet, jusqu’au début de la pièce. » …  » La figure montre une requête (la source de toutes les lignes d’attention) et les mémoires précédentes en cours de traitement (les notes qui reçoivent plus de probabilités softmax sont surlignées). La coloration des lignes d’attention correspond aux différentes têtes et la largeur des poids de la probabilité softmax.
  </figcaption>
</figure>
</center>
<br><br><br>




# <span style="color: #FF0000"> **Conclusion** </span>
J'espère que cet article vous aura permi de vous familiariser avec le GPT-2.<br>
Voici quelques liens qui pourraient vous être utiles :
- les outils développés par HuggingFace, à savoir le [générateur de textes](https://transformer.huggingface.co/doc/gpt2-large) et le [chatbox](https://convai.huggingface.co/),
- un [tutoriel](https://minimaxir.com/2020/01/twitter-gpt2-bot/) en anglais de Max Woolf pour créer un bot twitter générant du texte,
- le [code du GPT2 annoté](https://amaarora.github.io/2020/02/18/annotatedGPT2.html) en anglais par Aman Arora,
- une [présentation vidéo](https://www.youtube.com/watch?v=5gRNOkQaUyI) où j'explique comment à l'INSERM, nous utilions le GPT2 afin d'effectuer de la classification.
<br><br><br>



# <span style="color: #FF0000"> **Citation** <span>
Si vous venez à utiliser des éléments de cet article, veillez s'il vous plait en créditer les auteurs en utilisant par exemple comme suit :<br>
“*Illustration du GPT2* par Loïck BOURDOIS (https://lbourdois.github.io/blog/nlp/GPT2/), d’après Jay ALAMMAR, *The Illustrated GPT-2 (Visualizing Transformer Language Models)* (https://jalammar.github.io/illustrated-gpt2/)”<br>
Merci :)
